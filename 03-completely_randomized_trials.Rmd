# Completely randomized designs with one factor {#onewayanova}

In completely randomized experiments with a single treatment with $K$ levels, we might be interested in testing whether the mean of $K$ different sub-populations are equal.
The basic assumption of most designs is that we can decompose the observation obtained into two components [@Cox:1958]
\begin{align*}
\begin{pfrac} \text{quantity depending only } \\ 
\text{on the particular unit} 
\end{pfrac} + 
\begin{pfrac} \text{quantity depending} \\
 \text{on the treatment used}\end{pfrac}
\end{align*}
This **additive** decomposition further assumes that each unit is unaffected by the treatment of the other units and that the average effect of the treatment is constant. 

 Let $\mu_1, \ldots, \mu_K$ denote the expectation (theoretical mean) of each of the $K$ sub-populations defined by the treatment. Lack of difference between treatments is equivalent to equality of means, which translates into the hypotheses
\begin{align*}
\mathscr{H}_0:& \mu_1 = \cdots = \mu_K \\
\mathscr{H}_a:& \text{at least two means are different, }\mu_i \neq \mu_j (1 \leq i < j \leq K).
\end{align*}
Note that the null hypothesis is a single numerical value (with $K-1$ restrictions), whereas the alternative is the complement of this event, i.e., all potential scenarios for which not all expectations are equal. The above assumption implies that the difference between treatments $T_i$ and $T_j$ is $a_i - a_j = \mu_i - \mu_j$.

One slight complication arising from the above is that the expectations $\mu_1, \ldots, \mu_K$ are unknown.  We can still assess the hypothesis by comparing the sample means in each group, which are noisy estimates of the expectation: their inherent variability will limit our ability to detect differences in mean if the signal-to-noise ratio is small.

```{r samplevar2, eval = FALSE, echo = FALSE, fig.cap = "Three samples from hypothetical populations with a common variance, but different means."}
set.seed(1234)
samp <- data.frame(dat = rep(c(2,10,5), each = 10) +  rt(n = 30, df = 4),
                   group = factor(rep(1:3, each = 10L)))
ggplot(data = samp,
       aes(x = group, y = dat, col = group)) +
  geom_point() +
  geom_jitter() +
  labs(col = "sample", y = "observations", x = "sample number") +
  theme(legend.position = "none") +
  stat_summary(fun = mean,
               geom = "point",
               shape = 95,
               size = 20)
```


Example of application + citation

Thinking outside the box: 

- potential confounders?
- to extrapolate to the population, we potentially need to reweigth using demographic information
- blocking


The global null hypothesis of the one-way analysis of variance (ANOVA) problem is $\mathscr{H}_0: \mu_1 = \mu_2 = \mu_3$, where $\mu_i$ $(i=1, \ldots, 3)$ is the expectation of group $i$.

Homogeneity

Null hypothesis and comparing means of multiple groups
Assumptions
Plot your data
Data compression: what you need to report?
EXPERT TIPS: visualisation tools, histograms and line charts for ordered factors.

Reminder: must be handling the confounding via randomization or blocking (explicitly)

Where do we get our power from? Decomposition of the sum of squares TOTAL = BETWEEN + WITHIN

Anova table
What happens under the null
What happens under the alternative: some illustrations with simulated data

Potential ways of assessing the null hypothesis

- what assumptions are we making in the process? 
- where do we draw our power from
- Alternative test statistics: Welch and permutation (and their validity)
- Power considerations: choosing the sample size
- estimate of effect size, type I error, number of levels (subsample size), variability
- Careful with observed power (using the observed effect size in the calculation)
- Practical significance versus statistical significance


- Thinking outside the box: how to allocate sample sizes when we do not have equal variance

-What about unbalanced experiments? An example of what can go WRONG?
- The experiment is not properly conducted; see S 2.4.6 in Lawson (2014)
- lurking variables: effective teaching methods, confounded with instructor/class time/voluntary participation
-> independence of errors to unit
-> measurements are not normal (or continuous)
- need experimental units and large enough sample sizes
- Example with variance increasing together with the level of the response

## Contrasts and pairwise tests

Reparametrizing the model
Statistical fallacy: data dredging
Multiple comparisons and family-wise error rates
  Preplanned experiments: why penalie large studies
  How much data do you need to reliably estimate
Methods for controlling multiplicity

Thinking outside the box: no free lunch.
