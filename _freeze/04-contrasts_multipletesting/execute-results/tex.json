{
  "hash": "7be62173e8c2893cdce35a3b51efc7f7",
  "result": {
    "markdown": "# Contrasts and multiple testing {#contrasts-multiple-testing}\n\n\n\n::: {.cell file='_common.R' hash='04-contrasts_multipletesting_cache/pdf/setup_79dbc48c5832a87bb296b0e3622c9867'}\n\n:::\n\n\n\nThe analysis of variance model tests the (global) null hypothesis that the average of all groups is equal. In an experimental context, this implies one or more of the manipulation has a different effect from the others on the mean response. Oftentimes, this isn't interesting in itself: we could be interested in comparing different options relative to a *status quo*, or determine whether specific combinations work better than separately. The scientific question of interest that warranted the experiment may lead to a specific set of hypotheses, which can be formulated by researchers as comparisons between means of different subgroups.\n\n## Contrasts\n\nWe can normally express these as **contrasts**. As [Dr. Lukas Meier](https://stat.ethz.ch/~meier) puts it, if the global $F$-test for equality of means is equivalent to a dimly lit room, contrasts are akin to spotlight that let one focus on particular aspects of differences in treatments.\n\nMore formally speaking, a contrast is a linear combination of averages: in plain English, this means we assign a weight to each group average and add them up. We can then build a $t$ statistic as usual by standardizing the resulting weighted sum of the group means. \n\nContrasts encode research question of interest: if $c_i$ denotes the weight of group average $\\mu_i$ $(i=1, \\ldots, K)$, then we can write the contrast as $C = c_1 \\mu_1 + \\cdots + c_K \\mu_K$ with the null hypothesis $\\mathscr{H}_0: C=a$ for a two-sided alternative. The numerical value $a$ is typically zero. The sample estimate of the linear contrast is obtained by replacing the unknown population average $\\mu_i$ by the sample average of that group, $\\widehat{\\mu}_i = \\overline{y}_{i}$. We can easily obtain the standard error of the linear combination $C$.^[Should you need the formula, the standard error assuming sample size of $n_1, \\ldots, n_K$ and a common variance $\\sigma^2$ is $\\sqrt{\\mathsf{Va}(\\widehat{C})}$, where $$\\mathsf{Va}(\\widehat{C}) = \\widehat{\\sigma}^2\\left(\\frac{c_1^2}{n_1} + \\cdots + \\frac{c_K^2}{n_K}\\right).$$]\n\n\n### Orthogonal contrasts\n\nSometimes, linear contrasts encode disjoint bits of information about the sample: for example, one contrast that compares groups the first two groups versus one that compares the third and fourth is in effect using data from two disjoint samples, as contrasts will be based on sample averages. Whenever the contrasts vectors are orthogonal, the tests will be uncorrelated as they contain independent bits of information from the population.^[The constraint $c_1 + \\cdots + c_K=0$ ensures that linear contrasts are orthogonal to the mean, which has weight $c_i=n_i/n$ and for balanced samples $c_i =1/n$.] Mathematically, if we let $c_{i}$ and $c^{*}_{i}$ denote weights attached to the mean of group $i$ comprising $n_i$ observations, contrasts are orthogonal if $c_{1}c^{*}_{1}/n_1 + \\cdots + c_{K}c^{*}_K/n_K = 0$; if the sample is balanced with the same number of observations in each group, $n/K = n_1 =\\cdots = n_K$^[This is the dot product of the two contrast vectors]. If we have $K$ groups, there are $K-1$ contrasts for pairwise differences, the last one being captured by the sample mean for the overall effect.If we care only about difference between groups (as opposed to the overall effect of all treatments), we impose a sum-to-zero constraint on the weights so $c_1 + \\cdots + c_K=0$. This ensures^[The sample mean for a balanced sample amounts to equi-weighted average of the groups, i.e., a contrast with weight vector $(1, 1, \\ldots, 1)$. Thus, any contrast whose elements sum to zero is orthogonal to the global mean. End of the mathematical digression.] Keep in mind that, although independent tests are nice mathematically, contrasts should encode the hypothesis of interest to the researchers: we choose contrasts because they are meaningful, not because they are orthogonal.\n\n\n\n:::{ .example name=\"Contrasts for encouragement on teaching\"}\n\nThe `arithmetic` data example considered five different treatment groups with 9 individuals in each. Two of them were control groups, one received praise, another was reproved and the last was ignored.\n\nSuppose that researchers were interested in assessing whether the experimental manipulation had an effect, and whether the impact of positive and negative feedback is the same on students.^[These would be formulated *at registration time*, but for the sake of the argument we proceed as if they were.]\n\nSuppose we have five groups in the order (control 1, control 2, praised, reproved, ignored). \nWe can express these hypothesis as\n\n- $\\mathscr{H}_{01}$: $\\mu_{\\text{praise}} = \\mu_{\\text{reproved}}$\n- $\\mathscr{H}_{02}$: \n\\begin{align*}\n\\frac{1}{2}(\\mu_{\\text{control}_1}+\\mu_{\\text{control}_2}) = \\frac{1}{3}\\mu_{\\text{praised}} + \\frac{1}{3}\\mu_{\\text{reproved}} + \\frac{1}{3}\\mu_{\\text{ignored}}\n\\end{align*}\n\nNote that, for the hypothesis of control vs experimental manipulation, we look at average of the different groups associated with each item. Using the ordering, the weights of the contrast vector are $(1/2, 1/2, -1/3, -1/3, -1/3)$ and $(0, 0, 1, -1, 0)$. There are many equivalent formulation: we could multiply the weights by any number (different from zero) and we would get the same test statistic, as the latter is standardized.\n\n\n\n\n::: {.cell layout-align=\"center\" hash='04-contrasts_multipletesting_cache/pdf/unnamed-chunk-2_bda633527b466c717dea73f9faa0c8bf'}\n\n```{.r .cell-code}\nlibrary(emmeans)\ndata(arithmetic, package = \"hecedsm\")\nlinmod <- aov(score ~ group, data = arithmetic)\nlinmod_emm <- emmeans(linmod, specs = 'group')\ncontrast_specif <- list(\n  controlvsmanip = c(0.5, 0.5, -1/3, -1/3, -1/3),\n  praisedvsreproved = c(0, 0, 1, -1, 0)\n)\ncontrasts_res <- \n  contrast(object = linmod_emm, \n                    method = contrast_specif)\n# Obtain confidence intervals instead of p-values\nconfint(contrasts_res)\n```\n:::\n\n::: {#tbl-contrast-arithmetic-confint .cell layout-align=\"center\" tbl-cap='Contrasts estimates for the arithmetic data' hash='04-contrasts_multipletesting_cache/pdf/tbl-contrast-arithmetic-confint_41784f73833423e94ab33a036dea56a1'}\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}{lrrrrr}\n\\toprule\ncontrast & estimate & std. error & df & lower conf. limit & upper conf. limit\\\\\n\\midrule\ncontrol vs manip & -3.33 & 1.05 & 40 & -5.45 & -1.22\\\\\npraised vs reproved & 4.00 & 1.62 & 40 & 0.72 & 7.28\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\n:::\n\n::: {#exm-teachingtoread name=\"Teaching to read\"}\n\n\nWe consider data from @Baumann:1992. The abstract of the paper provides a brief description of the study\n\n> This study investigated the effectiveness of explicit instruction in think aloud as a means to promote elementary students' comprehension monitoring abilities. Sixty-six fourth-grade students were randomly assigned to one of three experimental groups: (a) a Think-Aloud (TA) group, in which students were taught various comprehension monitoring strategies for reading stories (e.g., self-questioning, prediction, retelling, rereading) through the medium of thinking aloud; (b) a Directed reading-Thinking Activity (DRTA) group, in which students were taught a predict-verify strategy for reading and responding to stories; or (c) a Directed reading Activity (DRA) group, an instructed control, in which students engaged in a noninteractive, guided reading of stories. \n\n\nLooking at @tbl-print-pairwise-baumann, we can see that `DRTA` has the highest average, followed by `TA` and directed reading (`DR`).\n\n\n\n::: {.cell layout-align=\"center\" hash='04-contrasts_multipletesting_cache/pdf/pairwise-baumann_48198741a9878994520b7661da9a4f82'}\n\n```{.r .cell-code}\nlibrary(emmeans) #load package\ndata(BSJ92, package = \"hecedsm\")\nmod_post <- aov(posttest1 ~ group, data = BSJ92)\nemmeans_post <- emmeans(object = mod_post, \n                        specs = \"group\")\n```\n:::\n\n::: {#tbl-print-pairwise-baumann .cell layout-align=\"center\" tbl-cap='Estimated group averages with standard errors and 95% confidence intervals for post-test 1.' hash='04-contrasts_multipletesting_cache/pdf/tbl-print-pairwise-baumann_e2f63500be2290fa990547734e89b390'}\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}{lrrrrr}\n\\toprule\nTerms & Marginal mean & Standard error & Degrees of freedom & Lower limit (CI) & Upper limit (CI)\\\\\n\\midrule\nDR & 6.68 & 0.68 & 63 & 5.32 & 8.04\\\\\nDRTA & 9.77 & 0.68 & 63 & 8.41 & 11.13\\\\\nTA & 7.77 & 0.68 & 63 & 6.41 & 9.13\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\n\nThe purpose of @Baumann:1992 was to make a particular comparison between treatment groups. \nFrom the abstract:\n\n> The primary quantitative analyses involved two planned orthogonal contrasts—effect of instruction (TA + DRTA vs. 2 x DRA) and intensity of instruction (TA vs. DRTA)—for three whole-sample dependent measures: (a) an error detection test, (b) a comprehension monitoring questionnaire, and (c) a modified cloze test.\n\nThe hypothesis of @Baumann:1992 is $\\mathscr{H}_0: \\mu_{\\mathrm{TA}} + \\mu_{\\mathrm{DRTA}} = 2 \\mu_{\\mathrm{DRA}}$ \nor, rewritten slightly,\n\\begin{align*}\n\\mathscr{H}_0: - 2 \\mu_{\\mathrm{DR}} + \\mu_{\\mathrm{DRTA}} + \\mu_{\\mathrm{TA}} = 0.\n\\end{align*}\nwith weights $(-2, 1, 1)$; the order of the levels for the treatment are \n($\\mathrm{DRA}$, $\\mathrm{DRTA}$, $\\mathrm{TA}$) and it must match that of the coefficients.\nAn equivalent formulation is $(2, -1, -1)$ or $(1, -1/2, -1/2)$: in either case, the estimated differences will be different\n(up to a constant multiple or a sign change).\nThe vector of weights for $\\mathscr{H}_0:  \\mu_{\\mathrm{TA}} = \\mu_{\\mathrm{DRTA}}$ \nis ($0$, $-1$, $1$): the zero appears because the first component, $\\mathrm{DRA}$ doesn't appear.\nThe two contrasts are orthogonal since\n$(-2 \\times 0) + (1 \\times -1) + (1 \\times 1) = 0$. \n\n\n\n::: {.cell layout-align=\"center\" hash='04-contrasts_multipletesting_cache/pdf/contrasts_4b93f642bebaecc18b2dc8a1fe44103e'}\n\n```{.r .cell-code}\n# Identify the order of the level of the variables\nwith(BSJ92, levels(group))\n#> [1] \"DR\"   \"DRTA\" \"TA\"\n# DR, DRTA, TA (alphabetical)\ncontrasts_list <- list(\n  \"C1: DRTA+TA vs 2DR\" = c(-2, 1, 1), \n  # Contrasts: linear combination of means, coefficients sum to zero\n  # 2xDR = DRTA + TA => -2*DR + 1*DRTA + 1*TA = 0 and -2+1+1 = 0\n  \"C1: average (DRTA+TA) vs DR\" = c(-1, 0.5, 0.5), \n  #same thing, but halved so in terms of average\n  \"C2: DRTA vs TA\" = c(0, 1, -1),\n  \"C2: TA vs DRTA\" = c(0, -1, 1) \n  # same, but sign flipped\n)\ncontrasts_post <- \n  contrast(object = emmeans_post,\n           method = contrasts_list)\ncontrasts_summary_post <- summary(contrasts_post)\n```\n:::\n\n::: {#tbl-print-contrasts .cell layout-align=\"center\" tbl-cap='Estimated contrasts for post-test 1.' hash='04-contrasts_multipletesting_cache/pdf/tbl-print-contrasts_4fd329b751726d264e6c923fb163f75a'}\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}{lrrrrr}\n\\toprule\nContrast & Estimate & Standard error & Degrees of freedom & t statistic & p-value\\\\\n\\midrule\nC1: DRTA+TA vs 2DR & 4.18 & 1.67 & 63 & 2.51 & 0.01\\\\\nC1: average (DRTA+TA) vs DR & 2.09 & 0.83 & 63 & 2.51 & 0.01\\\\\nC2: DRTA vs TA & 2.00 & 0.96 & 63 & 2.08 & 0.04\\\\\nC2: TA vs DRTA & -2.00 & 0.96 & 63 & -2.08 & 0.04\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\nWe can look at these differences; since `DRTA` versus `TA` is a pairwise difference, we could have obtained the $t$-statistic directly from the pairwise contrasts\nusing `pairs(emmeans_post)`. Note that the two different ways of writing the comparison between `DR` and the average of the other two methods yield different point estimates, but same inference (meaning the same $p$-values). For contrast $C_{1b}$, we get half the estimate (but the standard error is also halved) and likewise for the second contrasts we get an estimate of $\\mu_{\\mathrm{DRTA}} - \\mu_{\\mathrm{TA}}$ in the first case ($C_2$) and $\\mu_{\\mathrm{TA}} - \\mu_{\\mathrm{DRTA}}$: the difference in group averages is the same up to sign.\n\nWhat is the conclusion of our analysis of contrasts? \nIt looks like the methods involving teaching aloud have a strong impact on reading comprehension relative to only directed reading. The evidence is not as strong\nwhen we compare the method that combines directed reading-thinking activity and thinking aloud.\n\n:::\n\n## Multiple testing\n\nBeyond looking at the global null, we will be interested in a set of contrast statistics and typically this number can be large-ish. There is however a catch in starting to test multiple hypothesis at once.\n\n\nIf you do a **single** hypothesis test and the testing procedure is well calibrated (meaning that the model model assumptions hold), there is a probability of $\\alpha$ of making a type I error if the null is true, meaning when there is no difference between averages in the underlying population. The problem of the above approach is that the more you look, the higher the chance of finding something: with 20 independent tests, we expect that, on average, one of them will yield a $p$-value less than 5\\%. This, coupled with the tendency in the many fields to dichotomise the result of every test depending on whether $p \\leq \\alpha$ (statistically significant at level $\\alpha$ or not leads to selective reporting of findings. \n\nNot all tests are of interest, even if standard software will report all possible pairwise comparisons. However, the number of tests performed in the course of an analysis can be very large. Dr. Yoav Benjamini investigated the number of tests performed in each study of the Psychology replication project [@Nosek:2015]: this number ranged from 4 to 700, with an average of 72 per study. It is natural to ask then how many are spurious findings that correspond to type I errors. The paramount (absurd) illustration is the xkcd cartoon of @fig-xkcdsignificant.\n\n\nNot all tests are of interest, even if standard software will report all possible pairwise comparisons. If there are $K$ groups to compare and any comparison is of interest, than we could performs $\\binom{K}{2}$ pairwise comparisons with $\\mathscr{H}_{0}: \\mu_i = \\mu_j$ for $i \\neq j$. For $K=3$, there are three such comparisons, 10 pairwise comparisons if $K=5$ and 45 pairwise comparisons if $K=10$. Thus, some 'discoveries' are bound to be spurious.\n\nThe number of tests performed in the course of an analysis can be very large. Y. Benjamini investigated the number of tests performed in each study of the Psychology replication project [@Nosek:2015]: this number ranged from 4 to 700, with an average of 72 --- most studies did not account for the fact they were performing multiple tests or selected the model. It is natural to ask then how many results are spurious findings that correspond to type I errors. The paramount (absurd) illustration is the cartoon presented in @fig-xkcdsignificant: note how there is little scientific backing for the theory (thus such test shouldn't be of interest to begin with) and likewise the selective reporting made of the conclusions, despite nuanced conclusions.\n\nWe can also assess mathematically the problem. Assume for simplicity that all tests are independent^[This is the case if tests are based on different data, or if the contrasts considered are orthogonal under normality.] and that each test is conducted at level $\\alpha$. The probability of making at least one type I error, say $\\alpha^{\\star}$, is^[The second line holds with independent observations, the second follows from the use of Boole's inequality and does not require independent tests.]\n\\begin{align}\n\\alpha^{\\star} &= 1 – \\text{probability of making no type I error} \n\\\\ &= 1- (1-\\alpha)^m\n\\\\ & \\leq m\\alpha\n\\end{align}\n\nWith $\\alpha = 5$% and $m=4$ tests, $\\alpha^{\\star} \\approx 0.185$ whereas for $m=72$ tests, $\\alpha^{\\star} \\approx 0.975$: this means we are almost guaranteed even when nothing is going on to find \"statistically significant\" yet meaningless results.\n\n\n\n::: {.cell layout-align=\"center\" hash='04-contrasts_multipletesting_cache/pdf/fig-xkcdsignificant_e0f469fe9b52c478ce4bd2d66e9e4a01'}\n::: {.cell-output-display}\n![xkcd 882: Significant. The alt text is 'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!'](figures/xkcd882_significant.png){#fig-xkcdsignificant fig-align='center' width=85%}\n:::\n:::\n\n\n\nIt is sensible to try and reduce or bound the number of false positive or control the probability of getting spurious findings. We consider a **family** of $m$ null hypothesis $\\mathscr{H}_{01}, \\ldots, \\mathscr{H}_{0m}$ tested. The family is simply a collection of $m$ hypothesis tests: the exact set depends on the context, but this comprises all hypothesis that are scientifically relevant and could be reported. These comparisons are called **pre-planned comparisons**: they should be chosen before the experiment takes place and pre-registered to avoid data dredging and selective reporting. The number of planned comparisons should be kept small relative to the number of parameters: for a one-way ANOVA, a general rule of thumb is to make no more comparisons than the number of groups, $K$.\n\nSuppose that we perform $m$ hypothesis tests in a study and define binary indicators\n\\begin{align}\nR_i &= \\begin{cases} 1 & \\text{if we reject the null hypothesis }  \\mathscr{H}_{0i} \\\\\n0 & \\text{if we fail to reject } \\mathscr{H}_{0i}\n\\end{cases}\\\\\nV_i &=\\begin{cases} 1 & \\text{type I error for } \\mathscr{H}_{0i}\\quad  (R_i=1 \\text{ and  }\\mathscr{H}_{0i} \\text{ is true}) \\\\ 0 & \\text{otherwise}.\n\\end{cases}\n\\end{align}\nWith this notation,  $R=R_1 + \\cdots + R_m$ simply encodes the total number of rejections ($0 \\leq R \\leq m$), and $V = V_1 + \\cdots + V_m$ is the number of null hypothesis rejected by mistake ($0 \\leq V \\leq R$). \n\nThe **familywise error rate** is the probability of making at least one type I error per family, \n\\begin{align*}\n\\mathsf{FWER} = \\Pr(V \\geq 1).\n\\end{align*}\nTo control the familywise error rate, one must be more stringent in rejecting the null and perform each test with a smaller level $\\alpha$ so that the overall or simultaneous probability is less than $\\mathsf{FWER}$.\n\n### Bonferroni's procedure\n\nThe easiest way to control for multiple testing is to perform each test at level $\\alpha/m$, thereby ensuring that the family-wise error is controlled at level $\\alpha$.\n\n\n\nThe Bonferroni adjustment also controls the **per-family error rate**, which is the expected (theoretical average) number of false positive $\\mathsf{PFER} = \\mathsf{E}(V)$. The latter is a more stringent criterion than the familywise error rate because $\\Pr(V \\geq 1) \\leq \\mathsf{E}(V)$: the familywise error rate does not make a distinction between having one or multiple type I errors.^[By definition, the expected number of false positive (PFER) is $\\mathsf{E}(V) = \\sum_{i=1}^m i \\Pr(V=i) \\geq \\sum_{i=1}^m \\Pr(V=i) = \\Pr(V \\geq 1)$, so larger than the probability of making at least type 1 error. Thus, any procedure that controls the per-family error rate (e.g., Bonferroni) also automatically bounds the familywise error rate.] \n\nWhy is Bonferroni's procedure popular? It is conceptually easy to understand and simple, and it applies to any design and regardless of the dependence between the tests. However, the number of tests to adjust for, $m$, must be prespecified and the procedure leads to low power when the size of the family is large. Moreover, if our sole objective is to control for the familywise error rate, then there are other procedures that are always better in the sense that they still control the $\\mathsf{FWER}$ while leading to increased capacity of detection when the null is false.\n\nIf the raw (i.e., unadjusted) $p$-values are reported, we reject hypothesis $\\mathscr{H}_{0i}$ if $m \\times p_i \\ge \\alpha$: operationally, we multiply each $p$-value by $m$ and reject if the result exceeds $\\alpha$.\n\n\n### Holm-Bonferroni's procedure\n\nThe idea of Holm's procedure is to use a sharper inequality bound and amounts to performing tests at different levels, with more stringent for smaller $p$-values.\n\nOrder the $p$-values of the family of $m$ tests from smallest to largest\n$p_{(1)} \\leq \\cdots \\leq p_{(m)}$ and test sequentially the hypotheses. Coupling Holm's method with Bonferroni's procedure, we compare $p_{(1)}$ to $\\alpha_{(1)} = \\alpha/m$, $p_{(2)}$ to $\\alpha_{(2)}=\\alpha/(m-1)$, etc. \n\nIf all of the $p$-values are less than their respective levels, than we still reject each null hypothesis. Otherwise, we reject all the tests whose $p$-values exceeds the smallest nonsignificant one. If $p_{(j)} \\geq \\alpha_{(j)}$ but $p_{(i)} \\leq \\alpha_{(i)}$ for $i=1, \\ldots, j-1$ (all smaller $p$-values), we reject the associated hypothesis $\\mathscr{H}_{0(1)}, \\ldots, \\mathscr{H}_{0(j-1)}$ but fail to reject $\\mathscr{H}_{0(j)}, \\ldots, \\mathscr{H}_{0(m)}$.\n\nThis procedure doesn't control the per-family error rate, but is uniformly more powerful and thus leads to increased detection than Bonferroni's method.\n\nTo see this, consider a family of $m=3$ $p$-values with values $0.01$, $0.04$ and $0.02$. Bonferroni's adjustment would lead us to reject the second and third hypotheses at level $\\alpha=0.05$, but not Holm-Bonferroni.\n\n**To be continued**...\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}