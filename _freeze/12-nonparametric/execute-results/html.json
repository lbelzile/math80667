{
  "hash": "71a6dc2cb5773f3b516d56a79a6d4aa5",
  "result": {
    "markdown": "# Nonparametric tests\n\n\n::: {.cell file='_common.R' hash='12-nonparametric_cache/html/setup_22fee411dacbdcdd9d8448a7fd3e04fb'}\n\n:::\n\n\nIn small samples or in the presence of very skewed outcome responses, often combined with extreme observations, the conclusions drawn from the large-sample approximations for $t$-tests or analysis of variance models need not hold. This chapter presents **nonparametric tests**.\n\nIf our responses are numeric (or at least ordinal, such as those measured by Likert scales), we could subtitute them by their ranks. Ranks give the relative ordering in a sample of size $n$, where rank 1 denotes the smallest observation and rank $n$ the largest. Ranks are not affected by outliers and are more robust (contrary to averages), but discard information. For example, ranking the set of four observations (8,2,1,2) gives ranks (4, 2.5., 1, 2.5) if we assign the average rank to ties.\n\nWhen are nonparametric tests used? The answer is that they are robust (meaning their conclusions are less affected) by departure from distributional assumptions (data are normal) and by outliers. In large samples, the central limit theorem kicks in and the behaviour of most group average is normal. However, in small samples, the quality of the $p$-value approximate depends more critically on whether the model assumptions hold or not. \n\nAll of what has been covered so far is part of parametric statistics: we assume summary statistics behave in a particular way and utilize the probabilistic model from which these originate to describe the range of likely outcomes under the null hypothesis. As ranks are simply numbers between $1$ to $n$ (if there are no ties), no matter how data are generated, we can typically assess the repartition of those integers under the null hypothesis. There is no free lunch: while rank-based tests require fewer modelling assumptions, but they have lower power than their parametric counterparts *if* the assumption underlying these tests are validated.\n\nIn short: the more you are willing to assume, the more information you can squeeze out of your problem. However, the inference can be fragile so you have to decide on a trade-off between efficiency (keeping all numerical records) and robustness (e.g., keeping only the signs or the ranking of the data).\n\n\nThe following list nonparametric tests and their popular parametric equivalent.\n\n- sign test: an alternative to a one-sample $t$-test (also valid for paired measurements, where we subtract the two to get a single numeric number and we rank differences). Only uses the sign of the difference, minimal assumptions but not powerful\n- Wilcoxon's signed rank test: idem, but using the ranks of the observations\n- Mann--Whitney $U$ or Wilcoxon's rank-sum test: the nonparametric analog of two-sample $t$-test, which ranks all observations in the sample and compares them between groups. For between-subject designs\n\nThese can be extended with repeated measurements to more than two groups: \n\n- [Friedman's rank sum test](https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/friedman.htm) for completely randomized block design: ranks are computed within each block (one block, one experimental factor) and we consider the sum of the ranks for each treatment level. Equivalent of sign test with more samples; also [Quade's test](https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/quade.htm)\n- Kruskal--Wallis test: one-way analysis of variance model with ranks, obtained by pooling all observations, computing the ranks, and splitting them by experimental condition.\n\nFor more than 15 observations, the normal, student or Fisher approximation obtained by running the tests from the linear model or ANOVA function yield more or less the same benchmarks for all useful purposes: [see J. K. LindelÃ¸v](https://lindeloev.github.io/tests-as-linear/) cheatsheet and examples for indications.\n\n\n\n\n## Wilcoxon signed rank test\n\n\n\nThe most common use of the signed rank test is for paired samples for which the response is measured on a numeric or ordinal scale. Let $Y_{ij}$ denote measurement $j$ of person $i$ and the matching observation $Y_{kj}$. For each pair $i=1, \\ldots, n$, we can compute the difference $D_i = Y_{ij}-Y_{ik}$.^[With one sample, we postulate a median $\\mu_0$ and set $D_i = Y_{i} - \\mu_0$.] If we assume there is no difference between the distributions of the values taken, then the distribution of the difference $D_j$ is symmetric around zero under the null hypothesis.^[We could subtract likewise $\\mu_0$ from the paired difference if we assume the distributions are $\\mu_0$ units apart.] The statistic tests thus tests whether the median is zero.^[When using ranks, we cannot talk about the mean of the distribution, but rather about quantiles.]\n\nOnce we have the new differences $D_1, \\ldots, D_n$, we take absolute values and compute their ranks, $R_i = \\mathsf{rank}|D_i|$. The test statistic is formed by computing the sum of the ranks $R_i$ associated with positive differences $D_i>0$. How does this test statistic work as a summary of evidence? If there was no difference we expect roughly half of the centered observations or paired difference to be positive and half to be negative. The sum of positive ranks should be close to the average rank: for a two-sided test, large or small sums are evidence against the null hypothesis that there is no difference between groups.\n\n\nIn management sciences, Likert scales are frequently used as response. The drawback of this approach, unless the response is the average of multiple questions, is that there will be ties and potentially zero differences $D_j=0$. There are subtleties associated with testing, since the signed rank assumes that all differences are either positive or negative. The `coin` package in **R** deals correctly with such instances, but it is important to specify the treatment of such values.^[For example, are zero difference discarded prior to ranking, as suggested by Wilcoxon, or kept for the ranking and discarded after, as proposed by @Pratt:1959? We also need to deal with ties, as the distribution of numbers changes with ties. If this seems complicated to you, well it is... so much that the default implementation in **R** is unreliable. [Charles Geyer](https://www.stat.umn.edu/geyer/5601/examp/signrank.html) illustrate the problems with the *zero fudge*, but the point is quite technical. His notes make a clear case that you can't trust default software, even if it's been sitting around for a long time.]\n\n\n\n:::{ .example name=\"Smartwatches and other distractions\"}\n\nWe consider a within-subject design from @Brodeur:2021, who conducted an experiment at Tech3Lab to check distraction while driving from different devices including smartwatches using a virtual reality environment. The authors wanted to investigate whether smartwatches were more distracting than cellphones while driving. Using a simulator, they ran a within-subject design where each participant was assigned to a distraction (phone, using a speaker, texting while driving or smartwatch) while using a  driving simulator. The response is the number of road safety violations conducted on the segment. Each task was assigned in a random order. The data can be found in the `BRLS21_T3` dataset in package `hecedsm`.\n\n\n::: {.cell layout-align=\"center\" hash='12-nonparametric_cache/html/unnamed-chunk-2_ac03af3373badd27b66d6632c652d789'}\n\n:::\n\n\nA quick inspection reveals that the data are balanced with four tasks and 31 individuals. We can view the within-subject design with a single replication as a complete block design (with `id` as block) and `task` as experimental manipulation.\n\nHow could we compare the different tasks? The data here are clearly very far from normally distributed and there are notable outliers among the residuals, as evidenced by \\@ref(fig:fig-normqqplot). Conclusions probably wouldn't be affected by using an analysis of variance, but it may be easier to convince reviewers that the findings are solid by ressorting to nonparametric procedures.\n\n\n\n::: {.cell layout-align=\"center\" hash='12-nonparametric_cache/html/fig-normqqplot_29bcb96295c8c291d9bd6a1f7490846e'}\n::: {.cell-output-display}\n![Normal quantile-quantile plot of the block design. There are many outliers](12-nonparametric_files/figure-html/fig-normqqplot-1.png){#fig-normqqplot fig-align='center' width=80%}\n:::\n:::\n\n\nBoth the Friedman and the Quade tests are obtained by computing ranks within each block (participant) and then performing a two-way analysis of variance. The Friedman test is less powerful than Quade's with a small number of groups. Both are applicable for block designs with a single factor. \n\n\n::: {.cell layout-align=\"center\" hash='12-nonparametric_cache/html/unnamed-chunk-4_ef7933d93fee24387fed79a1e96c5c26'}\n\n```{.r .cell-code}\ndata(BRLS21_T3, package = \"hecedsm\")\nfriedman <- coin::friedman_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\nquade <- coin::quade_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\neff_size <- effectsize::kendalls_w(\n  x = \"nviolation\", \n  groups = \"task\", \n  blocks = \"id\", \n  data = BRLS21_T3)\n```\n:::\n\n\n\nThe Friedman test is obtained by replacing observations by the rank within each block (so rather than the number of violations per task, we compute the rank among the four tasks). The Friedman's test statistic is $18.97$ and is compared to a benchmark $\\chi^2_3$ distribution, yielding a $p$-value of $0$. \n\n\nWe can also obtain effect sizes for the rank test, termed Kendall's $W$. A value of 1 indicates complete agreement in the ranking: here, this would occur if the ranking of the number of violations was the same for each participant. The estimated agreement (effect size) is $0.2$.\n\nThe test reveals significant differences in the number of road safety violations across tasks. We could therefore perform all pairwise differences using the signed-rank test and adjust $p$-values to correct for the fact we have performed six hypothesis tests.\n\n\nTo do this, we modify the data and map them to wide-format (each line corresponds to an individual). We can then feed the data to compute differences, here for `phone` vs `watch`. We could proceed likewise for the five other pairwise comparisons and then adjust $p$-values.\n\n\n::: {.cell layout-align=\"center\" hash='12-nonparametric_cache/html/unnamed-chunk-5_a412a2a686f230f9c771f054f96adaf7'}\n\n```{.r .cell-code}\nsmartwatch <- tidyr::pivot_wider(\n  data = BRLS21_T3,\n  names_from = task,\n  values_from = nviolation)\ncoin::wilcoxsign_test(phone ~ watch,\n                      data = smartwatch)\n#> \n#> \tAsymptotic Wilcoxon-Pratt Signed-Rank Test\n#> \n#> data:  y by x (pos, neg) \n#> \t stratified by block\n#> Z = 0.4, p-value = 0.7\n#> alternative hypothesis: true mu is not equal to 0\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='12-nonparametric_cache/html/unnamed-chunk-6_937864d08156f5bff4bf7ee5b47e7de7'}\n\n:::\n\n\nYou can think of the test as performing a paired $t$-test for the 31 signed ranks $R_i =\\mathsf{sign}(D_i) \\mathsf{rank}(|D_i|)$ and testing whether the mean is zero. The $p$-value obtained by doing this after discarding zeros is $0.73$, which is pretty much the same as the more complicated approximation.\n\n:::\n\n## Wilcoxon rank sum test and Kruskal--Wallis test\n\nThese testing procedures are the nonparametric analog of the one-way analysis in a between-subject design. One could be interested in computing the differences between experimental conditions (pairwise) or overall if there are $K \\geq 2$ experimental conditions. To this effect, we simply pool all observations, rank them and compare the average rank in each group. We can track what should be the repartition of data if there was no difference between groups (all ranks should be somehow uniformly distributed among the $K$ groups). If there are groups with larger averages than others, than this is evidence.\n\nIn the two-sample case, we may also be interested in providing an estimator of the difference between condition. To this effect, we can compute the average of pairwise differences between observations of each pair of groups: those are called Walsh's averages. The Hodges--Lehmann estimate of location is simply the median of Walsh's averages and we can use the Walsh's averages themselves to obtain a confidence interval.\n\n:::{ .example  name=\"Virtual communications\"}\n\n@Brucks.Levav:2022 measure the attention of participants based on condition using an eyetracker. We compare the time spend looking at the partner by experimental condition (face-to-face or videoconferencing). The authors used a Kruskal--Wallis test, but this is equivalent to Wilcoxon's rank-sum test.\n\n\n::: {.cell layout-align=\"center\" hash='12-nonparametric_cache/html/unnamed-chunk-7_7ca85e24fcbde6657c9ef6278b8927da'}\n\n```{.r .cell-code}\ndata(BL22_E, package = \"hecedsm\")\nmww <- coin::wilcox_test(\n  partner_time ~ cond, \n  data = BL22_E, \n  conf.int = TRUE)\nwelch <- t.test(partner_time ~ cond, \n  data = BL22_E, \n  conf.int = TRUE)\nmww\n#> \n#> \tAsymptotic Wilcoxon-Mann-Whitney Test\n#> \n#> data:  partner_time by cond (f2f, video)\n#> Z = -6, p-value = 1e-10\n#> alternative hypothesis: true mu is not equal to 0\n#> 95 percent confidence interval:\n#>  -50.7 -25.9\n#> sample estimates:\n#> difference in location \n#>                  -37.8\n```\n:::\n\n\nThe output of the test includes, in addition to the $p$-value for the null hypothesis that both median time are the same, a confidence interval for the time difference (in seconds). The Hodges--Lehmann estimate of location is $-37.81$ seconds, with a 95% confidence interval for the difference of $[-50.69, -25.91]$ seconds.\n\nThese can be compared with the usual Welch's two-sample $t$-test with unequal variance. The estimated mean difference is $-39.69$ seconds for face-to-face vs group video, with a 95% confidence interval of  $[-52.93, -26.45]$.\n\nIn either case, it's clear that the videoconferencing translates into longer time spent gazing at the partner than in-person meetings.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}