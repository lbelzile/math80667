{
  "hash": "d2c6fded21ef781ce0a3037fbfa0826c",
  "result": {
    "markdown": "# Replication crisis {#replication-crisis}\n\n\n::: {.cell file='_common.R' hash='08-reproducibility_crisis_cache/html/setup_bf7b8dc6e520a54c306c0a238919c00b'}\n\n:::\n\n\nIn recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications failed to yield anything like what authors used to claim, or found much weaker findings. This chapter examines some of the causes of this lack of replicability.\n\n:::keyidea\n\n* Defining replicability and reproducibility.\n* Understanding the scale of the replication crisis.\n* Recognizing common statistical fallacies.\n* Listing strategies for enhancing reproducibility.\n\n:::\n\n\nWe adopt the terminology of @Claerbout/Karrenbach:1992: a study is said to be **reproducible** if an external person with the same data and enough indications about the procedure (for example, the code and software versions, etc.) can obtain consistent results that match those of a paper. A related scientific matter is **replicability**, which is the process by which new data are collected to test the same hypothesis, potentially using different methodology. Reproducibility is important because it enhances the credibility of one's work. Extensions that deal with different analyses leading to the same conclusion are described in [The Turing Way]([https://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html]) and presented in @fig-turingrepdo.\n\n\n\n\n::: {.cell layout-align=\"center\" hash='08-reproducibility_crisis_cache/html/fig-turingrepdo_0d5350bc9331ba433b51629f7719bd21'}\n::: {.cell-output-display}\n![Definition of different dimensions of reproducible research (from The Turing Way project, illustration by Scriberia).](figures/turing_way.jpg){#fig-turingrepdo fig-align='center' width=85%}\n:::\n:::\n\n\nWhy is reproducibility and replicability important? In a thought provoking paper, @Ioannidis:2005 claimed that most research findings are wrong. The abstract of his paper stated\n\n> There is increasing concern that most current published research findings are false. [...] In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. \n\nSince its publication, collaborative efforts have tried to assess the scale of the problem by reanalysing data and trying to replicate the findings of  published research. For example, the \"Reproducibility [sic] Project: Psychology\" [@Nosek:2015]\n\n> conducted replications of 100 experimental and correlational studies published in three psychology journals using high powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety seven percent of original studies had significant results. Thirty six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and, if no bias in original results is assumed, combining original and replication results left 68% with significant effects. [...]\n\nA large share of findings in the review were not replicable or the effects were much smaller than claimed, as shown by [Figure 2 from the study](https://osf.io/447b3/).\nSuch findings show that the peer-review procedure is not foolproof: the \"publish-or-perish\" mindset in academia is leading many researchers to try and achieve statistical significance at all costs to meet the 5% level criterion, whether involuntarily or not. This problem has many names: $p$-hacking, harking or to paraphrase a [story of Jorge Luis Borges](https://en.wikipedia.org/wiki/The_Garden_of_Forking_Paths), the garden of forking paths. There are many degrees of freedom in the analysis for researchers to refine their hypothesis after viewing the data, conducting many unplanned comparisons and reporting selected results.\n\n\n::: {.cell layout-align=\"center\" hash='08-reproducibility_crisis_cache/html/fig-repropvaluescorr_ab117431ebe6f4b6f601bf5e58bc738c'}\n::: {.cell-output-display}\n![Figure 2 from @Nosek:2015, showing scatterplot of effect sizes for the original and the replication study by power, with rugs and density plots by significance at the 5% level.](figures/RPP_psycho_repro.png){#fig-repropvaluescorr fig-align='center' width=85%}\n:::\n:::\n\n\nAnother problem is selective reporting. Because a large emphasis is placed on statistical significance, many studies that find small effects are never published, resulting in a gap. @fig-reprozscores from @vanZwet:2021 shows $z$-scores obtained by transforming confidence intervals reported in @Barnett:2019. The authors used data mining techniques to extract confidence intervals from abstracts of nearly one million publication in Medline published between 1976 and 2019. If most experiments yielded no effect and were due to natural variability, the $z$-scores should be normally distributed, but @fig-reprozscores shows a big gap in the bell curve between approximately $-2$ and $2$, indicative of selective reporting. The fact that results that do not lead to $p < 0.05$ are not published is called the **file-drawer** problem.\n\n\n::: {.cell layout-align=\"center\" hash='08-reproducibility_crisis_cache/html/fig-reprozscores_3c1697b2601e6a7011a363f37107cbf8'}\n::: {.cell-output-display}\n![Figure from @vanZwet:2021 based on results of @Barnett:2019; histogram of $z$-scores from one million studies from Medline.](figures/vanZwet_Cator-zvalues.png){#fig-reprozscores fig-align='center' width=85%}\n:::\n:::\n\n\nThe ongoing debate surrounding the reproducibility crisis has sparked dramatic changes in the academic landscape: to enhance the quality of studies published, many journal now require authors to provide their code and data, to pre-register their studies, etc. Teams lead effort (e.g., the [Experimental Economics Replication Project](https://experimentaleconreplications.com/studies.html)) try to replicate studies, with mitigated success so far. This [inside recollection](https://devonprice.medium.com/questionable-research-practices-ive-taken-part-in-754b74dcaa51) by a graduate student shows the extent of the problem.\n\nThis course will place a strong emphasis on identifying and avoiding statistical fallacies and showcasing methods than enhance reproducibility. How can reproducible research enhance your work? For one thing, this workflow facilitates the publication of negative research, forces researchers to think ahead of time (and receive feedback). Reproducible research and data availability also leads to additional citations and increased credibility as a scientist.\n\nAmong good practices are\n\n- pre-registration of experiments and use of a logbook.\n- clear reporting of key aspects of the experiment (choice of metric, number of items in a Likert scale, etc.)\n- version control systems (e.g., Git) that track changes to files and records.\n- archival of raw data in a proper format with accompanying documentation.\n\n\nKeeping a logbook and documenting your progress helps your collaborators, reviewers and your future-self understand decisions which may seem unclear and arbitrary in the future, even if they were the result of a careful thought process at the time you made them. Given the pervasiveness of the garden of forking paths, pre-registration helps you prevents harking because it limits selective reporting and unplanned tests, but it is not a panacea. Critics often object to pre-registration claiming that it binds people. This is a misleading claim in my view: pre-registration doesn't mean that you must stick with the plan exactly, but merely requires you to explain what did not go as planned if anything.\n\nVersion control keeps records of changes to your file and can help you retrieve former versions if you make mistakes at some point.\n\n\n\n::: {.cell layout-align=\"center\" hash='08-reproducibility_crisis_cache/html/fig-reprotweetexcelgenes_64c7069a8f71b155166ed707b3944d27'}\n::: {.cell-output-display}\n![Tweet showing widespread problems related to unintentional changes to raw data by software.](figures/reproducibility.png){#fig-reprotweetexcelgenes fig-align='center' width=85%}\n:::\n:::\n\n\nArchival of data helps to avoid unintentional and irreversible manipulations of the original data, examples of which can have large scale consequences as illustrated in @fig-reprotweetexcelgenes [@Ziemann:2016], who report flaws in genetic journals due to the automatic conversion of gene names to dates in Excel. These problems are [far from unique](https://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england). While sensitive data cannot be shared \"as is\" because of confidentiality issues, in many instances the data can and should be made available with a licence and a DOI to allow people to reuse it, cite and credit your work.\n\n\nTo enforce reproducibility, many journals now have policy regarding data, material and code availability. Some journals encourage such, while the trend in recent years has been to enforce. For example, Nature require the following to be reported in all published papers:\n\n\n::: {.cell layout-align=\"center\" hash='08-reproducibility_crisis_cache/html/naturereportstat_347bf996e954fc6c7dd7f77c77be1d81'}\n::: {.cell-output-display}\n![Screenshot of the Nature Reporting summary for statistics, reproduced under the CC BY 4.0 license.](figures/Nature_reporting_statistics.png){fig-align='center' width=85%}\n:::\n:::\n\n\n## Causes of the replication crisis\n\nBelow are multiple (non-exclusive) explanations for the lack of replication of study findings. \n\n### The garden of forking paths\n\nThe garden of forking paths, named after a [novel of Borges](https://en.wikipedia.org/wiki/The_Garden_of_Forking_Paths), is a term coined by [Andrew Gelman](http://www.stat.columbia.edu/~gelman/research/unpublished/forking.pdf) to refer to researchers' degrees of freedom. With vague hypothesis and data collection rules, it is easy for the researcher to adapt and interpret the conclusions in a way that fits his or her chosen narratives. In the words of @Gelman.Loken:2014\n\n> Given a particular data set, it can seem entirely appropriate to look at the data and construct reasonable rules for data exclusion, coding, and analysis that can lead to statistical significance. In such a case, researchers need to perform only one test, but that test is conditional on the data.\n\nThis user case is not accomodated by classical testing theory. Research hypothesis are often formulated in a vague way, such that different analysis methods, tests may be compatible. [Abel et al. (2022) recent preprint](https://docs.iza.org/dp15476.pdf) found that preregistration alone did not solve this problem, but that publication bias in randomized control trial was alleviated by publication of pre-analysis plans. This is directly related to the garden of forking path.\n\n\n\n\n### Selective reporting\n\nAlso known as the file-drawer problem, selective reporting occurs because publication of results that fail to reach statistical significance (sic) are harder to publish. In much the same way as multiple testing, if 20 researchers perform a study but only one of them writes a paper and the result is a fluke, then this indicates. There are widespread indications publication bias, as evidence by the distribution of $p$-values reported in papers. A [recent preprint of a study](https://docs.iza.org/dp15478.pdf) found the prevalance to be higher in online experiments such as Amazon MTurks. \n\n\n_P_-hacking and the replication crisis has lead many leading statisticians to advocate much more stringent cutoff criterion such as $p < 0.001$ instead of the usual $p<0.05$ criterion as level for the test.\nThe level $\\alpha=5$\\% is essentially arbitrary and dates back to @Fisher:1926, who wrote\n\n> If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred. Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fails to reach this level. \n\n\n\n::: outsidethebox\n\nMethods that pool together results, such as meta-analysis, are sensitive to selective reporting. Why does it matter?\n\n:::\n\n\n\n### Non-representative samples\n\nMany researchers opt for convenience samples by using online panels such as Qualtrics, Amazon MTurks, etc. The quality of those observations is at best dubious: ask yourself whether you would answer such as survey for a small amount. Manipulation checks to ensure participants are following, information is not completed by bots, a threshold for the minimal time required to complete the study, etc. are necessary (but not sufficient) conditions to ensure that the data are not rubbish.\n\nA more important criticism is that the people who answer those surveys are not representative of the population as a whole: sampling bias thus plays an important role in the conclusions and, even if the summary statistics are not too different from the general population, they may exhibit different opinions, levels of skills, etc. than most.\n\n\n::: {.cell layout-align=\"center\" hash='08-reproducibility_crisis_cache/html/fig-samplingbias_940a94f253e7c139d78b9c57224557c0'}\n::: {.cell-output-display}\n![Sampling bias. Artwork by [Jonathan Hey (Sketchplanations)](https://sketchplanations.com/sampling-bias) shared under the [CC BY-NC 4.0 license](http://creativecommons.org/licenses/by-nc/4.0/).](figures/samplingbias.jpg){#fig-samplingbias fig-align='center' width=85%}\n:::\n:::\n\nThe same can be said of panels of students recruited in universities classes, who are more young, educated and perhaps may infer through backward induction the purpose of the study and answer accordingly.\n\n## Summary\n\nOperating in an open-science environment should be seen as an opportunity to make better science, offer more opportunities to increase your impact and increase the likelihood that your work gets published regardless of whether the results turn out to be negative. It is the *right thing* to do and it increases the quality of research produced, with collateral benefits because it forces researchers to validate their methodology before, to double-check their data and their analysis and to adopt good practice.\n\nThere are many platforms for preregistering studies and sharing preanalysis plans, scripts and data, with different level of formality. One such is the [Research Box](https://researchbox.org/).\n\n::: yourturn\n\nReflect on your workflow as applied researcher when designing and undertaking experiments. Which practical aspects could you improve upon to improve the reproducibility of your study?\n\n::: \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}