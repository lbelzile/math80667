# Statistical fallacies



## Multiple testing

If you do a single hypothesis test and the testing procedure is well calibrated (model assumptions met), there is a probability of $\alpha$ of making a type I error if the null is true, meaning when there is no difference between averages in the underlying population. The problem of the above approach is that the more you look, the higher the chance of finding something: with 20 independent tests, we expect that one of them will yield a $p$-value less than 5\%, for instance. This, coupled with the tendency in the many fields to dichotomize the result of every test depending on whether $p \leq \alpha$ (statistically significant at level $\alpha$ or not leads to selective reporting of findings. The level $\alpha=5$\% is essentially arbitrary: @Fisher:1926 wrote

> If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred. Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fails to reach this level. 

Not all tests are of interest, even if standard software will report all possible pairwise comparisons. If there are $K$ groups to compare and any comparison is of interest, than we could performs $\binom{K}{2}$ pairwise comparisons with $\mathscr{H}_{0}: \mu_i = \mu_j$ for $i \neq j$. For $K=3$, there are three such comparisons, 10 pairwise comparisons if $K=5$ and 45 pairwise comparisons if $K=10$. 

The number of tests performed in the course of an analysis can be very large. Y. Benjamini investigated the number of tests performed in each study of the Psychology replication project [@Nosek:2015]: this number ranged from 4 to 700, with an average of 72 per study --- most studies did not account for the fact they were performing multiple tests or selected the model. It is natural to ask then how many are spurious findings that correspond to type I errors. The paramount (absurd) illustration is the cartoon presented in Figure \@ref(fig:xkcdsignificant): note how there is little scientific backing for the theory (thus such test shouldn't be of interest to begin with) and likewise the selective reporting made of the conclusions, despite nuanced conclusions.

Why is this a problem? Assume for simplicity that all tests are independent^[This is the case if tests are based on different data, or if the contrasts considered are orthogonal under normality.] and that each test is conducted at level $\alpha$. The probability of making at least one type I error, say $\alpha^{\star}$, is^[The second line holds with independent observations, the second follows from the use of Boole's inequality and does not require independent tests.]
\begin{align}\alpha^{\star} &= 1 â€“ \text{probability of making no type I error} \\\ &= 1- (1-\alpha)^m\\
& \leq m\alpha
  (\#eq:bonferroni)
\end{align}

With $\alpha = 5$% and $m=4$ tests, $\alpha^{\star} \approx 0.185$ whereas for $m=72$ tests, $\alpha^{\star} \approx 0.975$: this means we are almost guaranteed even when nothing is going on to find "statistically significant" yet meaningless results.

```{r xkcdsignificant, fig.cap = "xkcd 882: Significant. The alt text is 'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!'", echo = FALSE, eval = TRUE}
knitr::include_graphics("figures/xkcd882_significant.png")  
```

It is sensible to try and reduce or bound the number of false positive. We consider a **family** of $m$ null hypothesis $\mathscr{H}_{01}, \ldots, \mathscr{H}_{0m}$ tested. The family may dependent on the context, but this comprises all hypothesis that are scientifically relevant and could be reported. These comparisons are called **pre-planned comparisons**: they should be chosen before the experiment takes place and pre-registered to avoid data dredging and selective reporting. The number of planned comparisons should be kept small relative to the number of parameters: for a one-way ANOVA, a general rule of thumb is to make no more comparisons than the number of groups, $K$.

Suppose that we perform $m$ hypothesis test in a study and define
\begin{align}
R_i &= \begin{cases} 1 & \text{if we reject }  \mathscr{H}_{0i} \\
0 & \text{if we fail to reject } \mathscr{H}_{0i}
\end{cases}\\
V_i &=\begin{cases} 1 & \text{type I error for } \mathscr{H}_{0i}\quad  (R_i=1 \text{ and  }\mathscr{H}_{0i} \text{ is true}) \\ 0 & \text{otherwise} 
\end{cases}
\end{align}
where $R=R_1 + \cdots + R_m$ is the total number of rejections ($0 \leq R \leq m$), and $V = V_1 + \cdots + V_m$ is the number of null hypothesis rejected by mistake ($0 \leq V \leq R$). 

The **familywise error rate** is the probability of making at least one type I error per family, 
\begin{align*}
\mathsf{FWER} = \Pr(V \geq 1).
\end{align*}
To control the familywise error rate, one must be more stringent in rejecting the null and perform each test with a smaller level so that the overall or simultaneous probability is less than $\mathsf{FWER}$.



### Bonferroni's procedure

The easiest way (and one of the least powerful option) is to directly use the inequality in eq. \@ref(eq:bonferroni). If each test is performed at level $\alpha/m$, than the family-wise error is controlled at level $\alpha$.



The Bonferroni adjustment also controls the **per-family error rate**, which is the expected (theoretical average) number of false positive $\mathsf{PFER} = \mathsf{E}(V)$. The latter is a more stringent criterion than the familywise error rate because $\Pr(V \geq 1) \leq \mathsf{E}(V)$, which does not make a distinction between having one or multiple type I errors.^[By definition, the expected number of false positive (PFER) is $\mathsf{E}(V) = \sum_{i=1}^m i \Pr(V=i) \geq \sum_{i=1}^m \Pr(V=i) = \Pr(V \geq 1)$, so larger than the probability of making at least type 1 error. Thus, any procedure that controls the per-family error rate (e.g., Bonferroni) also automatically bounds the familywise error rate.] 

Why is Bonferroni's procedure popular? It is conceptually easy to understand and simple, and it applies to any design and regardless of the dependence between the tests. However, the number of tests to adjust for, $m$, must be prespecified and the procedure leads to low power when the family is large. Moreover, if our sole objective is to control for the familywise error rate, then there are other procedures that are always better in the sense that they still control the $\mathsf{FWER}$ while leading to increased capacity of detection when the null is false.



If the raw (i.e., unadjusted) $p$-values are reported, we reject hypothesis $\mathscr{H}_{0i}$ if $m \times p_i \ge \alpha$: we multiply each $p$-value by $m$ and reject if the result exceeds $\alpha$.


### Holm-Bonferroni's procedure

The idea of Holm's procedure is to use a sharper inequality bound and amounts to performing tests at different levels, with more stringent for smaller $p$-values.

Order the $p$-values of the family of $m$ tests from smallest to largest
$p_{(1)} \leq \cdots \leq p_{(m)}$ and test sequentially the hypotheses. Coupling Holm's method with Bonferroni's procedure, we compare $p_{(1)}$ to $\alpha_{(1)} = \alpha/m$, $p_{(2)}$ to $\alpha_{(2)}=\alpha/(m-1)$, etc. 

If all of the $p$-values are less than their respective levels, than we still reject each null hypothesis. Otherwise, we reject all the tests whose $p$-values exceeds the smallest nonsignificant one. If $p_{(j)} \geq \alpha_{(j)}$ but $p_{(i)} \leq \alpha_{(i)}$ for $i=1, \ldots, j-1$ (all smaller $p$-values), we reject the associated hypothesis $\mathscr{H}_{0(1)}, \ldots, \mathscr{H}_{0(j-1)}$ but fail to reject $\mathscr{H}_{0(j)}, \ldots, \mathscr{H}_{0(m)}$.

This procedure doesn't control the per-family error rate, but is uniformly more powerful and thus leads to increased detection than Bonferroni's method.

To see this, consider a family of $m=3$ $p$-values with values $0.01$, $0.04$ and $0.02$. Bonferroni's adjustment would lead us to reject the second and third hypotheses at level $\alpha=0.05$, but not Holm-Bonferroni.


