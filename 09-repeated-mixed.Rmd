# Repeated measures

So far, all experiments we have considered can be classified as between-subject designs, meaning that each experimental unit was assigned to a single experimental (sub)-condition. In many instances, it may be possible to randomly assign multiple conditions to each experimental unit. For example, an individual coming to a lab to perform tasks in a virtual reality environment may be assigned to all treatments, the latter being presented in random order. There is an obvious benefit in doing so, as the participants can act as their own control group, leading to greater comparability among treatment conditions. For example, consider a study performed at Tech3Lab that looks at the reaction time for people texting or talking on a cellphone while walking. We may wish to determine whether disengagement is slower for people texting, yet we may also postulate that some elderly people have slower reflexes. By including multiple conditions, we can filter out effect due to subject, much like with blocking: this leads to increased precision of effect sizes and increased power (as we will see, hypothesis tests are based on within-subject variability). Together, this translates into the need to gather fewer observations or participants to detect a given effect in the population and thus experiments are cheaper to run.

There are of course drawbacks to gathering repeated measures from individuals. Because subjects are confronted with multiple tasks, there may be carryover effects (when one task influences the response of the subsequent ones, for example becoming more fluent as manipulations go on), period effects (practice of fatigue, e.g., leading to a decrease in accuity), permanent changes in the subject condition after a treatment or attrition (loss of subjects over time).

To minimize potential biases, there are multiple strategies one can use. One should randomize the order of treatment conditions among subjects to reduce confounding, or else use a balanced crossover design and include the period and carryover effect in the statistical model via control variables, so as to better isolate the treatment effect. The experimenter should also allow enough time between treatment conditions to reduce or eliminate period or carryover effects and plan tasks accordingly.

There are multiple approaches to handling repeated measures, the biggest difference being that observations are now correlated whereas we assumed measurements were independent until now. One can take averages over experimental condition per pation and treat them as additional blocking factors. One can fit a model that accounts for the multivariate nature of the response, using multivariate analysis of variance. The most popular and widely used method nowadays is to fit a mixed model, with random effects accounting to subject-specific characteristics. By doing so, we assume that the levels of a factor (here the subject identifiers) form a random sample from a large population.

### Data format

With repeated measures, it is sometimes convenient to store measurements associated to each experimental condition in different columns of a data frame or spreadsheet, with lines containing participants identifiers. Such data are said to be in wide format, since there are multiple measurements in each row. While this format is suitable for multivariate models, many statistical routines will instead expect data to be in **long format**. Figure \@ref(fig:fig-longvswide) shows the difference between the two format. All the databases in the `hecedsm` package are given in long format.


```{r}
#| label: fig-longvswide
#| fig-cap = "Long versus wide-format for data tables (illustration by Garrick Aden-Buie)."
#| echo: false
#| eval: true
knitr::include_graphics(path = "figures/original-dfs-tidy.png")
```

Ideally, a data base in long format with repeated measures would also include a column giving the order in which the treatments were assigned to participants. This is necessary in order to test whether there are fatigue or crossover effects, for example by plotting the residuals after accounting for treatment subject by subject, ordered over time. We could also perform formal tests by including time trends in the model and checking whether the slope is significant. 

## Random effects

There is no consensus as to what constitutes a random effect.

Gelman (2005) lists a handful of definitions:

> When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random [Green and Tukey (1960)].

> Effects are fixed if they are interesting in themselves or random if there is interest in the underlying population (e.g., Searle, Casella and McCulloch [(1992), Section 1.4])

In a between-subjects design, subjects are **nested** within condition/treatment. In a within-subjects designs, experimental factors and subjects are crossed.


To fix ideas, we consider a one-way ANOVA with a random effect. As before, we have one experimental factor $A$ with $n_a$ levels, which we write
$$\begin{align*}\underset{\text{response}\vphantom{l}}{Y_{ij}} = \underset{\text{global mean}}{\mu_{\vphantom{j}}} + \underset{\text{mean difference}}{\alpha_j} + \underset{\text{random effect for subject}}{S_{i\vphantom{j}}} + \underset{\text{error}\vphantom{l}}{\varepsilon_{ij}}\end{align*}$$
where we take $S_i \sim \mathsf{No}(0, \sigma^2_s)$ and $\varepsilon_{ij} \sim \mathsf{No}(0, \sigma^2_e)$ are random variables. We assume that the errors and random effects are independent from one another. The model **parameters** are $\mu$, $\alpha_1, \ldots, \alpha_{n_a}$, $\sigma^2_s$ and $\sigma^2_e$ with the sum-to-zero constraint $\alpha_1 + \cdots + \alpha_{n_a}=0$.

Mixed models include, by definition, both random and fixed effects. Fixed effects are model parameters corresponding to overall average or difference in means for the experimental conditions. These are the terms for which we want to perform hypothesis tests and compute contrasts. Random effects, on the other hand, are typically used to model dependence (clusters), since observations from these factors will be correlated, and capture the variability arising from different units. 

Inclusion of random effects introduces positive correlation between measurements: specifically, the correlation between two observations from the same subject will be $\sigma^2_s/(\sigma^2_s+\sigma^2_e)$ and zero otherwise. This correlation structure is termed compound symmetry, since the correlation between measurements is the same regardless of the order of the observations. If there are multiple random effects, the structure will be more complicated.

Correlation has an important impact on testing, as was observed in Subsection \@ref(independence): failing to account for correlation leads to $p$-values that are much too low. To see why, think about a stupid setting under which we duplicate every observation in the database: the estimated marginal means will be the same, but the variance will be halved despite the fact there is no additional information. Intuitively, correlation reduces the amount of information provided by each individual: if we have repeated measures from participants, we expect the effective sample size to be anywhere between the total number of subjects and the total number of observations.

What is the purpose of the random effect? For one, it will allow us to predict the individual differences; with fixed effects and blocking, we would estimate the average for each subject despite the fact that this quantity is of no interest. Random effects also shrink effects toward the global mean and so these regularize estimation, the effect being more important when there are few measurements per person and estimated effects for subjects are based on a handful of measurements. 


:::{ .example name="Happy fakes"}
We consider an experiment conducted in a graduate course at HEC, *Information Technologies and Neuroscience*, in which PhD students gathered electroencephalography (EEG) data. The project focused on human perception of deepfake image created by a generative adversarial network: Amirabdolahian and Ali-Adeeb (2021) expected the attitude towards real and computer generated image of people smiling to change.

The response variable is the amplitude of a brain signal measured at 170 ms after the participant has been exposed to different faces. Repeated measures were collected on 9 participants given in the database `AA21`, who were expected to look at 120 faces. Not all participants completed the full trial, as can be checked by looking at the cross-tabs of the counts

```{r}
data(AA21, package = "hecedsm")
xtabs(~stimulus + id, data = AA21)
```

The experimental manipulation is encoded in the `stimuli`, with levels control (`R`) for real facial images, whereas the others were generated using a generative adversarial network (GAN) with be slightly smiling (`GAN1`) or extremely smiling (`GAN2`); the latter looks more fake. While the presentation order was randomized, the order of presentation of the faces within each type is recorded using the `epoch` variable: this allows us to measure the fatigue effect.

Since our research question is whether images generated from generative adversarial networks trigger different reactions, we will be looking at pairwise differences with the control.

```{r}
#| eval: true
#| echo: false
#| label: fig-GANfaces
#| fig-cap: "Example of faces presented in Amirabdolahian and Ali-Adeeb (2021): real, slightly modified and extremely modified (from left to right)."
knitr::include_graphics(c("figures/face_real.jpg", 
                          "figures/face_GAN_S.jpg",
                          "figures/face_GAN_E.jpg"))
```

We could begin by grouping the data and computing the average for each experimental condition `stimulus` per participant and set `id` as blocking factor. The analysis of variance table obtained from `aov` would be correct, but fails to account for correlation. Unfortunately, the 

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false
AA21_m <- AA21 |>
  dplyr::group_by(id, stimulus) |>
  dplyr::summarize(latency = mean(latency))
fixedmod <- aov(latency ~ stimulus + Error(id), data = AA21_m)
summary(fixedmod)
# Random intercept for participant
mixedmod <- lme4::lmer(latency ~ stimulus + (1 | id), 
               data = AA21_m)
car::Anova(mixedmod, test = "F", type = 3)
```

Since the design is balanced after averaging, we can use `aov` in **R**. This approach has a drawback, as variance components can be negative. While `aov` is fast, it only works for simple balanced designs. 
If we use `aov`, we need to specify the subject id within `Error` term. 

To fit one within-subject analysis of variance model, which is a special case of linear mixed model, we can use the `lmer` function in the `lme4` package. The random effect (a form of random intercept) will be specified using  `(1 | id)` to specify that the data from each participant are correlated and include conditionally for each participant. Note that the $p$-values are the same in both cases.

We could try to keep all observations instead: this gives better evidence of some differences between the groups, although there is too much background noise to really distinguish differences between experimental conditions. We will come back to this design later.


:::

In mixed models, it is essential to determine the correct structure: identifiers of subjects must be declared as factors and the user must pay attention to random terms and how the models are fitted. Unfortunately, this requires some good knowledge of the machinery behind the tests and the model specification. Performing sanity checks to ensure the means and degrees of freedom match the intuition is necessary.

With balanced data, the estimated marginal means coincide with the row averages. If we have a single replication or the average for each subject/condition, we could create a new column with the contrast and then fit a model with an intercept-only (global mean) to check whether the latter is zero. With 12 participants, we should thus expect our test statistic to have 11 degrees of freedom, since one unit is spent on estimating the mean parameter.

Unfortunately, the `emmeans` package analysis for the mixed model or object fitted using `aov` will be incorrect. The `afex` package includes functionalities that are tailored for within-subject and between-subjects and has an interface with `emmeans`.

```{r}
afexmod <- afex::aov_car( 
  latency ~ stimulus + Error(id/stimulus),
  data = AA21_m)
afexmod <- afex::aov_ez(
  id = "id",           # subject id
  dv = "latency",      # response variable
  within = "stimulus", # within-subject factor
  data = AA21,
  fun_aggregate = mean)
# Correct output
afexmod |>
  emmeans(spec = "stimulus") |> 
  contrast(method = list("real vs GAN" = c(1, -0.5, -0.5)))
# Incorrect output - note the degrees of freedom
fixedmod |> 
  emmeans(spec = "stimulus") |> 
  contrast(method = list("real vs GAN" = c(1, -0.5, -0.5)))
```


```{r}
options(contrasts = c("contr.sum", "contr.poly"))
mixedmod <- lme4::lmer(latency ~ stimuli + (1 | id) + (1 |id:stimuli), 
               data = AA21)
car::Anova(mixedmod, test = "Chisq", type = 3)
mixedmod |> emmeans::emmeans(specs = "stimuli")
```


```{r}
ggplot(data = hecedsm::AA21,
       aes(x = id,
           group = stimulus,
           colour = stimulus,
           y = latency)) +
    geom_point() + theme_classic()
```

- No detectable difference between conditions.
- The _p_-value (0.782) for the mixed model is the same as `aov`. 
- Residual degrees of freedom is $(a-1) \times (n-1)=18$ for $n=9$ subjects and $a=3$ levels.
]
]
.panel[.panel-name[QQ plots]
.pull-left[
```{r qqplot1, echo = FALSE, eval = TRUE, out.width = '90%',fig.width = 5, fig.height = 5, fig.asp = 1, fig.retina = 3}
car::qqPlot(as.vector(unlist(lme4::ranef(model)$id)), xlab = "theoretical normal quantiles", ylab = "random effects", id = FALSE)
```
]
.pull-right[
```{r qqplot2, echo = FALSE, eval = TRUE, out.width = '90%',fig.width = 5, fig.height = 5, fig.asp = 1, fig.retina = 3}
car::qqPlot(as.vector(resid(model)), xlab = "theoretical normal quantiles", ylab = "residuals", id = FALSE)
```
]
]
]
---

class: title title-7
# Model assumptions

The validity of the $F$ null distribution relies on the model having the correct structure.

- Same variance per observation
- equal correlation between measurements of the same subject
- normality of the random effect


- Since we care only about differences in treatment, can get away with a weaker assumption than compound symmetry
   - *sphericity*: variance of difference between treatment is constant

---
class: title title-7
# Testing for sphericity

Popular two-stage approach:

- Mauchly's test of sphericity
   - if statistically significant, use a correction
   - if no evidence, proceed with $F$ test as usual
   
---
class: title title-7
# Corrections for sphericity

An idea due to Box is to correct the degrees of freedom from $\mathsf{F}\{a-1, (a-1)(n-1)\}$ to $\mathsf{F}\{\epsilon(a-1), \epsilon(a-1)(n-1)\}$ for $\epsilon < 1$

- Since the statistic is a ratio, it is unaffected
- Three widely used corrections:
   - Greenhouse-Geisser 
   - Huynh-Feldt (more powerful, but can be larger than 1 - cap)
   - lower bound with $\epsilon = (a-1)^{-1}$, giving $\mathsf{F}(1, n-1)$.

Another option is to go fully multivariate.

---
layout: false
name: mixed-models
class: center middle section-title section-title-6

# Mixed models

---
class: title title-6
# Generalization

Using mixed models in place of *old school* ANOVA has benefits in that it's easier to account for complex designs.

In general, things are not obvious

- Estimation via restricted maximum likelihood
- Theory for testing is more complicated 
   - $F$-tests via Kenward-Rogers (best, but costly) or Satterthwaite approximation
   - Determining the degrees of freedom is not always trivial (Hass diagrams)
- For more layers, need replications to estimate variability (estimability/identifiability)


---

class: title title-6
# Nested versus crossed

.pull-left-wide[
Nested effects if a factor appears only within a particular level of another factor.

Crossed is for everything else (typically combinations of all factors).
]
.pull-right-narrow[
![Russian dolls](img/10/matroshka.jpg)
]

.small[

Example of nested random effects: class nested within schools 
- class 1 is not the same in school 1 than in school 2
```{r out.width = '70%', eval = TRUE, echo = FALSE}
knitr::include_graphics("img/10/nested.png")
```

]

???

Matroschka from Wikimedia Commons CC-BY-SA 3.0
https://en.wikipedia.org/wiki/Matryoshka_doll#/media/File:Matryoshka_transparent.png
---
class: title title-6
# Formulae for nested effects

**R** uses the following notation for nested effect: `group1/group2`, to mean `group2` is nested within `group1`. 
This formula expands to `group1 + group1:group2`

For crossed effects, use rather `group1*group2` which expands to `group1 + group2 + group1:group2`.

In package `lme4`, a random intercept per group is written `(1 | group1/group2)`.

