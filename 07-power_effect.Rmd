# Effect sizes and power

In social studies, it is common to write a paper containing multiple studies on a similar topic. These may use different designs, with varying sample size. If the studies uses different questionnaires, or change the Likert scale, the results and the mean difference between groups are not directly comparable between experiments.

Another related task is replication of study, whereby researchers use the same material and setting and re-run an experiment with different data. For the replication to be somewhat successful (or at least reliable), one needs to determine beforehand how many participants should be recruited in the study. In order to do so, one needs to have a measure of effect size and set the desired power.

We could think for an example of comparing statistics or $p$-values, which are by construction standardized unitless measures, making them comparable across study.
Test statistics show how outlying observed differences between experimental conditions relative to a null hypothesis, typically that of no effect (equal mean in each subgroup). However, statistics are usually a function of both the sample size (the number of observations in each experimental condition) and the effect size (how large the standardized differences between groups are), making them unsuitable for describing differences.



```{r}
#| label: fig-effectsize
#| cache: true
#| echo: false
#| fig-width: 11
#| fig-height: 5
#| out-width: '90%'
#| fig-cap: "True sampling distribution for a two-sample $t$-test under the alternative (rightmost curve) and null distribution (leftmost curve) with small  (left panel) and large (right panel) sample sizes."
region <- data.frame(start = c(-Inf, qnorm(0.025, sd = 2), qnorm(0.975, sd = 2)),
                     end = c(qnorm(0.025, sd = 2), qnorm(0.975, sd = 2), Inf),
                     region = factor(c("reject","fail to reject","reject")))
p1 <- ggplot(region) +
  geom_rect(aes(xmin = start, xmax = end, fill = region),
            ymin = -Inf, ymax = Inf, alpha = 0.2, data = region) +
  scale_fill_manual(values = c("blue","red")) +
  coord_cartesian(xlim = c(-6,10), ylim = c(0, 0.46), expand = FALSE) +
  geom_vline(xintercept = c(0,3), alpha = 0.1) +
  stat_function(fun = dnorm, args = list(mean = 3, sd = 2), xlim = c(qnorm(0.975, sd = 2), 10),
                geom = "area", fill = "white") +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 2), xlim = c(-6,10)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 3, sd = 2), lty = 2, xlim = c(-6,10)) +
  ylab("density") +
  geom_segment(data = data.frame(x = 0, 
                                 xend = 3, 
                                 y = 0.45, 
                                 yend = 0.45), 
               mapping = aes(x = x, 
                             xend = xend, 
                             y = y, 
                             yend = yend),
               arrow =  arrow(ends = "both",
                              length = unit(0.1, "cm"))) + 
  theme_classic()

region1 <- data.frame(start = c(-Inf, qnorm(0.025), qnorm(0.975)),
                      end = c(qnorm(0.025), qnorm(0.975), Inf),
                      region = factor(c("reject","fail to reject","reject")))
p2 <- ggplot(region1) +
  geom_rect(aes(xmin = start, xmax = end, fill = region),
            ymin = -Inf, ymax = Inf, alpha = 0.2, data = region1) +
  scale_fill_manual(values = c("blue","red")) +
  coord_cartesian(xlim = c(-6,10), ylim = c(0, 0.46), expand = FALSE) +
  stat_function(fun = dnorm, args = list(mean = 3, sd = 1), xlim = c(qnorm(0.975),10),
                geom = "area", fill = "white") +
  ylab("density") +
  geom_vline(xintercept = c(0,3), alpha = 0.1) +
  stat_function(fun = dnorm, args = list(mean = 3, sd = 1), xlim = c(-5, 10), n = 1000) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), lty = 2, xlim = c(-5,10)) +
  geom_segment(data = data.frame(x = 0, 
                                 xend = 3, 
                                 y = 0.45, 
                                 yend = 0.45), 
               mapping = aes(x = x, 
                             xend = xend, 
                             y = y, 
                             yend = yend),
               arrow =  arrow(ends = "both",
                              length = unit(0.1, "cm"))) + 
  theme_classic()
p1 + p2 + plot_layout(guides = 'collect') & theme(legend.position = 'bottom')
```


 Figure \@ref(fig:fig-effectsize) shows an example with the sampling distributions of the difference in mean under the null (curve centered at zero) and the true alternative (mean difference of two).  The area in white under the curve represents the power, which is larger with larger sample size and coincides with smaller average $p$-values for the testing procedure. 
 
 One could argue that, on the surface, every null hypothesis is wrong and that, with a sufficiently large number of observation, all observed differences eventually become "statistically significant". This has to do with the fact that we become more and more certain of the estimated means of each experimental sub-condition. Statistical significance of a testing procedure does not translate into practical relevance, which itself depends on the scientific question at hand.
For example, consider the development of a new drug for commercialization by Health Canada: what is the minimum difference between two treatments that would be large enough to justify commercialization of the new drug? If the effect is small but it leads to many lives saved, would it still be relevant? Such decision involve a trade-off between efficacy of new treatment relative to the status quo, the cost of the drug, the magnitude of the improvement, etc.


Effect size are summaries to inform about the standardized magnitude of these differences; they are used to combine results of multiple experiments using meta-analysis, or to calculate sample size requirements to replicate an effect in power studies.


## Effect sizes

There are two main classes of effect size: standardized mean differences and ratio (percentages) of explained variance. The latter are used in analysis of variance when there are multiple groups to compare.

Unfortunately, the literature on effect size is quite large. Researchers often fail to distinguish between estimand (unknown target) and the estimator that is being used, with frequent notational confusion arising due to conflicting standards and definitions. Terms are also overloaded: the same notation may be used to denote an effect size, but it will be calculated differently depending on whether the design is between-subject or within-subject (with repeated correlated measures per participant), or whether there are blocking factors.

### Standardized mean differences

To gather intuition, we begin with the task of comparing the means of two groups using a two-sample $t$-test, with the null hypothesis of equality in means or $\mathscr{H}_0: \mu_1 = \mu_2$. The test statistic is 
\begin{align*}
T =  \frac{\widehat{\mu}_2 - \widehat{\mu}_1}{\widehat{\sigma}} \left(\frac{1}{n_1}+\frac{1}{n_2}\right)^{-1/2}
\end{align*}
where $\widehat{\sigma}$ is the pooled sample size estimator. The first term, $\widehat{d}_s = (\widehat{\mu}_2 - \widehat{\mu}_1)/\widehat{\sigma}$, is termed Cohen's $d$ [@Cohen:1988] and it measures the standardized difference between groups, a form of signal-to-noise ratio. As the sample size gets larger and larger, the sample mean and pooled sample variance become closer and closer to the true population values $\mu_1$, $\mu_2$ and $\sigma$; at the same time, the statistic $T$ becomes bigger as $n$ becomes larger because of the second term.^[If we consider a balanced sample, $n_1 = n_2 = n/2$ we can rewrite the statistic as $T = \sqrt{n} \widehat{d}_s/2$ and the statement that $T$ increases with $n$ on average becomes more obvious.] 

The difference $d=(\mu_1-\mu_2)/\sigma$ has an obvious interpretation: a distance of $a$ indicates that the means of the two groups are $a$ standard deviation apart. Cohen's $d$ is sometimes loosely categorized in terms of weak ($d = 0.2$), medium ($d=0.5$) and large ($d=0.8$) effect size; these, much like arbitrary $p$-value cutoffs, are rules of thumbs. Alongside $d$, there are many commonly reported metrics that are simple transformations of $d$ describing the observed difference. This interactive  [applet](https://rpsychologist.com/cohend/) by Kristoffer Magnusson [@magnussonCohend] shows the visual impact of changing the value of $d$ along.
There are different estimators of $d$ depending on whether or not the pooled variance estimator is used. Cohen's $d$, is upward biased, meaning it gives values that are on average larger than the truth. Hedge's $g$ [@Hedges:1981] offers a bias-correction and should always be preferred as an estimator.

For these different estimators, it is possible to obtain (asymmetric) confidence intervals or tolerance intervals.[By using the pivot method [@Steiger:2004] and relating the effect size to the noncentrality parameter of the null distribution, whether $\mathsf{St}$, $\mathsf{F}$ or $\chi^2$.]


::: {.example #LiuRimMinMin2022E1effect name="The Surprise of Reaching Out"}


```{r}
#| eval: true
#| echo: false
data(LRMM22_S1, package = "hecedsm")
ttest <- t.test(appreciation ~ role, 
                data = LRMM22_S1,
                var.equal = TRUE)
effect <- effectsize::hedges_g(appreciation ~ role, 
                data = LRMM22_S1, pooled_sd = TRUE)
cles <- effectsize::d_to_cles(effect)
```


We consider a two-sample $t$-test for the study of @Liu.Rim.Min.Min:2022 discussed in Example \@ref(exm:LiuRimMinMin2022E1). The difference in average response index is `r round(as.numeric(diff(ttest$estimate)), 3)`, indicating that the responder have a higher score. The $p$-value is `r round(ttest$p.value,3)`, showing a small effect. 

If we consider the standardized difference $d$, the group means are `r round(effect$Hedges_g, 3)` standard deviations apart based on Hedge's $g$, with an associated 95% confidence interval of [`r round(effect$CI_low, 3)`, `r round(effect$CI_high, 3)`]: thus, the difference found is small (using @Cohen:1988's convention) and there is a large uncertainty surrounding it. 

There is a `r round(cles$Coefficient[1]*100,0)`% probability that an observation drawn at random from the responder condition will exceed the mean of the initiator group (probability of superiority) and `r round(cles$Coefficient[1]*100,1)`% of the responder observations will exceed the mean of the initiator.

```{r}
#| eval: false
#| echo: true
data(LRMM22_S1, package = "hecedsm")
ttest <- t.test(
  appreciation ~ role, 
  data = LRMM22_S1,
  var.equal = TRUE)
effect <- effectsize::hedges_g(
  appreciation ~ role, 
  data = LRMM22_S1, 
  pooled_sd = TRUE)
effectsize::d_to_cles(effect)
```

:::






## Power


To calculate the power of a test, we need to single out a specific alternative hypothesis. In very special case, analytic derivations are possible. For a given alternative, we 

- simulate repeatedly samples from the model from the hypothetical alternative world
- we compute the test statistic for each of these new samples
- we transform these to the associated *p*-values based on the postulated null hypothesis.

At the end, we calculate the proportion of tests that lead to a rejection of the null hypothesis at level $\alpha$, namely the percentage of *p*-values smaller than $\alpha$.



:::keyidea

**Summary**:

* Effect sizes are used to provide a standardized measure of the strength of a result, independent of the design and the sample size.
* There are two classes: standardized differences and proportions of variance.
* Multiple estimators exists: report the latter along with the software used to compute confidence intervals.
* The adequate measure of variability to use for the effect size depends on the design: we normally include the variability of blocking factors and residual variance.
* Given a design, we can deduce either the sample size, the power or the effect size from the other two metrics. This allows us to compute sample size for a study or replication.

:::

