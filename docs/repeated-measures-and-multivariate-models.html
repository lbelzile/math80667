<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>9 Repeated measures and multivariate models | Experimental Design and Statistical Methods</title>
<meta name="author" content="LÃ©o Belzile">
<meta name="description" content="So far, all experiments we have considered can be classified as between-subject designs, meaning that each experimental unit was assigned to a single experimental (sub)-condition. In many...">
<meta name="generator" content="bookdown 0.30 with bs4_book()">
<meta property="og:title" content="9 Repeated measures and multivariate models | Experimental Design and Statistical Methods">
<meta property="og:type" content="book">
<meta property="og:description" content="So far, all experiments we have considered can be classified as between-subject designs, meaning that each experimental unit was assigned to a single experimental (sub)-condition. In many...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="9 Repeated measures and multivariate models | Experimental Design and Statistical Methods">
<meta name="twitter:description" content="So far, all experiments we have considered can be classified as between-subject designs, meaning that each experimental unit was assigned to a single experimental (sub)-condition. In many...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.1/transition.js"></script><script src="libs/bs3compat-0.4.1/tabs.js"></script><script src="libs/bs3compat-0.4.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Experimental Design and Statistical Methods</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Experimental Design and Statistical Methods</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">2</span> Hypothesis testing</a></li>
<li><a class="" href="CRT.html"><span class="header-section-number">3</span> Completely randomized designs</a></li>
<li><a class="" href="contrasts-multiple-testing.html"><span class="header-section-number">4</span> Contrasts and multiple testing</a></li>
<li><a class="" href="complete-factorial-designs.html"><span class="header-section-number">5</span> Complete factorial designs</a></li>
<li><a class="" href="designs-to-reduce-the-error.html"><span class="header-section-number">6</span> Designs to reduce the error</a></li>
<li><a class="" href="effect-sizes-and-power.html"><span class="header-section-number">7</span> Effect sizes and power</a></li>
<li><a class="" href="replication-crisis.html"><span class="header-section-number">8</span> Replication crisis</a></li>
<li><a class="active" href="repeated-measures-and-multivariate-models.html"><span class="header-section-number">9</span> Repeated measures and multivariate models</a></li>
<li><a class="" href="introduction-to-mixed-models.html"><span class="header-section-number">10</span> Introduction to mixed models</a></li>
<li><a class="" href="causal-inference.html"><span class="header-section-number">11</span> Causal inference</a></li>
<li><a class="" href="nonparametric-tests.html"><span class="header-section-number">12</span> Nonparametric tests</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/lbelzile/math80667a">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="repeated-measures-and-multivariate-models" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Repeated measures and multivariate models<a class="anchor" aria-label="anchor" href="#repeated-measures-and-multivariate-models"><i class="fas fa-link"></i></a>
</h1>
<p>So far, all experiments we have considered can be classified as between-subject designs, meaning that each experimental unit was assigned to a single experimental (sub)-condition. In many instances, it may be possible to randomly assign multiple conditions to each experimental unit. For example, an individual coming to a lab to perform tasks in a virtual reality environment may be assigned to all treatments, the latter being presented in random order to avoid confounding. There is an obvious benefit to doing so, as the participants can act as their own control group, leading to greater comparability among treatment conditions.</p>
<p>For example, consider a study performed at Tech3Lab that looks at the reaction time for people texting or talking on a cellphone while walking. We may wish to determine whether disengagement is slower for people texting, yet we may also postulate that some elderly people have slower reflexes.</p>
<p>In a between-subjects design, subjects are <strong>nested</strong> within experimental condition, as a subject can only be assigned a single treatment. In a within-subjects designs, experimental factors and subjects are <strong>crossed</strong>: it is possible to observed all combination of subject and experimental conditions.</p>
<p>By including multiple conditions, we can filter out effect due to subject, much like with blocking: this leads to increased precision of effect sizes and increased power (as we will see, hypothesis tests are based on within-subject variability). Together, this translates into the need to gather fewer observations or participants to detect a given effect in the population and thus experiments are cheaper to run.</p>
<p>There are of course drawbacks to gathering repeated measures from individuals. Because subjects are confronted with multiple tasks, there may be carryover effects (when one task influences the response of the subsequent ones, for example becoming more fluent as manipulations go on), period effects (practice of fatigue, e.g., leading to a decrease in acuity), permanent changes in the subject condition after a treatment or attrition (loss of subjects over time).</p>
<p>To minimize potential biases, there are multiple strategies one can use. While can randomize the order of treatment conditions among subjects to reduce confounding, or use a balanced crossover design and include the period and carryover effect in the statistical model via control variables so as to better isolate the treatment effect. The experimenter should also allow enough time between treatment conditions to reduce or eliminate period or carryover effects and plan tasks accordingly.</p>
<p>Due to fatigue or learning effects, randomization of the order of the within-subject experimental conditions. If each is assigned a single time, one good way to do this is via <strong>counterbalancing</strong>. We proceed as follows: first, enumerate all possible orders of the condition and then assign participants as equally as possible between conditions. For example, with a single within-factor design with three conditions <span class="math inline">\(A, B, C\)</span>, we have six possible orderings (either <span class="math inline">\(ABC\)</span>, <span class="math inline">\(ACB\)</span>, <span class="math inline">\(BAC\)</span>, <span class="math inline">\(BCA\)</span>, <span class="math inline">\(CAB\)</span> or <span class="math inline">\(CBA\)</span>). Much like other forms of randomization, this helps us remove confounding effects and letâs us estimate what is the average effect of task ordering on the response.</p>
<p>There are multiple approaches to handling repeated measures. The first option is to take averages over experimental condition per subject and treat them as additional blocking factors, but it may be necessary to adjust the resulting statistics. The second approach consists in fitting a multivariate model for the response and explicitly account for the correlation, otherwise the null distribution commonly used are off and so are the conclusions, as illustrated with the absurd comic displayed in Figure <a href="hypothesis-testing.html#fig:xkcd2569">2.1</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:xkcd2569"></span>
<img src="figures/xkcd2533-slope_hypothesis_testing.png" alt="xkcd comic [2533 (Slope Hypothesis Testing) by Randall Munroe](https://xkcd.com/2533/). Alt text: `What? I can't hear--` `I said, are you sure--`; `CAN YOU PLEASE SPEAK--`. Cartoon reprinted under the [CC BY-NC 2.5 license](https://creativecommons.org/licenses/by-nc/2.5/)." width="60%"><p class="caption">
Figure 2.1: xkcd comic <a href="https://xkcd.com/2533/">2533 (Slope Hypothesis Testing) by Randall Munroe</a>. Alt text: <code>What? I can't hear--</code> <code>I said, are you sure--</code>; <code>CAN YOU PLEASE SPEAK--</code>. Cartoon reprinted under the <a href="https://creativecommons.org/licenses/by-nc/2.5/">CC BY-NC 2.5 license</a>.
</p>
</div>
<p>Multivariate analysis of variance (MANOVA) leads to procedures that are analogous to univariate analysis of variance, but we now need to estimate correlation and variance parameters for each measurement separately and there are multiple potential statistics that can be defined for testing effects. While we can benefit from the correlation and find differences that wouldnât be detected from univariate models, the additional parameters to estimate lead to a loss of power. Finally, the most popular method nowadays for handling repeated measures is to fit a mixed model, with random effects accounting to subject-specific characteristics. By doing so, we assume that the levels of a factor (here the subject identifiers) form a random sample from a large population. These models can be difficult to fit and one needs to take great care in specifying the model.</p>
<div id="repeated-measures" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Repeated measures<a class="anchor" aria-label="anchor" href="#repeated-measures"><i class="fas fa-link"></i></a>
</h2>
<p>We introduce the concept of repeated measure and within-subject ANOVA with an example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-23" class="example"><strong>Example 9.1  (Happy fakes) </strong></span>We consider an experiment conducted in a graduate course at HEC, <em>Information Technologies and Neuroscience</em>, in which PhD students gathered electroencephalography (EEG) data. The project focused on human perception of deepfake image created by a generative adversarial network: Amirabdolahian and Ali-Adeeb (2021) expected the attitude towards real and computer generated image of people smiling to change.</p>
<p>The response variable is the amplitude of a brain signal measured at 170 ms after the participant has been exposed to different faces. Repeated measures were collected on 9 participants given in the database <code>AA21</code>, who were expected to look at 120 faces. Not all participants completed the full trial, as can be checked by looking at the cross-tabs of the counts</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">AA21</span>, package <span class="op">=</span> <span class="st">"hecedsm"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html">xtabs</a></span><span class="op">(</span><span class="op">~</span><span class="va">stimulus</span> <span class="op">+</span> <span class="va">id</span>, data <span class="op">=</span> <span class="va">AA21</span><span class="op">)</span></span>
<span><span class="co">#&gt;         id</span></span>
<span><span class="co">#&gt; stimulus  1  2  3  4  5  6  7  8  9 10 11 12</span></span>
<span><span class="co">#&gt;     real 30 32 34 32 38 29 36 36 40 30 39 33</span></span>
<span><span class="co">#&gt;     GAN1 32 31 40 33 38 29 39 31 39 28 35 34</span></span>
<span><span class="co">#&gt;     GAN2 31 33 37 34 38 29 34 36 40 33 35 32</span></span></code></pre></div>
<p>The experimental manipulation is encoded in the <code>stimuli</code>, with levels control (<code>real</code>) for real facial images, whereas the others were generated using a generative adversarial network (GAN) with be slightly smiling (<code>GAN1</code>) or extremely smiling (<code>GAN2</code>); the latter looks more fake. While the presentation order was randomized, the order of presentation of the faces within each type is recorded using the <code>epoch</code> variable: this allows us to measure the fatigue effect.</p>
<p>Since our research question is whether images generated from generative adversarial networks trigger different reactions, we will be looking at pairwise differences with the control.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fig-GANfaces"></span>
<img src="figures/face_real.png" alt="Example of faces presented in Amirabdolahian and Ali-Adeeb (2021): real, slightly modified and extremely modified (from left to right)." width="25%"><img src="figures/face_GAN_S.png" alt="Example of faces presented in Amirabdolahian and Ali-Adeeb (2021): real, slightly modified and extremely modified (from left to right)." width="25%"><img src="figures/face_GAN_E.jpg" alt="Example of faces presented in Amirabdolahian and Ali-Adeeb (2021): real, slightly modified and extremely modified (from left to right)." width="25%"><p class="caption">
Figure 9.1: Example of faces presented in Amirabdolahian and Ali-Adeeb (2021): real, slightly modified and extremely modified (from left to right).
</p>
</div>
<p>We could begin by grouping the data and computing the average for each experimental condition <code>stimulus</code> per participant and set <code>id</code> as blocking factor. The analysis of variance table obtained from <code>aov</code> would be correct, but fails to account for correlation.</p>
<p>The one-way analysis of variance with <span class="math inline">\(n_s\)</span> subjects, each of which was exposed to the <span class="math inline">\(n_a\)</span> experimental conditions, can be written
<span class="math display">\[\begin{align*}\underset{\text{response}\vphantom{l}}{Y_{ij}} = \underset{\text{global mean}}{\mu_{\vphantom{j}}} + \underset{\text{mean difference}}{\alpha_j} + \underset{\text{subject difference}}{s_{i\vphantom{j}}} + \underset{\text{error}\vphantom{l}}{\varepsilon_{ij}}\end{align*}\]</span></p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute mean for each subject + </span></span>
<span><span class="co"># experimental condition subgroup</span></span>
<span><span class="va">AA21_m</span> <span class="op">&lt;-</span> <span class="va">AA21</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">id</span>, <span class="va">stimulus</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>latency <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">latency</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Use aov for balanced sample</span></span>
<span><span class="va">fixedmod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/aov.html">aov</a></span><span class="op">(</span></span>
<span>  <span class="va">latency</span> <span class="op">~</span> <span class="va">stimulus</span> <span class="op">+</span> <span class="fu">Error</span><span class="op">(</span><span class="va">id</span><span class="op">/</span><span class="va">stimulus</span><span class="op">)</span>, </span>
<span>  data <span class="op">=</span> <span class="va">AA21_m</span><span class="op">)</span></span>
<span><span class="co"># Print ANOVA table</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fixedmod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Error: id</span></span>
<span><span class="co">#&gt;           Df Sum Sq Mean Sq F value Pr(&gt;F)</span></span>
<span><span class="co">#&gt; Residuals 11    188    17.1               </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Error: id:stimulus</span></span>
<span><span class="co">#&gt;           Df Sum Sq Mean Sq F value Pr(&gt;F)</span></span>
<span><span class="co">#&gt; stimulus   2    1.9    0.97     0.5   0.62</span></span>
<span><span class="co">#&gt; Residuals 22   43.0    1.96</span></span></code></pre></div>
<p>Since the design is balanced after averaging, we can use <code>aov</code> in <strong>R</strong>: we need to specify the subject identifier within <code>Error</code> term. This approach has a drawback, as variance components can be negative if the variability due to subject is negligible. While <code>aov</code> is fast, it only works for simple balanced designs.</p>
</div>
<div id="contrasts-1" class="section level3" number="9.1.1">
<h3>
<span class="header-section-number">9.1.1</span> Contrasts<a class="anchor" aria-label="anchor" href="#contrasts-1"><i class="fas fa-link"></i></a>
</h3>
<p>With balanced data, the estimated marginal means coincide with the row averages. If we have a single replication or the average for each subject/condition, we could create a new column with the contrast and then fit a model with an intercept-only (global mean) to check whether the latter is zero. With 12 participants, we should thus expect our test statistic to have 11 degrees of freedom, since one unit is spent on estimating the mean parameter and we have 12 participants.</p>
<p>Unfortunately, the <code>emmeans</code> package analysis for object fitted using <code>aov</code> will be incorrect: this can be seen by passing a contrast vector and inspecting the degrees of freedom. The <code>afex</code> package includes functionalities that are tailored for within-subject and between-subjects and has an interface with <code>emmeans</code>.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">afexmod</span> <span class="op">&lt;-</span> <span class="fu">afex</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/afex/man/aov_car.html">aov_ez</a></span><span class="op">(</span></span>
<span>  id <span class="op">=</span> <span class="st">"id"</span>,           <span class="co"># subject id</span></span>
<span>  dv <span class="op">=</span> <span class="st">"latency"</span>,      <span class="co"># response variable</span></span>
<span>  within <span class="op">=</span> <span class="st">"stimulus"</span>, <span class="co"># within-subject factor</span></span>
<span>  data <span class="op">=</span> <span class="va">AA21</span>,</span>
<span>  fun_aggregate <span class="op">=</span> <span class="va">mean</span><span class="op">)</span></span></code></pre></div>
<p>The <code>afex</code> package has different functions for computing the within-subjects design and the <code>aov_ez</code> specification, which allow people to list within and between-subjects factor separately with subject identifiers may be easier to understand. It also has an argument, <code>fun_aggregate</code>, to automatically average replications.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Set up contrast vector</span></span>
<span><span class="va">cont_vec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  <span class="st">"real vs GAN"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rvlenth/emmeans">emmeans</a></span><span class="op">)</span></span>
<span><span class="co"># Correct output</span></span>
<span><span class="va">afexmod</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">emmeans</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/emmeans/man/emmeans.html">emmeans</a></span><span class="op">(</span></span>
<span>    spec <span class="op">=</span> <span class="st">"stimulus"</span>, </span>
<span>    contr <span class="op">=</span> <span class="va">cont_vec</span><span class="op">)</span></span>
<span><span class="co">#&gt; $emmeans</span></span>
<span><span class="co">#&gt;  stimulus emmean    SE df lower.CL upper.CL</span></span>
<span><span class="co">#&gt;  real      -10.8 0.942 11    -12.8    -8.70</span></span>
<span><span class="co">#&gt;  GAN1      -10.8 0.651 11    -12.3    -9.40</span></span>
<span><span class="co">#&gt;  GAN2      -10.3 0.662 11    -11.8    -8.85</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Confidence level used: 0.95 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $contrasts</span></span>
<span><span class="co">#&gt;  contrast    estimate    SE df t.ratio p.value</span></span>
<span><span class="co">#&gt;  real vs GAN   -0.202 0.552 11  -0.366  0.7210</span></span>
<span><span class="co"># Incorrect output - </span></span>
<span><span class="co"># note the wrong degrees of freedom</span></span>
<span><span class="va">fixedmod</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">emmeans</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/emmeans/man/emmeans.html">emmeans</a></span><span class="op">(</span></span>
<span>    spec <span class="op">=</span> <span class="st">"stimulus"</span>, </span>
<span>    contr <span class="op">=</span> <span class="va">cont_vec</span><span class="op">)</span></span>
<span><span class="co">#&gt; $emmeans</span></span>
<span><span class="co">#&gt;  stimulus emmean    SE   df lower.CL upper.CL</span></span>
<span><span class="co">#&gt;  real      -10.8 0.763 16.2    -12.4    -9.15</span></span>
<span><span class="co">#&gt;  GAN1      -10.8 0.763 16.2    -12.4    -9.21</span></span>
<span><span class="co">#&gt;  GAN2      -10.3 0.763 16.2    -11.9    -8.69</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Warning: EMMs are biased unless design is perfectly balanced </span></span>
<span><span class="co">#&gt; Confidence level used: 0.95 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $contrasts</span></span>
<span><span class="co">#&gt;  contrast    estimate    SE df t.ratio p.value</span></span>
<span><span class="co">#&gt;  real vs GAN   -0.202 0.494 22  -0.409  0.6870</span></span></code></pre></div>
</div>
<div id="sphericity-assumption" class="section level3" number="9.1.2">
<h3>
<span class="header-section-number">9.1.2</span> Sphericity assumption<a class="anchor" aria-label="anchor" href="#sphericity-assumption"><i class="fas fa-link"></i></a>
</h3>
<p>The validity of the <span class="math inline">\(F\)</span> statistic null distribution relies on the model having the correct structure.</p>
<p>In repeated-measure analysis of variance, we assume again that each measurement has the same variance. We equally require the correlation between measurements of the same subject to be the same, an assumption that corresponds to the so-called compound symmetry model.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Note that, with two measurements, there is a single correlation parameter to estimate and this assumption is irrelevant.&lt;/p&gt;"><sup>46</sup></a></p>
<p>What if the within-subject measurements have unequal variance or the correlation between those responses differs?</p>
<p>Since we care only about differences in treatment, can get away with a weaker assumption than compound symmetry (equicorrelation) by relying instead on <em>sphericity</em>, which holds if the variance of the difference between treatment is constant. Sphericity is not a relevant concept when there is only two measurements (as there is a single correlation); we could check this by comparing the fit of a model with an unstructured covariance (difference variances for each and correlations for each pair of variable)</p>
<p>The most popular approach to handling correlation in tests is a two-stage approach: first, check for sphericity (using, e.g., Mauchlyâs test of sphericity). If the null hypothesis of sphericity is rejected, one can use a correction for the <span class="math inline">\(F\)</span> statistic by modifying the parameters of the Fisher <span class="math inline">\(\mathsf{F}\)</span> null distribution used as benchmark.</p>
<p>An idea due to Box is to correct the degrees of freedom of the <span class="math inline">\(\mathsf{F}(\nu_1, \nu_2)\)</span> distribution by multiplying them by a common factor <span class="math inline">\(\epsilon&lt;1\)</span> and use <span class="math inline">\(\mathsf{F}(\epsilon\nu_1, \epsilon\nu_2)\)</span>. Since the <span class="math inline">\(F\)</span> statistic is a ratio of variances, the <span class="math inline">\(\epsilon\)</span> term would cancel. Using the scaled <span class="math inline">\(\mathsf{F}\)</span> distribution leads to larger <span class="math inline">\(p\)</span>-values, thus accounting for the correlation.</p>
<p>There are three widely used corrections: GreenhouseâGeisser, HuynhâFeldt and Box correction, which divides by <span class="math inline">\(\nu_1\)</span> both degrees of freedom and gives a very conservative option. The HuynhâFeldt method is reported to be more powerful so should be preferred, but the estimated value of <span class="math inline">\(\epsilon\)</span> can be larger than 1.</p>
<p>Using the <code>afex</code> functions, we get the result for Mauchlyâs test of sphericity and the <span class="math inline">\(p\)</span> values from using either correction method</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">afexmod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Univariate Type III Repeated-Measures ANOVA Assuming Sphericity</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;             Sum Sq num Df Error SS den Df F value  Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; (Intercept)   4073      1      188     11   238.6 8.4e-09 ***</span></span>
<span><span class="co">#&gt; stimulus         2      2       43     22     0.5    0.62    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Mauchly Tests for Sphericity</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;          Test statistic p-value</span></span>
<span><span class="co">#&gt; stimulus          0.678   0.143</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Greenhouse-Geisser and Huynh-Feldt Corrections</span></span>
<span><span class="co">#&gt;  for Departure from Sphericity</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;          GG eps Pr(&gt;F[GG])</span></span>
<span><span class="co">#&gt; stimulus  0.757       0.57</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;          HF eps Pr(&gt;F[HF])</span></span>
<span><span class="co">#&gt; stimulus  0.851      0.587</span></span></code></pre></div>
<div class="example">
<p><span id="exm:unlabeled-div-24" class="example"><strong>Example 9.2  (Visual acuity) </strong></span>We consider a model with both within-subject and between-subject factors. Data for a study on visual acuity of participants. The data represent the number of words correctly detected at different font size; interest is in effect of illusory contraction on detection. The mixed analysis of variance includes the experimental factors <code>adaptation</code> (2 levels, within), <code>fontsize</code> (4 levels, within), <code>position</code> (5 levels, within) and visual <code>acuity</code> (2 levels, between). There are a total of 1760 measurements for 44 participants in <code>LBJ17_S1A</code>, balanced.
The within-subject factors give a total of 40 measurements (<span class="math inline">\(2 \times 4 \times 5\)</span>) per participant; all of these factors are crossed and we can estimate interactions for them. The subjects are nested within visual acuity groups, The participants were dichotomized in two groups based on their visual acuity, obtained from preliminary checks, using a median split.</p>
<p>To fit the model, we rely on the <code>aov_ez</code> function from <code>afex</code>. By default, the latter includes all interactions.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">LBJ_mod</span> <span class="op">&lt;-</span> <span class="fu">afex</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/afex/man/aov_car.html">aov_ez</a></span><span class="op">(</span></span>
<span>  id <span class="op">=</span> <span class="st">"id"</span>,     <span class="co"># subject id</span></span>
<span>  dv <span class="op">=</span> <span class="st">"nerror"</span>, <span class="co"># response</span></span>
<span>  between <span class="op">=</span> <span class="st">"acuity"</span>,</span>
<span>  within <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"adaptation"</span>,</span>
<span>             <span class="st">"fontsize"</span>, </span>
<span>             <span class="st">"position"</span><span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="fu">hecedsm</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/hecedsm/man/LBJ17_S1A.html">LBJ17_S1A</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">LBJ_mod</span>,  <span class="co"># model</span></span>
<span>      correction <span class="op">=</span> <span class="st">"none"</span>, <span class="co"># no correction for sphericity</span></span>
<span>      es <span class="op">=</span> <span class="st">"pes"</span><span class="op">)</span> </span>
<span><span class="co">#&gt; Anova Table (Type 3 tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: nerror</span></span>
<span><span class="co">#&gt;                                     num Df den Df   MSE       F   pes  Pr(&gt;F)</span></span>
<span><span class="co">#&gt; acuity                                   1     42 1.229   30.77 0.423 1.8e-06</span></span>
<span><span class="co">#&gt; adaptation                               1     42 0.300    7.76 0.156 0.00800</span></span>
<span><span class="co">#&gt; acuity:adaptation                        1     42 0.300   12.73 0.233 0.00091</span></span>
<span><span class="co">#&gt; fontsize                                 3    126 0.756 1705.68 0.976 &lt; 2e-16</span></span>
<span><span class="co">#&gt; acuity:fontsize                          3    126 0.756   10.04 0.193 5.6e-06</span></span>
<span><span class="co">#&gt; position                                 4    168 0.213    9.42 0.183 6.8e-07</span></span>
<span><span class="co">#&gt; acuity:position                          4    168 0.213    4.17 0.090 0.00303</span></span>
<span><span class="co">#&gt; adaptation:fontsize                      3    126 0.230    3.29 0.073 0.02296</span></span>
<span><span class="co">#&gt; acuity:adaptation:fontsize               3    126 0.230    6.98 0.142 0.00022</span></span>
<span><span class="co">#&gt; adaptation:position                      4    168 0.138    0.60 0.014 0.66209</span></span>
<span><span class="co">#&gt; acuity:adaptation:position               4    168 0.138    0.90 0.021 0.46406</span></span>
<span><span class="co">#&gt; fontsize:position                       12    504 0.206    9.05 0.177 7.5e-16</span></span>
<span><span class="co">#&gt; acuity:fontsize:position                12    504 0.206    2.70 0.060 0.00155</span></span>
<span><span class="co">#&gt; adaptation:fontsize:position            12    504 0.134    0.51 0.012 0.90739</span></span>
<span><span class="co">#&gt; acuity:adaptation:fontsize:position     12    504 0.134    1.18 0.027 0.29550</span></span>
<span><span class="co">#&gt;                                        </span></span>
<span><span class="co">#&gt; acuity                              ***</span></span>
<span><span class="co">#&gt; adaptation                          ** </span></span>
<span><span class="co">#&gt; acuity:adaptation                   ***</span></span>
<span><span class="co">#&gt; fontsize                            ***</span></span>
<span><span class="co">#&gt; acuity:fontsize                     ***</span></span>
<span><span class="co">#&gt; position                            ***</span></span>
<span><span class="co">#&gt; acuity:position                     ** </span></span>
<span><span class="co">#&gt; adaptation:fontsize                 *  </span></span>
<span><span class="co">#&gt; acuity:adaptation:fontsize          ***</span></span>
<span><span class="co">#&gt; adaptation:position                    </span></span>
<span><span class="co">#&gt; acuity:adaptation:position             </span></span>
<span><span class="co">#&gt; fontsize:position                   ***</span></span>
<span><span class="co">#&gt; acuity:fontsize:position            ** </span></span>
<span><span class="co">#&gt; adaptation:fontsize:position           </span></span>
<span><span class="co">#&gt; acuity:adaptation:fontsize:position    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#partial eta-square for effect sizes (es)</span></span></code></pre></div>
<p>This is the most complicated model we tested so far: there are four experimental factor being manipulated at once, and all interactions of order two, three and four are included!</p>
<p>The fourth order interaction isnât statistically significant: this means that we can legitimately marginalize over and look at each of the four three-way ANOVA designs in turn. We can also see that the third order interaction <code>adaptation:fontsize:position</code> and <code>acuity:adaptation:position</code> are not really meaningful.</p>
<p>The following paragraph is technical and can be skipped. One difficult bit with designs including both within-subject and between-subject factors is the degrees of freedom and the correct sum of square terms to use to calculate the <span class="math inline">\(F\)</span> statistics for each hypothesis of interest. The correct setup is to use the next sum of square (and the associated degrees of freedom) from this. For any main effect or interaction, we count the number of instances of this particular (e.g., 10 for the interaction between position and adaptation). We subtract the number of mean parameter used to estimate means and differences in mean (1 global mean, 4 means for position, 1 for adaptation), which gives <span class="math inline">\(4=10-6\)</span> degrees of freedom. Next, this term is compared to the mean square which contains only subject (here via acuity levels, since subjects are nested within acuity) and the corresponding variables; the correct mean square is for <code>acuity:adaptation:position</code>. In the balanced design setting, this can be formalized using Hasse diagram <span class="citation">(<a href="references.html#ref-Oehlert:2010" role="doc-biblioref">Oehlert 2000</a>)</span>.</p>
<p>We can produce an interaction plot to see what comes out: since we canât draw in four dimensions, we map visual acuity and adaptation level to panels and use different colours for the position. The figure looks different altogether from the paper because their <span class="math inline">\(y\)</span> axis is flipped.</p>
<div class="inline-figure"><img src="09-repeated_files/figure-html/fig-LBJ-interactionplot-1.png" width="85%" style="display: block; margin: auto;"></div>
</div>
</div>
</div>
<div id="multivariate-analysis-of-variance" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Multivariate analysis of variance<a class="anchor" aria-label="anchor" href="#multivariate-analysis-of-variance"><i class="fas fa-link"></i></a>
</h2>
<p>The second paradigm for modelling is to specify that the response from each subject is in fact a multivariate object: we can combine all measurements from a given individual in a vector <span class="math inline">\(\boldsymbol{Y}\)</span>. In the example with the happy fakes, this would be the tuple of measurements for (<code>real</code>, <code>GAN1</code>, <code>GAN2</code>).</p>
<p>The multivariate analysis of variance model is designed by assuming observations follow a (multivariate) normal distribution with mean vector <span class="math inline">\(\boldsymbol{\mu}_j\)</span> in group <span class="math inline">\(j\)</span> and common covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> and comparing means between groups. As in univariate analysis of variance, the multivariate normal assumption holds approximately by virtue of the central limit theorem in large samples, but the convergence is slower and larger numbers are needed to ensure this is valid.</p>
<p>The difference with the univariate approach is now that we will compare a global mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> between comparisons. In the one-way analysis of variance model with an experimental factor having <span class="math inline">\(K\)</span> levels and a balanced sample <span class="math inline">\(n_g\)</span> observations per group and <span class="math inline">\(n=n_gK\)</span> total observations, we assume that each group has average <span class="math inline">\(\boldsymbol{\mu}_k\)</span> <span class="math inline">\((k=1, \ldots, K)\)</span>, which we can estimate using only the observations from that group. Under the null hypothesis, all groups have the same mean, so the estimator is the overall mean <span class="math inline">\(\boldsymbol{\mu}\)</span> combining all <span class="math inline">\(n\)</span> observations.</p>
<p>The statistic is obtained by decomposing the total variance around the global mean into components due to the different factors and the leftover variability. Because these equivalent to the sum of square decomposition results in multiple matrices, there are multiple ways of constructing test statistics. Wilkâs <span class="math inline">\(\Lambda\)</span> is the most popular choice. Another common choice, which leads to a statistic giving lower power but more robust to departure from model assumptions is Pillaiâs trace.</p>
<p>The MANOVA model assumes that the covariance matrices are the same within each experimental condition. Under normality assumption, we can use Boxâs <span class="math inline">\(M\)</span> statistic to test the hypothesis.</p>
<div id="data-format" class="section level3" number="9.2.1">
<h3>
<span class="header-section-number">9.2.1</span> Data format<a class="anchor" aria-label="anchor" href="#data-format"><i class="fas fa-link"></i></a>
</h3>
<p>With repeated measures, it is sometimes convenient to store measurements associated to each experimental condition in different columns of a data frame or spreadsheet, with lines containing participants identifiers. Such data are said to be in <strong>wide format</strong>, since there are multiple measurements in each row. While this format is suitable for multivariate models, many statistical routines will instead expect data to be in <strong>long format</strong>, for which there is a single measurement per line. Figure <a href="repeated-measures-and-multivariate-models.html#fig:fig-longvswide">9.2</a> illustrates the difference between the two formats.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fig-longvswide"></span>
<img src="figures/original-dfs-tidy.png" alt="Long versus wide-format for data tables (illustration by Garrick Aden-Buie)." width="85%"><p class="caption">
Figure 9.2: Long versus wide-format for data tables (illustration by Garrick Aden-Buie).
</p>
</div>
<p>Ideally, a data base in long format with repeated measures would also include a column giving the order in which the treatments were assigned to participants. This is necessary in order to test whether there are fatigue or crossover effects, for example by plotting the residuals after accounting for treatment subject by subject, ordered over time. We could also perform formal tests by including time trends in the model and checking whether the slope is significant.</p>
<p>Overall, the biggest difference with within-subject designs is that observations are correlated whereas we assumed measurements were independent until now. This needs to be explicitly accounted for, as correlation has an important impact on testing as discussed Subsection <a href="CRT.html#independence">3.4.4</a>: failing to account for correlation leads to <span class="math inline">\(p\)</span>-values that are much too low. To see why, think about a stupid setting under which we duplicate every observation in the database: the estimated marginal means will be the same, but the variance will be halved despite the fact there is no additional information. Intuitively, correlation reduces the amount of information provided by each individual: if we have repeated measures from participants, we expect the effective sample size to be anywhere between the total number of subjects and the total number of observations.</p>
</div>
<div id="mathematical-complement" class="section level3" number="9.2.2">
<h3>
<span class="header-section-number">9.2.2</span> Mathematical complement<a class="anchor" aria-label="anchor" href="#mathematical-complement"><i class="fas fa-link"></i></a>
</h3>
<p>This section is technical and can be omitted. Analogous to the univariate case, we can decompose the variance estimator in terms of within, between and total variance. Let <span class="math inline">\(\boldsymbol{Y}_{ik}\)</span> denote the response vector for the <span class="math inline">\(i\)</span>th observation of group <span class="math inline">\(k\)</span>; then, we can decompose the variance as
<span class="math display">\[\begin{align*} &amp;
\underset{\text{total variance}}{\sum_{k=1}^K \sum_{i=1}^{n_g} (\boldsymbol{Y}_{ik} - \widehat{\boldsymbol{\mu}})(\boldsymbol{Y}_{ik} - \widehat{\boldsymbol{\mu}})^\top} \\\qquad &amp;= \underset{\text{within variance}}{\sum_{k=1}^K \sum_{i=1}^{n_g} (\boldsymbol{Y}_{ik} - \widehat{\boldsymbol{\mu}}_k)(\boldsymbol{Y}_{ik} - \widehat{\boldsymbol{\mu}}_k)^\top} + \underset{\text{between variance}}{\sum_{k=1}^K n_g(\boldsymbol{\mu}_{k} - \widehat{\boldsymbol{\mu}})(\widehat{\boldsymbol{\mu}}_k - \widehat{\boldsymbol{\mu}})^\top}
\end{align*}\]</span>
defining covariance matrix estimators. If we write <span class="math inline">\(\widehat{\boldsymbol{\Sigma}}_T\)</span>, <span class="math inline">\(\widehat{\boldsymbol{\Sigma}}_W\)</span>, and <span class="math inline">\(\widehat{\boldsymbol{\Sigma}}_B\)</span> for respectively the total, within and between variance estimators, we can build a statistic from these ingredients to see how much variability is induced by centering using a common vector. When <span class="math inline">\(K&gt;2\)</span>, there are multiple statistics that be constructed, including</p>
<ul>
<li>Wilkâs <span class="math inline">\(\Lambda\)</span>: <span class="math inline">\(|\widehat{\boldsymbol{\Sigma}}_W|/|\widehat{\boldsymbol{\Sigma}}_W + \widehat{\boldsymbol{\Sigma}}_B|\)</span>
</li>
<li>Royâs maximum root: the largest eigenvalue of <span class="math inline">\(\widehat{\boldsymbol{\Sigma}}_W^{-1}\widehat{\boldsymbol{\Sigma}}_B\)</span>
</li>
<li>LawleyâHotelling trace: <span class="math inline">\(\mathrm{tr}(\widehat{\boldsymbol{\Sigma}}_W^{-1}\widehat{\boldsymbol{\Sigma}}_B)\)</span>
</li>
<li>Pillaiâs trace: <span class="math inline">\(\mathrm{tr}\left\{\widehat{\boldsymbol{\Sigma}}_B(\widehat{\boldsymbol{\Sigma}}_W +\widehat{\boldsymbol{\Sigma}}_B)^{-1}\right\}\)</span>.</li>
</ul>
<p>All four criteria lead to equivalent statistics and the same <span class="math inline">\(p\)</span>-values if <span class="math inline">\(K=2\)</span>.</p>
<p>With a two-way balanced MANOVA, we can perform simlar decomposition for each factor or interaction, with
<span class="math display">\[\widehat{\boldsymbol{\Sigma}}_T = \widehat{\boldsymbol{\Sigma}}_A + \widehat{\boldsymbol{\Sigma}}_B + \widehat{\boldsymbol{\Sigma}}_{AB} + \widehat{\boldsymbol{\Sigma}}_W.\]</span></p>
<p>Wilkâs <span class="math inline">\(\Lambda\)</span> is based on taking the ratio of the determinant of the within-variance and that of the sum of effect-variance plus within-variance, e.g., <span class="math inline">\(|\widehat{\boldsymbol{\Sigma}}_{AB} + \widehat{\boldsymbol{\Sigma}}_W|\)</span> for the interaction term.</p>
</div>
<div id="model-fitting" class="section level3" number="9.2.3">
<h3>
<span class="header-section-number">9.2.3</span> Model fitting<a class="anchor" aria-label="anchor" href="#model-fitting"><i class="fas fa-link"></i></a>
</h3>
<p>We can treat the within-subject responses as a vector of observations and estimate the model using using multivariate linear regression. Contrary to the univariate counterpart, the model explicitly models the correlation between observations from the same subject.</p>
<p>In order to fit a model with a multivariate response, we first need to pivot the data into wider format so as to have a matrix with rows for the number of subjects and <span class="math inline">\(M\)</span> columns for the number of response variables.</p>
<p>Once the data are in a suitable format, we fit the multivariate model with the <code>lm</code> function using the sum-to-zero constraints, here imposed globally by changing the <code>contrasts</code> option. Syntax-wise, the only difference with the univariate case is that the response on the left of the tilde sign (<code>~</code>) is now a matrix composed by binding together the vectors with the different responses.</p>
<div class="example">
<p><span id="exm:unlabeled-div-25" class="example"><strong>Example 9.3  (Happy fakes - multivariate) </strong></span>We use the data from Amirabdolahian and Ali-Adeeb (2021), but this time treating the averaged repeated measures for the different stimulus as a multivariate response. We first pivot the data to wide format, then fit the multivariate linear model.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Pivot to wide format</span></span>
<span><span class="va">AA21_mw</span> <span class="op">&lt;-</span> <span class="va">AA21_m</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">tidyr</span><span class="fu">::</span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_wider.html">pivot_wider</a></span><span class="op">(</span>names_from <span class="op">=</span> <span class="va">stimulus</span>, <span class="co"># within-subject factor labels</span></span>
<span>                     values_from <span class="op">=</span> <span class="va">latency</span><span class="op">)</span> <span class="co"># response measurements </span></span>
<span><span class="co"># Model with each variable with a different mean</span></span>
<span><span class="co"># Specify all columns with column bind </span></span>
<span><span class="co"># left of the ~, following </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html">options</a></span><span class="op">(</span>contrasts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"contr.sum"</span>, <span class="st">"contr.poly"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">mlm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">real</span>, <span class="va">GAN1</span>, <span class="va">GAN2</span><span class="op">)</span> <span class="op">~</span> <span class="fl">1</span>,</span>
<span>          data <span class="op">=</span> <span class="va">AA21_mw</span><span class="op">)</span></span></code></pre></div>
<p>Since the within-subject factor <code>stimulus</code> disappeared when we consider the multivariate response, we only specify a global mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> via <code>~1</code>. In general, we would add the between-subject factors to the right-hand side of the equation. Our hypothesis of equal mean translates into the hypothesis <span class="math inline">\(\boldsymbol{\mu} = \mu\boldsymbol{1}_3\)</span>, which can be imposed using a call to <code>anova</code>. The output returns the statistic and <span class="math inline">\(p\)</span>-values including corrections for sphericity.</p>
<p>We can also use <code>emmeans</code> to set up post-hoc contrasts. Since we have no variable, we need to set in <code>specs</code> the repeated measure variable appearing on the left hand side of the formula; the latter is labelled <code>rep.meas</code> by default.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Test the multivariate model against</span></span>
<span><span class="co"># equal mean (X = ~1)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html">anova</a></span><span class="op">(</span><span class="va">mlm</span>, X <span class="op">=</span> <span class="op">~</span><span class="fl">1</span>, test <span class="op">=</span> <span class="st">"Spherical"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Analysis of Variance Table</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Contrasts orthogonal to</span></span>
<span><span class="co">#&gt; ~1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Greenhouse-Geisser epsilon: 0.7565</span></span>
<span><span class="co">#&gt; Huynh-Feldt epsilon:        0.8515</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;             Df   F num Df den Df Pr(&gt;F) G-G Pr H-F Pr</span></span>
<span><span class="co">#&gt; (Intercept)  1 0.5      2     22  0.615  0.567  0.587</span></span>
<span><span class="co">#&gt; Residuals   11</span></span>
<span><span class="co"># Follow-up contrast comparisons</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rvlenth/emmeans">emmeans</a></span><span class="op">)</span></span>
<span><span class="va">emm_mlm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/emmeans/man/emmeans.html">emmeans</a></span><span class="op">(</span><span class="va">mlm</span>, specs <span class="op">=</span> <span class="st">"rep.meas"</span><span class="op">)</span> </span>
<span><span class="va">emm_mlm</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/emmeans/man/contrast.html">contrast</a></span><span class="op">(</span>method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="op">-</span><span class="fl">0.5</span>,<span class="op">-</span><span class="fl">0.5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;  contrast         estimate    SE df t.ratio p.value</span></span>
<span><span class="co">#&gt;  c(1, -0.5, -0.5)   -0.202 0.552 11  -0.366  0.7210</span></span></code></pre></div>
<p>We can check that the output is the same in this case as the within-subject analysis of variance model fitted previously with the <code>afex</code> package.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 9.4  (Teaching to read) </strong></span>We consider a between-subject repeated measure multivariate analysis of variance model with the <span class="citation">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span>. The data are balanced by experimental condition and they include the results of three tests performed after the intervention: an error detection task, an expanded comprehension monitoring questionnaire and a cloze test. Note that the scale of the tests are different (16, 18 and 56).</p>
<p>We could obtain the estimated covariance matrix of the fitted model by extracting the residuals <span class="math inline">\(Y_{ik} - \widehat{\mu}_k\)</span> and computing the empirical covariance. The results shows a strong dependence between tests 1 and 3 (correlation of 0.39), but much weaker dependence with test2.</p>
<p>Let us compute the multivariate analysis of variance model</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">BSJ92</span>, package <span class="op">=</span> <span class="st">"hecedsm"</span><span class="op">)</span></span>
<span><span class="co"># Force sum-to-zero parametrization</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html">options</a></span><span class="op">(</span>contrasts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"contr.sum"</span>, <span class="st">"contr.poly"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Fit MANOVA model</span></span>
<span><span class="va">mmod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">posttest1</span>, <span class="va">posttest2</span>, <span class="va">posttest3</span><span class="op">)</span> <span class="op">~</span> <span class="va">group</span>,</span>
<span>   data <span class="op">=</span> <span class="va">BSJ92</span><span class="op">)</span></span>
<span><span class="co"># Calculate multivariate test</span></span>
<span><span class="va">mtest</span> <span class="op">&lt;-</span> <span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html">Anova</a></span><span class="op">(</span><span class="va">mmod</span>, test <span class="op">=</span> <span class="st">"Wilks"</span><span class="op">)</span></span>
<span><span class="co"># mtest</span></span>
<span><span class="co"># Get all statistics and univariate tests</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="fu">car</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html">Anova</a></span><span class="op">(</span><span class="va">mmod</span><span class="op">)</span>, univariate <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Type II MANOVA Tests:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for error:</span></span>
<span><span class="co">#&gt;           posttest1 posttest2 posttest3</span></span>
<span><span class="co">#&gt; posttest1     640.5      30.8       498</span></span>
<span><span class="co">#&gt; posttest2      30.8     356.4      -104</span></span>
<span><span class="co">#&gt; posttest3     498.3    -104.4      2512</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ------------------------------------------</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt; Term: group </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum of squares and products for the hypothesis:</span></span>
<span><span class="co">#&gt;           posttest1 posttest2 posttest3</span></span>
<span><span class="co">#&gt; posttest1    108.12      6.67     190.6</span></span>
<span><span class="co">#&gt; posttest2      6.67     95.12      56.7</span></span>
<span><span class="co">#&gt; posttest3    190.61     56.65     357.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Multivariate Tests: group</span></span>
<span><span class="co">#&gt;                  Df test stat approx F num Df den Df   Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; Pillai            2     0.408     5.30      6    124 0.000068 ***</span></span>
<span><span class="co">#&gt; Wilks             2     0.632     5.24      6    122 0.000078 ***</span></span>
<span><span class="co">#&gt; Hotelling-Lawley  2     0.519     5.19      6    120 0.000089 ***</span></span>
<span><span class="co">#&gt; Roy               2     0.318     6.58      3     62  0.00062 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Type II Sums of Squares</span></span>
<span><span class="co">#&gt;           df posttest1 posttest2 posttest3</span></span>
<span><span class="co">#&gt; group      2       108      95.1       357</span></span>
<span><span class="co">#&gt; residuals 63       641     356.4      2512</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  F-tests</span></span>
<span><span class="co">#&gt;       posttest1 posttest2 posttest3</span></span>
<span><span class="co">#&gt; group      5.32      8.41      4.48</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  p-values</span></span>
<span><span class="co">#&gt;       posttest1 posttest2 posttest3</span></span>
<span><span class="co">#&gt; group 0.007     0.0006    0.015</span></span></code></pre></div>
<p>By default, we get Pillaiâs trace statistic. Here, there is clear evidence of differences between groups of observations regardless of the statistic being used.</p>
<p>We can compute effect size as before by passing the table, for example using <code>eta_squared(mtest)</code> to get the effect size of the multivariate test, or simple the model to get the individual variable effect sizes.</p>
<p>Having found a difference, one could in principle investigate for which component of the response they are by performing univariate analysis of variance and accounting for multiple testing using, e.g., Bonferroniâs correction. A more fruitful avenue if you are trying to discriminate is to use descriptive discriminant analysis as a follow-up, which computes the best fitting hyperplanes that separate groups.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">group</span> <span class="op">~</span> <span class="va">posttest1</span> <span class="op">+</span> <span class="va">posttest2</span> <span class="op">+</span> <span class="va">posttest3</span>,</span>
<span>          data <span class="op">=</span> <span class="va">BSJ92</span><span class="op">)</span></span></code></pre></div>
<p>This amounts to compute the weights <span class="math inline">\(\boldsymbol{w}\)</span> such, that, computing <span class="math inline">\(\boldsymbol{w}^\top\boldsymbol{Y}\)</span> creating a composite score by adding up weighted components that leads to maximal separation between groups. Figure <a href="repeated-measures-and-multivariate-models.html#fig:fig-lindiscrim">9.3</a> shows the new coordinates.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fig-lindiscrim"></span>
<img src="09-repeated_files/figure-html/fig-lindiscrim-1.png" alt="Scatterplot of observations projected onto the linear discriminants for the post-experiment tests, by group." width="85%"><p class="caption">
Figure 9.3: Scatterplot of observations projected onto the linear discriminants for the post-experiment tests, by group.
</p>
</div>
<p>Linear discriminant analysis is a topic on itâs own that is beyond the scope of the course.</p>
</div>
</div>
<div id="model-assumptions-1" class="section level3" number="9.2.4">
<h3>
<span class="header-section-number">9.2.4</span> Model assumptions<a class="anchor" aria-label="anchor" href="#model-assumptions-1"><i class="fas fa-link"></i></a>
</h3>
<p>In addition to the usual model assumptions (independence of measurements from different subjects, equal variance, additivity, etc.), the MANOVA model adds two hypothesis that altogether determine how reliable our <span class="math inline">\(p\)</span>-values and conclusions are.</p>
<p>The first assumption is that of multivariate normality of the response. The central limit theorem can be applied to a multivariate response, but the sample size needed overall to reliably estimate the correlation and variance is larger than in the univariate setting. This hypothesis can be tested using the Shapiro-Wilk normality test (null hypothesis is that of normality) by passing the residuals of the multivariate model. Such a test can lead to rejection of the null hypothesis when specific variables are far from normal, or when the dependence structure isnât the one exhibited by a multivariate normal model. With decent sample sizes (say <span class="math inline">\(n=50\)</span> per group), this assumption isnât as important as others.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Shapiro-Wilk normality test</span></span>
<span><span class="co"># Must transpose the residuals </span></span>
<span><span class="co"># to get a 3 by n matrix</span></span>
<span><span class="fu">mvnormtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/mvnormtest/man/mshapiro.test.html">mshapiro.test</a></span><span class="op">(</span>U <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">resid</a></span><span class="op">(</span><span class="va">mmod</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Shapiro-Wilk normality test</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  Z</span></span>
<span><span class="co">#&gt; W = 1, p-value = 0.06</span></span></code></pre></div>
<p>The second assumption is that the covariance matrix is the same for all individuals, regardless of their experimental group assignment. We could try checking whether a covariance model in each group: under multivariate normal assumption, this leads to a test statistic called Boxâs <span class="math inline">\(M\)</span> test. Unfortunately, this test is quite sensitive to departures from the multivariate normal assumption and, if the <span class="math inline">\(p\)</span>-value is small, it may have to do more with the normality than the heterogeneity.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">BSJ92</span>, </span>
<span>     <span class="fu">biotools</span><span class="fu">::</span><span class="fu"><a href="https://arsilva87.github.io/biotools/reference/boxM.html">boxM</a></span><span class="op">(</span></span>
<span>       data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">posttest1</span>, <span class="va">posttest2</span>, <span class="va">posttest3</span><span class="op">)</span>,</span>
<span>       grouping <span class="op">=</span> <span class="va">group</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Box's M-test for Homogeneity of Covariance Matrices</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; data:  cbind(posttest1, posttest2, posttest3)</span></span>
<span><span class="co">#&gt; Chi-Sq (approx.) = 15, df = 12, p-value = 0.2</span></span></code></pre></div>
<p>In our example, there is limited evidence against any of those model assumptions. We should of course also check the assumptions of the analysis of variance model for each of <code>postest1</code>, <code>posttest2</code> and <code>posttest3</code> in turn; such a check is left as an exercice to the reader.</p>
</div>
<div id="power-and-effect-size" class="section level3" number="9.2.5">
<h3>
<span class="header-section-number">9.2.5</span> Power and effect size<a class="anchor" aria-label="anchor" href="#power-and-effect-size"><i class="fas fa-link"></i></a>
</h3>
<p>Since all of the multivariate statistics can be transformed for a comparison with a univariate <span class="math inline">\(\mathsf{F}\)</span> distribution, we can estimate partial effect size as before. The package <code>effectsize</code> offers a measure of partial <span class="math inline">\(\widehat{\eta}^2\)</span> for the multivariate tests.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;I must confess I havenât checked whether the output is sensical.&lt;/p&gt;"><sup>47</sup></a></p>
<p>Power calculations are beyond the reach of ordinary software as one needs to specify the variance of each observation, their correlation and their mean. Simulation is an obvious way for this kind of design to obtain answers, but the free <strong>G</strong><span class="math inline">\({}^{*}\)</span><strong>Power</strong> software <span class="citation">(<a href="references.html#ref-GPower3" role="doc-biblioref">Faul et al. 2007</a>)</span> also offers some tools. See also <span class="citation">LÃ¤uter (<a href="references.html#ref-Lauter:1978" role="doc-biblioref">1978</a>)</span> for pairwise comparisons.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="replication-crisis.html"><span class="header-section-number">8</span> Replication crisis</a></div>
<div class="next"><a href="introduction-to-mixed-models.html"><span class="header-section-number">10</span> Introduction to mixed models</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#repeated-measures-and-multivariate-models"><span class="header-section-number">9</span> Repeated measures and multivariate models</a></li>
<li>
<a class="nav-link" href="#repeated-measures"><span class="header-section-number">9.1</span> Repeated measures</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#contrasts-1"><span class="header-section-number">9.1.1</span> Contrasts</a></li>
<li><a class="nav-link" href="#sphericity-assumption"><span class="header-section-number">9.1.2</span> Sphericity assumption</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#multivariate-analysis-of-variance"><span class="header-section-number">9.2</span> Multivariate analysis of variance</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#data-format"><span class="header-section-number">9.2.1</span> Data format</a></li>
<li><a class="nav-link" href="#mathematical-complement"><span class="header-section-number">9.2.2</span> Mathematical complement</a></li>
<li><a class="nav-link" href="#model-fitting"><span class="header-section-number">9.2.3</span> Model fitting</a></li>
<li><a class="nav-link" href="#model-assumptions-1"><span class="header-section-number">9.2.4</span> Model assumptions</a></li>
<li><a class="nav-link" href="#power-and-effect-size"><span class="header-section-number">9.2.5</span> Power and effect size</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/lbelzile/math80667a/blob/master/09-repeated.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/lbelzile/math80667a/edit/master/09-repeated.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Experimental Design and Statistical Methods</strong>" was written by LÃ©o Belzile. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
