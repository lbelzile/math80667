<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>1 Introduction to Experimental Designs | Experimental Design and Statistical Methods</title>
<meta name="author" content="Léo Belzile">
<meta name="description" content="The field of causal inference is concerned with inferring the effect of a treatment variable (sometimes called independent variable) on a response variable (dependent variable). In general,...">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="1 Introduction to Experimental Designs | Experimental Design and Statistical Methods">
<meta property="og:type" content="book">
<meta property="og:description" content="The field of causal inference is concerned with inferring the effect of a treatment variable (sometimes called independent variable) on a response variable (dependent variable). In general,...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Introduction to Experimental Designs | Experimental Design and Statistical Methods">
<meta name="twitter:description" content="The field of causal inference is concerned with inferring the effect of a treatment variable (sometimes called independent variable) on a response variable (dependent variable). In general,...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Experimental Design and Statistical Methods</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preliminary remarks</a></li>
<li><a class="active" href="experimental-intro.html"><span class="header-section-number">1</span> Introduction to Experimental Designs</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="" href="reproducibility-crisis.html"><span class="header-section-number">3</span> Reproducibility crisis</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/lbelzile/math80667a">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="experimental-intro" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Introduction to Experimental Designs<a class="anchor" aria-label="anchor" href="#experimental-intro"><i class="fas fa-link"></i></a>
</h1>
<p>The field of causal inference is concerned with inferring the effect of a treatment variable (sometimes called independent variable) on a response variable (dependent variable). In general, however <span class="citation">(<a href="references.html#ref-Cox:1958" role="doc-biblioref">Cox 1958</a>)</span></p>
<blockquote>
<p>effects under investigation tend to be masked by fluctuations outside the experimenter’s control.</p>
</blockquote>
<p>The purpose of experiments is to arrange data collection so as to be capable of disentangling the differences due to treatment from those due to the (often large) intrinsic variation of the measurements. We typically expect differences between treatments (and thus the effect) to be <em>comparatively stable</em> relative to the measurement variation.</p>
<div class="example">
<p><span id="exm:experimentalexample1" class="example"><strong>Example 1.1  (Modern experiments and A/B testing) </strong></span>Most modern experiments happen online, with tech companies running thousands of experiments on an ongoing basis in order to discover improvement to their interfaces that lead to increased profits. An <a href="https://hbr.org/2017/09/the-surprising-power-of-online-experiments">Harvard Business Review article</a> <span class="citation">(<a href="references.html#ref-HBR2017" role="doc-biblioref">Kohavi and Thomke 2017</a>)</span> details how small tweaks to the display of advertisements in the Microsoft Bing search engine landing page lead to a whooping 12% increase in revenues. Such randomized control trials, termed A/B experiments, involve splitting incoming traffic into separate groups; each group will see different views of the webpage that differ only ever slightly. The experimenters then compare traffic and click revenues. At large scale, even small effects can have major financial consequences and can be learned despite the large variability in customer behaviour.</p>
</div>
<p>It is useful at this stage to introduce some terminology. In its simplest form, an <strong>experimental design</strong> is a comparison of two or more treatments (experimental conditions):</p>
<ul>
<li>The subjects (or <strong>experimental units</strong>) in the different groups of treatment have similar characteristics and are treated exactly the same way in the experimentation except for the treatment they are receiving. Formally, an experimental unit is the smallest division such that any two units may receive different treatments.</li>
<li>The <strong>observational unit</strong> is the smallest level (time point, individual) at which measurement are recorded.</li>
<li>The <strong>experimental treatments</strong> or conditions (also called <strong>factor</strong>, or independent variable), are <em>manipulated and controlled</em> by the researcher. Oftentimes, there is a <strong>control</strong> or baseline treatment relative to which we measure improvement (e.g., a placebo for drugs).</li>
<li>Additional explanatories that are intrinsic to the experimental (sub-)units are termed <strong>blocking factors</strong>. Controlling for these allows to reduce the variability of measurements, typically leading to improved inferences.</li>
<li>After the different treatments have been administered to subjects participating in a study, the researcher measures one or more outcomes (also called responses or dependent variables) on each subject.</li>
<li>Observed differences in the outcome variable between the experimental conditions (treatments) is called the treatment effect (or <strong>effect size</strong>).</li>
</ul>
<div class="example">
<p><span id="exm:experimentdefinitions" class="example"><strong>Example 1.2  (Pedagogical experience) </strong></span>Suppose we want to study the effectiveness of different pedagogical approaches to learning. Evidence-based pedagogical researchs point out that active learning leads to higher retention of information. To corroborate this research hypothesis, we can design an experiment in which different sections of a course are assigned to different teaching methods. In this example, each student in a class group receives the same teaching assignment, so the experimental units are the sections and the observations units are the individual students.</p>
<p>The treatment is the teaching method (traditional teaching versus flipped classroom).</p>
<p>Potential blocking factors for this experiment include the strength of individuals, which reflects their prior exposure to the topic, knowledge, maturity, etc. This could be measured using a preliminary exam before assignment of students to class groups is completed. Additional factors worth controlling for include timing of the classroom (morning, afternoon, evening classes) and instructors.</p>
</div>
<div class="yourturn">
<p>The marketing department wants to know the value of its brand by determining how much more customers are willing to pay for their product relative to the cheaper generic product offered by the store. Economic theory suggests a substitution effect: while customers may prefer the brand product, they will switch to the generic version if the price tag is too high. To check this theory, one could design an experiment.</p>
<p>As a researcher, how would you conduct this study? Identify a specific product. For the latter, define</p>
<ul>
<li>an adequate response variable</li>
<li>the experimental and observational units</li>
<li>potential confounding variables that would need to be accounted for.</li>
</ul>
</div>
<div id="basic-statistical-notions" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Basic statistical notions<a class="anchor" aria-label="anchor" href="#basic-statistical-notions"><i class="fas fa-link"></i></a>
</h2>
</div>
<div id="population-sample" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Population and samples<a class="anchor" aria-label="anchor" href="#population-sample"><i class="fas fa-link"></i></a>
</h2>
<p>Generally, we will seek to estimate characteristics of a population using only a sample (a sub-group of the population of smaller size). The <strong>population of interest</strong> is a collection of individuals which the study targets. For example, the Labour Force Survey (LFS) is a monthly study conducted by Statistics Canada, who define the target population as “all members of the selected household who are 15 years old and older, whether they work or not.” Asking every Canadian meeting this definition would be costly and the process would be long: the characteristic of interest (employment) is also a snapshot in time and can vary when the person leaves a job, enters the job market or become unemployed. In this observational study, collecting a census would be impossible.</p>
<p>In general, we therefore consider only <strong>samples</strong> to gather the information we seek to obtain. The purpose of <strong>statistical inference</strong> is to draw conclusions about the population, but using only a share of the latter and accounting for sources of variability. The pollster George Gallup made this great analogy between sample and population:</p>
<blockquote>
<p>One spoonful can reflect the taste of the whole pot, if the soup is well-stirred</p>
</blockquote>
<p>A <strong>sample</strong> is a sub-group of individuals drawn at random from the population. We won’t focus on data collection, but keep in mind the following information: for a sample to be good, it must be representative of the population under study.</p>
<div class="yourturn" name="Confounding">
<p>The <a href="https://www.hec.ca/etudiants/mon-programme/baa/cohorte-agir/index.html">Parcours AGIR</a> at HEC Montréal is a pilot project for Bachelor in Administration students that was initiated to study the impact of flipped classroom and active learning on performance.</p>
<p>Do you think we can draw conclusions about the efficacy of this teaching method by comparing the results of the students with those of the rest of the bachelor program? List potential issues with this approach addressing the internal and external validity, generalizability, effect of lurking variables, etc.</p>
</div>
<p>Because the individuals are selected at <strong>random</strong> to be part of the sample, the measurement of the characteristic of interest will also be random and change from one sample to the next. While larger samples typically carry more information, sample size is not a guarantee of quality, as the following example demonstrates.</p>
<div class="example">
<p><span id="exm:Galluppoll" class="example"><strong>Example 1.3  (Polling for the 1936 USA Presidential Election) </strong></span><em>The Literary Digest</em> surveyed 10 millions people by mail to know voting preferences for the 1936 USA Presidential Election. A sizeable share, 2.4 millions answered, giving Alf Landon (57%) over incumbent President Franklin D. Roosevelt (43%). The latter nevertheless won in a landslide election with 62% of votes cast, a 19% forecast error. <a href="https://www.jstor.org/stable/2749114">Biased sampling and differential non-response are mostly responsible for the error:</a> the sampling frame was built using ``phone number directories, drivers’ registrations, club memberships, etc.’’, all of which skewed the sample towards rich upper class white people more susceptible to vote for the GOP.</p>
<p>In contrast, Gallup correctly predicted the outcome by polling (only) 50K inhabitants. <a href="https://medium.com/@ozanozbey/how-not-to-sample-11579793dac">Read the full story here.</a></p>
</div>
<p>Because sampling is costly, we can only collect limited information about the variable of interest. Experimental design revolves in large part in understanding how best to allocate our resources to attain a specified goal.</p>
<div id="variable-types" class="section level3" number="1.2.1">
<h3>
<span class="header-section-number">1.2.1</span> Variable types<a class="anchor" aria-label="anchor" href="#variable-types"><i class="fas fa-link"></i></a>
</h3>
</div>
<div id="variables" class="section level3" number="1.2.2">
<h3>
<span class="header-section-number">1.2.2</span> Variables<a class="anchor" aria-label="anchor" href="#variables"><i class="fas fa-link"></i></a>
</h3>
<p>The choice of statistical model and test depends on the underlying type of the data collected. There are many choices: quantitative (discrete or continuous) if the variables are numeric, or qualitative (binary, nominal, ordinal) if they can be described using an adjective; I prefer the term categorical, which is more evocative. The choice of graphical representation for data is contingent on variable type. Specifically,</p>
<ul>
<li>a <strong>variable</strong> represents a characteristic of the population, for example the sex of an individual, the price of an item, etc.</li>
<li>an <strong>observation</strong> is a set of measures (variables) collected under identical conditions for an individual or at a given time.</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:variablesquanti"></span>
<img src="figures/continuous_discrete.png" alt="Artwork by Allison Horst of continuous (left) and discrete variables (right)." width="85%"><p class="caption">
Figure 1.1: Artwork by Allison Horst of continuous (left) and discrete variables (right).
</p>
</div>
<p>Most of the models we will deal with are so-called regression models, in which the mean of a quantitative variable is a function of other variables, termed explanatories. There are two types of numerical variables</p>
<ul>
<li>a discrete variable takes a countable number of values, prime examples being binary variables or count variables.</li>
<li>a continuous variable can take (in theory) an infinite possible number of values, even when measurements are rounded or measured with a limited precision (time, width, mass). In many case, we could also consider discrete variables as continuous if they take enough values (e.g., money).</li>
</ul>
<p>Categorical variables take only a finite of values. They are regrouped in two groups, nominal if there is no ordering between levels (sex, colour, country of origin) or ordinal if they are ordered (Likert scale, salary scale) and this ordering should be reflected in graphs or tables. We will bundle every categorical variable using arbitrary encoding for the levels: for modelling, these variables taking <span class="math inline">\(K\)</span> possible values (or levels) must be transformed into a set of <span class="math inline">\(K-1\)</span> binary 0/1 variables, the omitted level corresponding to a baseline. Failing to declare categorical variables in your favourite software is a common mistake, especially when these are saved in the database using integers rather than strings.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:variablescateg"></span>
<img src="figures/nominal_ordinal_binary.png" alt="Artwork by Allison Horst with examples of categorical variables: nominal (left), ordinal (middle) and binary (right)." width="85%"><p class="caption">
Figure 1.2: Artwork by Allison Horst with examples of categorical variables: nominal (left), ordinal (middle) and binary (right).
</p>
</div>
</div>
<div id="sampling" class="section level3" number="1.2.3">
<h3>
<span class="header-section-number">1.2.3</span> Sampling<a class="anchor" aria-label="anchor" href="#sampling"><i class="fas fa-link"></i></a>
</h3>
<p>TODO
- Random sampling allows generalizability
- Why sampling is hard
- example from opinion polling (IVR, etc.)
- Basic methods: simple random, stratified, cluster sampling</p>
</div>
<div id="a-primer-on-causal-inference" class="section level3" number="1.2.4">
<h3>
<span class="header-section-number">1.2.4</span> A primer on causal inference<a class="anchor" aria-label="anchor" href="#a-primer-on-causal-inference"><i class="fas fa-link"></i></a>
</h3>
<p>TODO add introduction about correlation, etc.</p>
<p>A pet peeve of statisticians is that correlation is not causation. Observational studies often allow people to detect association between variables, but The reason it matter’s is that we are mostly interested by causes and logical relations. For example, the fact that weather forecast of rain and the number of people carrying umbrellas in the streets are positively correlated, but the relationship is asymmetrical: if I force people to carry umbrellas, it won’t impact forecasts.</p>
<p>TODO add plot of X and Y with causal graph</p>
<p>Because everything else is the same in a well controlled experiment, any treatment effect should be in principle caused by the factor. We make this intuition a bit more rigourous.</p>
<p>We contrast experimental and observational settings: in the latter, the researcher cannot intervene. For example, an economist studying the impact of interest rates on the price of housing can only look at historical records with observed differences. Most surveys studying the labour market are also observational: people cannot influence the type of job performed by employees or their social benefits. Observational studies can lead to detection of association, but only an experiment in which the researcher controls the allocation mechanism through randomization can lead to establish existence of a causal relationship.</p>
<p>There is another aspect related to the generalization of the conclusion of a study: only for well-designed sampling schemes does results generalize beyond the group observed. It is thus of paramount importance to define the objective and the population of interest should we want to make conclusions.</p>
</div>
<div id="observational-versus-experimental-studies" class="section level3" number="1.2.5">
<h3>
<span class="header-section-number">1.2.5</span> Observational versus experimental studies<a class="anchor" aria-label="anchor" href="#observational-versus-experimental-studies"><i class="fas fa-link"></i></a>
</h3>
<p>TODO
- Definition of difference
- Respective merits of both</p>
</div>
</div>
<div id="examples-of-experimental-designs" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Examples of experimental designs<a class="anchor" aria-label="anchor" href="#examples-of-experimental-designs"><i class="fas fa-link"></i></a>
</h2>
<p>TODO add examples</p>
<div id="evidence-based-policies" class="section level3" number="1.3.1">
<h3>
<span class="header-section-number">1.3.1</span> Evidence-based policies<a class="anchor" aria-label="anchor" href="#evidence-based-policies"><i class="fas fa-link"></i></a>
</h3>
<p>There are multiple examples of randomized control experiments used for policy making. These include</p>
<ul>
<li>Tennessee’s Student Teacher Achievement Ratio (STAR) project <span class="citation">(<a href="references.html#ref-STAR2008" role="doc-biblioref">Achilles et al. 2008</a>)</span>: this study looked at the effect of student to teacher ratio and conclude that smaller class sizes lead to better outcomes.</li>
</ul>
<blockquote>
<p>Four-year longitudinal class-size study funded by the Tennessee General Assembly and conducted by the State Department of Education. Over 7,000 students in 79 schools were randomly assigned into one of 3 interventions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher’s aide). Classroom teachers were also randomly assigned to the classes they would teach. The interventions were initiated as the students entered school in kindergarten and continued through third grade.</p>
</blockquote>
<ul>
<li>RAND’s Health Insurance Experiment <span class="citation">(<a href="references.html#ref-RANDHIE" role="doc-biblioref">Brook et al. 2006</a>)</span>: this study concluded that that cost sharing reduced “inappropriate or unnecessary” medical care (overutilization), but also lead to to areduction in “appropriate or needed” medical care.</li>
</ul>
<blockquote>
<p>The HIE was a large-scale, randomized experiment conducted between 1971 and 1982. For the study, RAND recruited 2,750 families encompassing more than 7,700 individuals, all of whom were under the age of 65. They were chosen from six sites across the United States to provide a regional and urban/rural balance. Participants were randomly assigned to one of five types of health insurance plans created specifically for the experiment. There were four basic types of fee-for-service plans: One type offered free care; the other three types involved varying levels of cost sharing — 25 percent, 50 percent, or 95 percent coinsurance (the percentage of medical charges that the consumer must pay). The fifth type of health insurance plan was a nonprofit, HMO-style group cooperative. Those assigned to the HMO received their care free of charge. For poorer families in plans that involved cost sharing, the amount of cost sharing was income-adjusted to one of three levels: 5, 10, or 15 percent of income. Out-of-pocket spending was capped at these percentages of income or at $1,000 annually (roughly $3,000 annually if adjusted from 1977 to 2005 levels), whichever was lower.</p>
</blockquote>
<blockquote>
<p>Families participated in the experiment for 3–5 years. The upper age limit for adults at the time of enrollment was 61, so that no participants would become eligible for Medicare before the experiment ended. To assess participant service use, costs, and quality of care, RAND served as the families’ insurer and processed their claims. To assess participant health, RAND administered surveys at the beginning and end of the experiment and also conducted comprehensive physical exams. Sixty percent of participants were randomly chosen to receive exams at the beginning of the study, and all received physicals at the end. The random use of physicals at the beginning was intended to control for possible health effects that might be stimulated by the physical exam alone, independent of further participation in the experiment.</p>
</blockquote>
<ul>
<li>
<a href="https://www.povertyactionlab.org/evaluation/oregon-health-insurance-experiment-united-states">Oregon Health Insurance Experiment</a> <span class="citation">(<a href="references.html#ref-Baicker2013" role="doc-biblioref">Baicker et al. 2013</a>)</span>: this study is described at length in <a href="https://www.tellingstorieswithdata.com/hunt-data.html#case-study---the-oregon-health-insurance-experiment">Section 9.5 of Telling stories with data by Rohan Alexander</a> <span class="citation">(<a href="references.html#ref-Alexander2021" role="doc-biblioref">Alexander 2022</a>)</span>.</li>
</ul>
</div>
</div>
<div id="planning-experiments" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Planning of experiments<a class="anchor" aria-label="anchor" href="#planning-experiments"><i class="fas fa-link"></i></a>
</h2>
<p>We outline the various steps a research must undertake in an experimental setting.</p>
<p>There are four pillars in experimental designs;</p>
<ol style="list-style-type: decimal">
<li>
<strong>Control</strong>: in an experiment, the allocation to treatment is controlled by the experimenter, allowing direct comparisons between groups.</li>
<li>
<strong>Randomization</strong>: to prevent lurking variables and confounders from impacting the conclusions, observational units are randomly allocated to treatment groups so that any estimate of the effect has a causal interpretation.</li>
<li>
<strong>Replication</strong>: having multiple observations for each treatment allocation is necessary to both estimate the variability of the measurements and increase the precision of the average measurement.</li>
<li>
<strong>Blocking</strong>: a technique that allows experimenters to control the variability between experimental units by dividing them into blocks so that those in the same blocks are similar. This allows us to separate the variability due to differences between levels of the blocking variable from the overall variability, leading to precision gains.</li>
</ol>
<p>Underlying the design of experiments is use of techniques that eliminate as much as possible the variability and detecting cause and effects with the least amount of resources by allocating them wisely.</p>
</div>
<div id="requirements-for-good-experiments" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Requirements for good experiments<a class="anchor" aria-label="anchor" href="#requirements-for-good-experiments"><i class="fas fa-link"></i></a>
</h2>
<p>Section 1.2 of <span class="citation">Cox (<a href="references.html#ref-Cox:1958" role="doc-biblioref">1958</a>)</span> describes the various requirements that are necessary for experiments to be useful. These are</p>
<ol style="list-style-type: decimal">
<li>absence of systematic error</li>
<li>precision</li>
<li>range of validity</li>
<li>simplicity</li>
</ol>
<p>We review each in turn.</p>
<div id="absence-of-systematic-error" class="section level3" number="1.5.1">
<h3>
<span class="header-section-number">1.5.1</span> Absence of systematic error<a class="anchor" aria-label="anchor" href="#absence-of-systematic-error"><i class="fas fa-link"></i></a>
</h3>
<p>This point requires careful planning and listing potential confounding variables that could affect the response.</p>
<div class="example" title="Systematic error">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 1.4  </strong></span>Suppose we wish to consider the differences in student performance between two instructors. If the first teaches only morning classes, while the second only teaches in the evening, it will be impossible to disentangle the effect of timing with that of instructor performance. Such comparisons should only be undertaken if there is compelling prior evidence that timing does not impact the outcome of interest.</p>
</div>
<p>The first point raised by Cox is thus that we
&gt; ensure that experimental units receiving one treatment differ in no systematic way from those receiving another treatment.</p>
<p>This point also motivates use of <strong>double-blind</strong> procedures (where both experimenters and participants are unaware of the treatment allocation) and use of placebo and control groups.</p>
<p>Randomization is at the core of achieving this goal, and ensuring measurements are independent of one another also comes out as corolary.</p>
</div>
<div id="variability" class="section level3" number="1.5.2">
<h3>
<span class="header-section-number">1.5.2</span> Variability<a class="anchor" aria-label="anchor" href="#variability"><i class="fas fa-link"></i></a>
</h3>
<p>The second point listed by <span class="citation">Cox (<a href="references.html#ref-Cox:1958" role="doc-biblioref">1958</a>)</span> is that of the variability of estimator. Much of the precision can be captured by the signal to noise ratio, in which the effect size is divided by its standard error. The latter is a function of
(a) the accuracy of the experimental work and measurements apparatus and the intrinsic variability of the phenomenon under study, (b) the number of experimental and observational units, i.e., the sample size and (c) the choice of design and statistical procedures.</p>
<p>Point (a) typically cannot be influenced by the experimenter outside of choosing the response variable to obtain more reliable measurements. Point (c) related to the method of analysis, is oftentimes standard unless there are robustness considerations. Point (b) is at the core of the planning, notably in choosing the number of units to use and the allocation of treatment to the different (sub)-units.</p>
</div>
<div id="generalizability" class="section level3" number="1.5.3">
<h3>
<span class="header-section-number">1.5.3</span> Generalizability<a class="anchor" aria-label="anchor" href="#generalizability"><i class="fas fa-link"></i></a>
</h3>
<p>Most studies are done with an objective of generalizing the findings beyond the particular units analyzed. The range of validity thus crucially depends with the choice of population from which a sample is drawn and the particular sampling scheme. Non-random sampling severely limits the extrapolation of the results to more general settings. This leads Cox to advocate having
&gt; not just empirical knowledge about what the treatment differences are, but also some understanding of the reasons for the differences.</p>
<p>Even if we believe a factor to have no effect, it may be wise to introduce it in the experiment to check this assumption: if it is not a source of variability, it shouldn’t impact the findings and at the same time would provide some more robustness.</p>
<p>If we look at a continuous treatment, than it is probably only safe to draw conclusions within the range of doses administered. Comic in <a href="experimental-intro.html#fig:xkcd605">1.3</a> is absurd, but makes this point.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:xkcd605"></span>
<img src="figures/xkcd605_extrapolating.png" alt="xkcd comic [645 (Extrapolating) by Randall Munroe](https://xkcd.com/645/). Alt text: By the third trimester, there will be thousands of babies inside you." width="50%"><p class="caption">
Figure 1.3: xkcd comic <a href="https://xkcd.com/645/">645 (Extrapolating) by Randall Munroe</a>. Alt text: By the third trimester, there will be thousands of babies inside you.
</p>
</div>
<p>::: {.example name=“Generalizability”}
Replication studies done in university often draw participants from students enrolled in the institutions. The findings are thus not necessarily robust if extrapolated to the whole population if there are characteristics for which they have strong (familiarity to technology, acquaintance with administrative system, political views, etc). These samples are often <strong>convenience samples</strong>.
```</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 1.5  (Spratt-Archer barley in Ireland) </strong></span>Example 1.9 in <span class="citation">Cox (<a href="references.html#ref-Cox:1958" role="doc-biblioref">1958</a>)</span> mentions recollections of ``Student’’ on Spratt-Archer barley, a new variety of barley that performed well in experiments and which the Irish Department of Agriculture encouraged to have introduced elsewhere. Fuelled by a district skepticism with the new variety, the Department ran an experiment comparing the yield of the Spratt-Archer barley with that of the native race. Their findings surprised the experimenters: the native barley grew more quickly and was more resistant to weeds, leading to higher yields. It was concluded that the initial experiments were misleading because Spratt-Archer barley was experimented in well-farmed areas.</p>
</div>
</div>
<div id="simplicity" class="section level3" number="1.5.4">
<h3>
<span class="header-section-number">1.5.4</span> Simplicity<a class="anchor" aria-label="anchor" href="#simplicity"><i class="fas fa-link"></i></a>
</h3>
<p>The fourth requirement is one of simplicity of design, which almost invariably leads to simplicity of the statistical analysis. Randomized control-trials are often viewed as the golden rule for determining efficacy of policies or treatments because the set of assumptions they make is pretty minimalist due to randomization. Most researchers in management are not necessarily comfortable with advanced statistical techniques and this also minimizes the burden. <a href="experimental-intro.html#fig:xkcd2400">1.4</a> shows an <a href="https://www.zq1.de/~bernhard/images/share/mRNA-1273-trial.png">hypothetical graph</a> on the efficacy of the Moderna MRNA vaccine for Covid: if the difference is clearly visible in a suitable experimental setting, then conclusions are easily drawn.</p>
<p>Randomization justifies the use of the statistical tools we will use under very weak assumptions, if units measurements are independent from one another. Drawing conclusions from observational studies, in contrast to experimental designs requires making often unrealistic or unverifiable assumptions and the choice of techniques required to handle the lack of randomness is often beyond the toolbox of applied researchers.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:xkcd2400"></span>
<img src="figures/xkcd2400_statistics.png" alt="xkcd comic [2400 (Statistics) by Randall Munroe](https://xkcd.com/2400/). Alt text: We reject the null hypothesis based on the 'hot damn, check out this chart' test." width="50%"><p class="caption">
Figure 1.4: xkcd comic <a href="https://xkcd.com/2400/">2400 (Statistics) by Randall Munroe</a>. Alt text: We reject the null hypothesis based on the ‘hot damn, check out this chart’ test.
</p>
</div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="index.html">Preliminary remarks</a></div>
<div class="next"><a href="introduction.html"><span class="header-section-number">2</span> Introduction</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#experimental-intro"><span class="header-section-number">1</span> Introduction to Experimental Designs</a></li>
<li><a class="nav-link" href="#basic-statistical-notions"><span class="header-section-number">1.1</span> Basic statistical notions</a></li>
<li>
<a class="nav-link" href="#population-sample"><span class="header-section-number">1.2</span> Population and samples</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#variable-types"><span class="header-section-number">1.2.1</span> Variable types</a></li>
<li><a class="nav-link" href="#variables"><span class="header-section-number">1.2.2</span> Variables</a></li>
<li><a class="nav-link" href="#sampling"><span class="header-section-number">1.2.3</span> Sampling</a></li>
<li><a class="nav-link" href="#a-primer-on-causal-inference"><span class="header-section-number">1.2.4</span> A primer on causal inference</a></li>
<li><a class="nav-link" href="#observational-versus-experimental-studies"><span class="header-section-number">1.2.5</span> Observational versus experimental studies</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#examples-of-experimental-designs"><span class="header-section-number">1.3</span> Examples of experimental designs</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#evidence-based-policies"><span class="header-section-number">1.3.1</span> Evidence-based policies</a></li></ul>
</li>
<li><a class="nav-link" href="#planning-experiments"><span class="header-section-number">1.4</span> Planning of experiments</a></li>
<li>
<a class="nav-link" href="#requirements-for-good-experiments"><span class="header-section-number">1.5</span> Requirements for good experiments</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#absence-of-systematic-error"><span class="header-section-number">1.5.1</span> Absence of systematic error</a></li>
<li><a class="nav-link" href="#variability"><span class="header-section-number">1.5.2</span> Variability</a></li>
<li><a class="nav-link" href="#generalizability"><span class="header-section-number">1.5.3</span> Generalizability</a></li>
<li><a class="nav-link" href="#simplicity"><span class="header-section-number">1.5.4</span> Simplicity</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/lbelzile/math80667a/blob/master/01-introduction.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/lbelzile/math80667a/edit/master/01-introduction.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Experimental Design and Statistical Methods</strong>" was written by Léo Belzile. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
