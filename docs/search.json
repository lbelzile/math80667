[{"path":"index.html","id":"experimental-design-and-statistical-methods","chapter":"Experimental Design and Statistical Methods","heading":"Experimental Design and Statistical Methods","text":"book web complement MATH 80667A Experimental Designs Statistical Methods Quantitative Research Management, graduate course offered joint Ph.D. program Management HEC Montréal.notes licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License last compiled Monday, November 07 2022.objective course teach basic principles experimental designs statistical inference using R programming language. pay particular attention correct reporting interpretation results learn review critically scientific papers using experimental designs.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"advancement science built ability study assess research hypotheses. chapter covers basic concepts experiments, starting vocabulary associated field. Emphasis placed difference experiments observations.course covers experimental designs. experiment, researcher manipulates one features (say complexity text person must read, type advertisement campaign displayed, etc.) study impact. field causal inference concerned inferring effect treatment variable (sometimes called independent variable) response variable (dependent variable). general, however (Cox 1958)effects investigation tend masked fluctuations outside experimenter’s control.purpose experiments arrange data collection capable disentangling differences due treatment due (often large) intrinsic variation measurements. typically expect differences treatments (thus effect) comparatively stable relative measurement variation.Learning objectives:Learning terminology associated experiments.Assessing generalizability study based consideration sample characteristics, sampling scheme population.Distinguishing observational experimental studies.Understanding rationale behind requirements good experimental studies.","code":""},{"path":"introduction.html","id":"study-type","chapter":"1 Introduction","heading":"1.1 Study type","text":"two main categories studies: observational experimental. main difference two treatment assignment. observational studies, feature potential cause measured, assigned experimenter. contrast, treatment assignment mechanism fully determined experimenter latter case.example, economist studying impact interest rates price housing can look historical records sales. Similarly, surveys studying labour market also observational: people influence type job performed employees social benefits see happened. Observational studies can lead detection association, experiment researcher controls allocation mechanism randomization can lead directly establish existence causal relationship. everything else well controlled experiment, treatment effect principle caused experimental manipulation.1\nFigure 1.1: Two two classification matrix experiments based sampling study type. Source: Mine Çetinkaya-Rundel OpenIntro, distributed CC -SA license.\nFigure 1.1 summarizes two preceding sections. Random allocation observational units treatment random samples population lead ideal studies, may impossible due ethical considerations.","code":""},{"path":"introduction.html","id":"terminology","chapter":"1 Introduction","heading":"1.2 Terminology","text":"simplest form, experimental design comparison two treatments (experimental conditions):subjects (experimental units) different groups treatment similar characteristics treated exactly way experimentation except treatment receiving. Formally, experimental unit smallest division two units may receive different treatments.observational unit smallest level (time point, individual) measurement recorded.Explanatories (independent variables) variables impact response. can continuous (dose) categorical variables; latter case, termed factors.experimental treatments conditions manipulated controlled researcher. Oftentimes, control baseline treatment relative measure improvement (e.g., placebo drugs).Additional explanatories intrinsic experimental (sub-)units termed blocking variables. Controlling typically leads improved inferences, filter unwanted variability generate.different treatments administered subjects participating study, researcher measures one outcomes (also called responses dependent variables) subject.Observed differences outcome variable experimental conditions (treatments) called treatment effects.Example 1.1  (Pedagogical experience) Suppose want study effectiveness different pedagogical approaches learning. Evidence-based pedagogical researchs point active learning leads higher retention information. corroborate research hypothesis, can design experiment different sections course assigned different teaching methods. example, student class group receives teaching assignment, experimental units sections observational units individual students. treatment teaching method (traditional teaching versus flipped classroom).Potential blocking factors experiment include strength individuals, reflects prior exposure topic, knowledge, maturity, etc. measured using preliminary exam assignment students class groups completed. Additional factors worth controlling include timing classroom (morning, afternoon, evening classes) instructors.marketing department company wants know value brand determining much customers willing pay product relative cheaper generic product offered store. Economic theory suggests substitution effect: customers may prefer brand product, switch generic version price tag high. check theory, one design experiment.researcher, conduct study? Identify specific product. latter, definean adequate response variablethe experimental observational unitspotential blocking factorsThe main reason experiments preferred collection observational data allow us, conducted properly, draw causal conclusions phenomenon interest. take random sample population interest, split randomly manipulate certain aspects, differences groups must due changes.Hariton Locascio (2018) put :Randomised controlled trials (RCTs) reference standard studying causal\nrelationships interventions outcomes randomisation eliminates much \nbias inherent study designsQuasi experimentsSometimes, impossible unethical conduct experiment. seemingly precludes study many social phenomena, effect women infantile mortality strict bans abortions. changes legislation occur (Supreme court overturning Roe Wade), offers window compare neighbouring states.Canadian economist David Card co-awarded 2021 Nobel Memorial Prize Economic Sciences work experimental economics. One cited paper Card Krueger (1994), study looked impact increase minimum wage employment figures. Card Krueger (1994) used planned increase minimum wage $0.80 USD New Jersey make comparisons neighbouring Eastern Pennsylvania counties studying 410 fast food outlets. authors found evidence negative impact employment hike.Point terminology: internal external validityA study can study causal relationships said internal validity. design, good experiments desirable property random allocation treatment guarantees, randomization well performed, effect interest causal. many aspects, covered class, can threaten internal validity.External validity refers directly generalizability conclusions study: Figure 1.1 shows external validity directly related random sampling populationPoint terminology: -subjects within-subjects designsIn -subjects designs, subjects randomly assigned one different\nexperimental conditions. contrary, participants receive many experimental treatments within-subjects design, order assignment conditions typically random.within-subject designs allow better use available ressources (cheaper fewer participants perform multiple tasks), observations within-design correlated subject missingness learning effects, require special statistical treatment.","code":""},{"path":"introduction.html","id":"review-of-basic-concepts","chapter":"1 Introduction","heading":"1.3 Review of basic concepts","text":"","code":""},{"path":"introduction.html","id":"variables","chapter":"1 Introduction","heading":"1.3.1 Variables","text":"choice statistical model test depends underlying type data collected. many choices: quantitative (discrete continuous) variables numeric, qualitative (binary, nominal, ordinal) can described using adjective; prefer term categorical, evocative. choice graphical representation data contingent variable type. Specifically,variable represents characteristic population, example sex individual, price item, etc.observation set measures (variables) collected identical conditions individual given time.\nFigure 1.2: Illustration continuous (left) discrete variables (right). Artwork Allison Horst shared CC 4.0 license.\nmodels deal -called regression models, mean quantitative variable function variables, termed explanatories. two types numerical variablesa discrete variable takes countable number values, prime examples binary variables count variables.continuous variable can take (theory) infinite possible number values, even measurements rounded measured limited precision (time, width, mass). many case, also consider discrete variables continuous take enough values (e.g., money).Categorical variables take finite values. regrouped two groups, nominal ordering levels (sex, colour, country origin) ordinal ordered (Likert scale, salary scale) ordering reflected graphs tables. bundle every categorical variable using arbitrary encoding levels: modelling, variables taking \\(K\\) possible values (levels) must transformed set \\(K-1\\) binary variables \\(T_1, \\ldots, T_K\\), corresponds logical group \\(k\\) (yes = 1, = 0), omitted level corresponding baseline \\(K-1\\) indicators zero. Failing declare categorical variables software common mistake, especially saved database using integers (1,2, \\(\\ldots\\)) rather text (Monday, Tuesday, \\(\\ldots\\)).\nFigure 1.3: Examples categorical nominal (left), ordinal (middle) binary (right) variables. Artwork Allison Horst shared CC 4.0 license.\ncan characterize set potential values measurements can take, together frequency, via distribution. latter can represented graphically using histogram density plot2 data continuous, bar plot discrete categorical measurements.Example 1.2  (Die toss) distribution outcomes die toss discrete takes values \\(1, \\ldots, 6\\). outcome equally likely probability 1/6.Example 1.3  (Normal distribution) Mathematical theory suggests , general conditions, distribution sample average approximately distributed according normal (aka Gaussian) distribution: result central statistics. Normally distributed data continuous; distribution characterized bell curve, light tails symmetric around ’s mean. shape facade Hallgrímskirkja church Reykjavik, shown Figure 1.4, closely resembles density normal distribution, lead Khoa Vu call ‘normal church’ (chuckles).\nFigure 1.4: Photography Hallgrímskirkja church Reykjavik, Iceland Dolf van der Haven, reproduced CC -ND-NC 2.0 license.\nnormal distribution fully characterized two parameters: average \\(\\mu\\) standard deviation \\(\\sigma\\). left panel Figure 1.5 shows arbitrary continuous distribution values random sample \\(n=1000\\) draws. right panel shows histogram sample mean value based large number random samples size \\(n=25\\), drawn distribution. superimposed black curve normal density curve whose parameters match given central limit theorem: approximation seemingly quite accurate.fact explains omnipresence normal distribution introductory data science courses, well prevalence sample mean sample variance key summary statistics.3\nFigure 1.5: Graphical representation distribution continuous variable, histogram sample \\(n=1000\\) observations drawn distribution (left) distribution sample mean, obtained repeatedly drawing random sample \\(n=25\\) observations computing average (right). curve shows normal distribution approximation based central limit theorem.\nOne key aspect, often neglected studies, discussion metric used measurement response. previous research may identified instruments (like questionnaires) particular wording studying particular aspect individuals, lot free room researchers choose may impact conclusions. example, one uses Likert scale, range scale? coarse choice may lead limited capability detect, truthfulness, may\nlarger intrinsic measurement finer scale.Likewise, many continuous measures (say fMRI signal) can discretized provide single numerical value. Choosing average signal, range, etc. outcome variable may lead different conclusions.Choosing particular instrument metric principle done studying (apriori) distribution values chosen metric using pilot study: give researchers grasp variability measures.heart analysis measurements. data presented course cleaned oftentimes choice explanatory variables experimental factor4 evident context. applications, however, choice always trivial.","code":""},{"path":"introduction.html","id":"population-sample","chapter":"1 Introduction","heading":"1.3.2 Population and samples","text":"well-designed sampling schemes results generalize beyond group observed. thus paramount importance define objective population interest want make conclusions.Generally, seek estimate characteristics population using sample (sub-group population smaller size). population interest collection individuals study targets. example, Labour Force Survey (LFS) monthly study conducted Statistics Canada, define target population “members selected household 15 years old older, whether work .” Asking every Canadian meeting definition costly process long: characteristic interest (employment) also snapshot time can vary person leaves job, enters job market become unemployed. example, collecting census impossible costly.general, therefore consider samples gather information seek obtain. purpose statistical inference draw conclusions population, using share latter accounting sources variability. pollster George Gallup made great analogy sample population:One spoonful can reflect taste whole pot, soup well-stirredA sample sub-group individuals drawn random population. won’t focus data collection, keep mind following information: sample good, must representative population study.Parcours AGIR HEC Montréal pilot project Bachelor Administration students initiated study impact flipped classroom active learning performance.think can draw conclusions efficacy teaching method comparing results students rest bachelor program? List potential issues approach addressing internal external validity, generalizability, effect lurking variables, etc.individuals selected random part sample, measurement characteristic interest also random change one sample next. larger samples typically carry information, sample size guarantee quality, following example demonstrates.Example 1.4  (Polling 1936 USA Presidential Election) Literary Digest surveyed 10 millions people mail know voting preferences 1936 USA Presidential Election. sizeable share, 2.4 millions answered, giving Alf Landon (57%) incumbent President Franklin D. Roosevelt (43%). latter nevertheless won landslide election 62% votes cast, 19% forecast error. Biased sampling differential non-response mostly responsible error: sampling frame built using ``phone number directories, drivers’ registrations, club memberships, etc.’’, skewed sample towards rich upper class white people susceptible vote GOP.contrast, Gallup correctly predicted outcome polling () 50K inhabitants. Read full story .considerations guide determining population interest study?","code":""},{"path":"introduction.html","id":"sampling","chapter":"1 Introduction","heading":"1.3.3 Sampling","text":"sampling costly, can collect limited information variable interest, drawing population sampling frame (phone books, population register, etc.) Good sampling frames can purchased sampling firms.general, randomization necessary order obtain representative sample5, one match characteristics population. Failing randomize leads introduction bias generally conclusions drawn study won’t generalizable.Even observational units selected random participate, may bias introduced due non-response. 1950s, conducting surveys relatively easier people listed telephone books; nowadays, sampling firms rely mix interactive voice response live callers, sampling frames mixing landlines, cellphones online panels together (heavy) weighting correct non-response. Sampling difficult problem engage cursorily, readers urged exercise scrutiny reading papers.Reflect choice platform used collect answers think influence composition sample returned affect non-response systematic way.examining problems related sampling, review main random sampling methods. simplest simple random sampling, whereby \\(n\\) units drawn completely random (uniformly) \\(N\\) elements sampling frame. second common scheme stratified sampling, whereby certain numbers units drawn uniformly strata, namely subgroups (e.g., gender). Finally, cluster sampling consists sampling subgroups.Example 1.5  (Illustration sampling schemes) Suppose wish look student satisfaction regarding material taught introductory statistics course offered multiple sections. population consists students enrolled course given semester list provides sampling frame. can define strata consist class group. simple random sample obtaining sampling randomly abstracting class groups, stratified sample drawing randomly number class group cluster sampling drawing students selected class groups. Cluster sampling mostly useful groups similar costs associated sampling multiple strata expensive.\nFigure 1.6: Illustration three sampling schemes nine groups: simple random sampling (left), stratified sampling (middle) cluster sampling (right). middle, grouping corresponds stratum (e.g., age bands) whereas right contains cluster (e.g., villages classrooms)\nStratified sampling typically superior care similar proportions sampled group useful reweighting: 1.6, true proportion sampled 1/3, simple random sampling range [0.22, 0.39] among strata, compared [0.32, 0.34] stratified sample.credibility study relies large part quality data collection. customary report descriptive statistics sample description population?instances sampling, non-random avoided whenever possible. include convenience samples, consisting observational units easy access include (e.g., friends, students university, passerby street). Much like anecdotal reports, observational units need representative whole population difficult understand relate latter.recent years, proliferation studies employing data obtained web experimentation plateforms Amazon’s Mechanical Turk (MTurk), point Journal Management commissioned review (Aguinis, Villamor, Ramani 2021). samples subject self-selection bias articles using read healthy dose skepticism. Unless good manipulation checks conducted (e.g., ensure participants faithful answer reasonable amount time), reserve tools paired samples (e.g., asking people perform multiple tasks presented random order) composition population less important. make sure sample matches target population, can use statistical tests informal comparison compare distribution individuals composition obtained census.\nFigure 1.7: Sampling bias. Artwork Jonathan Hey (Sketchplanations), licensed CC--SA 4.0\n","code":""},{"path":"introduction.html","id":"examples-of-experimental-designs","chapter":"1 Introduction","heading":"1.4 Examples of experimental designs","text":"field experimental design long history, starting agricultural field trials.Example 1.6  (Agricultural field trials Rothamsted Research Station.) Rothamsted Research Station UK conducting experiments since 1843. Ronald . Fisher, worked 14 years Rothamsted 1919, developed much statistical theory underlying experimental design, inspired work . Yates (1964) provides recollection contribution field.\nFigure 1.8: 1958 plan Highfield Ley–Arable Experiment. Source: Rothamsted Research Station, reproduced CC 4.0 license.\nExperimental design revolves large part understanding best allocate resources, determine impact policies choosing effective “treatment” series option.Example 1.7  (Modern experiments /B testing) modern experiments happen online, tech companies running thousands experiments ongoing basis order discover improvement interfaces lead increased profits. Harvard Business Review article (Kohavi Thomke 2017) details small tweaks display advertisements Microsoft Bing search engine landing page lead whooping 12% increase revenues. randomized control trials, termed /B experiments, involve splitting incoming traffic separate groups; group see different views webpage differ ever slightly. experimenters compare traffic click revenues. large scale, even small effects can major financial consequences can learned despite large variability customer behaviour.also multiple examples randomized control experiments used policy making.Example 1.8  (Experiments wellness programs) Song Baicker (2019) conducted large randomized trial period 18 months study impact wellness programs US companies. industry, worth 8 billions USD, significantly increased following passage Affordable Care Act, aka Obamacare. findings vulgarized press release Jake Miller Harvard News & Research: show , seemingly impact physical activity well-, evidence changes absenteeism, job tenure job performance. Jones, Molitor, Reif (2019) reach similar conclusion.findings strikingly different previous observational studies, found increase participation sportive activities, increased job duration, reduced medical spendings.Example 1.9  (STAR) Tennessee’s Student Teacher Achievement Ratio (STAR) project (Achilles et al. 2008) another important example large scale experiment broad ramifications. study suggested smaller class sizes lead better outcomes pupils.7,000 students 79 schools randomly assigned one 3 interventions: small class (13 17 students per teacher), regular class (22 25 students per teacher), regular--aide class (22 25 students full-time teacher’s aide). Classroom teachers also randomly assigned classes teach. interventions initiated students entered school kindergarten continued third grade.Example 1.10  (RAND health care programs) large-scale multiyear experiment conducted RAND Corporation (Brook et al. 2006), participants paid share health care used fewer health services comparison group given free care. study concluded cost sharing reduced “inappropriate unnecessary” medical care (overutilization), also reduced “appropriate needed” medical care.HIE large-scale, randomized experiment conducted 1971 1982. study, RAND recruited 2,750 families encompassing 7,700 individuals, age 65. chosen six sites across United States provide regional urban/rural balance. Participants randomly assigned one five types health insurance plans created specifically experiment. four basic types fee--service plans: One type offered free care; three types involved varying levels cost sharing — 25 percent, 50 percent, 95 percent coinsurance (percentage medical charges consumer must pay). fifth type health insurance plan nonprofit, HMO-style group cooperative. assigned HMO received care free charge. poorer families plans involved cost sharing, amount cost sharing income-adjusted one three levels: 5, 10, 15 percent income. --pocket spending capped percentages income $1,000 annually (roughly $3,000 annually adjusted 1977 2005 levels), whichever lower.Families participated experiment 3–5 years. upper age limit adults time enrollment 61, participants become eligible Medicare experiment ended. assess participant service use, costs, quality care, RAND served families’ insurer processed claims. assess participant health, RAND administered surveys beginning end experiment also conducted comprehensive physical exams. Sixty percent participants randomly chosen receive exams beginning study, received physicals end. random use physicals beginning intended control possible health effects might stimulated physical exam alone, independent participation experiment.many great examples dedicated section Chapter 10 Telling stories data Rohan Alexander (Alexander 2022). Section 1.4 Berger, Maurer, Celli (2018) also lists various applications experimental designs variety fields.","code":""},{"path":"introduction.html","id":"requirements-for-good-experiments","chapter":"1 Introduction","heading":"1.5 Requirements for good experiments","text":"Section 1.2 Cox (1958) describes various requirements necessary experiments useful. areabsence systematic errorprecisionrange validitysimplicityWe review turn.","code":""},{"path":"introduction.html","id":"absence-of-systematic-error","chapter":"1 Introduction","heading":"1.5.1 Absence of systematic error","text":"point requires careful planning listing potential confounding variables affect response.Example 1.11  Suppose wish consider differences student performance two instructors. first teaches morning classes, second teaches evening, impossible disentangle effect timing instructor performance. comparisons undertaken compelling prior evidence timing impact outcome interest.first point raised Cox thus weensure experimental units receiving one treatment differ systematic way receiving another treatment.point also motivates use double-blind procedures (experimenters participants unaware treatment allocation) use placebo control groups (avoid psychological effects, etc. associated receiving treatment lack thereof).Randomization6 core achieving goal, ensuring measurements independent one another also comes corollary.","code":""},{"path":"introduction.html","id":"variability","chapter":"1 Introduction","heading":"1.5.2 Variability","text":"second point listed Cox (1958) variability estimator. Much precision can captured signal--noise ratio, difference mean treatment divided standard error, form effect size. intuition ’s easier detect something signal large background noise low. latter function ofthe accuracy experimental work measurements apparatus intrinsic variability phenomenon study,number experimental observational units (sample size).choice design statistical procedures.Point () typically influenced experimenter outside choosing response variable obtain reliable measurements. Point (c) related method analysis, usually standard unless robustness considerations. Point (b) core planning, notably choosing number units use allocation treatment different (sub)-units.","code":""},{"path":"introduction.html","id":"generalizability","chapter":"1 Introduction","heading":"1.5.3 Generalizability","text":"studies done objective generalizing findings beyond particular units analyzed. range validity thus crucially depends choice population sample drawn particular sampling scheme. Non-random sampling severely limits extrapolation results general settings. leads Cox advocate havingnot just empirical knowledge treatment differences , also understanding reasons differences.Even believe factor effect, may wise introduce experiment check assumption: source variability, shouldn’t impact findings time provide robustness.look continuous treatment, probably safe draw conclusions within range doses administered. Comic 1.9 absurd, makes point.\nFigure 1.9: xkcd comic 605 (Extrapolating) Randall Munroe. Alt text: third trimester, thousands babies inside . Cartoon reprinted CC -NC 2.5 license.\nExample 1.12  (Generalizability) Replication studies done university often draw participants students enrolled institutions. findings thus necessarily robust extrapolated whole population characteristics strong (familiarity technology, acquaintance administrative system, political views, etc). samples often convenience samples.Example 1.13  (Spratt-Archer barley Ireland) Example 1.9 Cox (1958) mentions recollections “Student”7 Spratt-Archer barley, new variety barley performed well experiments whose culture Irish Department Agriculture encouraged. Fuelled district skepticism new variety, Department ran experiment comparing yield Spratt-Archer barley native race. findings surprised experimenters: native barley grew quickly resistant weeds, leading higher yields. concluded initial experiments misleading Spratt-Archer barley experimented well-farmed areas, exempt nuisance.","code":""},{"path":"introduction.html","id":"simplicity","chapter":"1 Introduction","heading":"1.5.4 Simplicity","text":"fourth requirement one simplicity design, almost invariably leads simplicity statistical analysis. Randomized control-trials often viewed golden rule determining efficacy policies treatments set assumptions make pretty minimalist due randomization. researchers management necessarily comfortable advanced statistical techniques also minimizes burden. Figure 1.10 shows hypothetical graph efficacy Moderna MRNA vaccine Covid: difference clearly visible suitable experimental setting, conclusions easily drawn.Randomization justifies use statistical tools use weak assumptions, units measurements independent one another. Drawing conclusions observational studies, contrast experimental designs, requires making often unrealistic unverifiable assumptions choice techniques required handle lack randomness often beyond toolbox applied researchers.\nFigure 1.10: xkcd comic 2400 (Statistics) Randall Munroe. Alt text: reject null hypothesis based ‘hot damn, check chart’ test. Cartoon reprinted CC -NC 2.5 license.\nDefine following terms word: experimental unit, factor, treatment.main benefit experimental studies observational studies?","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"2 Hypothesis testing","heading":"2 Hypothesis testing","text":"applied domains, empirical evidences drive advancement field data well designed experiments contribute built science. order draw conclusions favour theory, researchers turn (often unwillingly) statistics back claims. led prevalence use null hypothesis statistical testing (NHST) framework. One important aspect reproducibility crisis misuse \\(p\\)-values journal articles: falsification null hypothesis enough provide substantive findings theory.introductory statistics course typically present hypothesis tests without giving much thoughts underlying construction principles procedures, users often reductive view statistics catalogue pre-determined procedures. make culinary analogy, users focus learning recipes rather trying understand basics cookery. chapter focuses understanding key ideas related testing.Learning objectives:Understanding role uncertainty decision making.Understanding importance signal--noise ratio measure evidence.Knowing basic ingredients hypothesis testing capable correctly formulating identifying components paper.Correctly interpreting \\(p\\)-values confidence intervals parameter.","code":""},{"path":"hypothesis-testing.html","id":"hypothesis","chapter":"2 Hypothesis testing","heading":"2.1 Hypothesis","text":"first step design formulating research question. Generally, hypothesis specify potential differences population characteristics due intervention (treatment) researcher wants quantify. step researchers decide sample size, choice response variable metric measurement, write study plan, etc.important note research questions answered simple tools. Researchers wishing perform innovative methodological research contact experts consult statisticians collect data get information best proceed mind avoid risk making misleading false claims based incorrect analysis data collection.\nFigure 2.1: xkcd comic 2569 (Hypothesis generation) Randall Munroe. Alt text: Frazzled scientists requesting everyone please stop generating hypotheses little bit work backlog. Cartoon reprinted CC -NC 2.5 license.\n","code":""},{"path":"hypothesis-testing.html","id":"sampling-variability","chapter":"2 Hypothesis testing","heading":"2.2 Sampling variability","text":"Given data, researcher interested estimating particular characteristics population. can characterize set potential values measurements can take, together frequency, via distribution.purpose section illustrate simply use raw differences groups make meaningful comparisons: due sampling variability, samples alike even generated way, always differences summary statistics. differences tend attenuate (increase) collect sample. Inherent fact gather data (thus information) target, portrait becomes precise. ultimately allows us draw meaningful conclusions , order , need first determine likely plausible stroke luck, likely occur solely due randomness.Example 2.1  (/B testing) Consider two webpage design: one current version (status quo) implementation contains clickable banner location eyetracker suggest viewers eyes spend time attention. number clicks headlines generate longer viewing, thus higher revenues advertisement. characteristic interest average click conversation rate webpage design.fairly simple redirect traffic random fraction gets assigned new design study. suitable period time, data can analyzed see new webpage generates clicks.hypothesis test focus one multiple characteristics. Suppose simplicity two groups, control treatment, whose population averages \\(\\mu_C\\) \\(\\mu_T\\) wish compare. People commonly look difference average, say \\(\\delta=\\mu_T - \\mu_C\\) measure effectiveness treatment.8 properly randomized observations subgroup nothing else changes, measures impact treatment. sample hand whole population, don’t know sure values \\(\\mu_C\\) \\(\\mu_T\\). quantities exist, unknown us best can estimate using sample. random sample population, characteristics sample (noisy) proxys population.call numerical summaries data statistics. important distinguish procedures/formulas numerical values. estimator rule formula used calculate estimate parameter quantity interest based observed data (like recipe cake). observed data can actually compute sample mean, , estimate — actual value (cake), single realization random. words,estimand conceptual target, like population characteristic interest (population mean).estimator procedure formula telling us transform sample data numerical summary proxy target.estimate number, numerical value obtained apply formula observed data.\nFigure 2.2: Estimand (left), estimator (middle) estimate (right) illustrated cakes based original idea Simon Grund. Cake photos shared CC -NC 2.0 license.\nexample, may use estimand population average \\(Y_1, \\ldots\\), say \\(\\mu\\). estimator sample mean, .e., sum elements sample divided sample size, \\(\\overline{Y}=(Y_1 + \\cdots + Y_n)/n\\). estimate numerical value, say 4.3.inputs estimator random, output also random change one sample next: even repeat recipe, won’t get exact result every time.\nFigure 1.9: xkcd comic 2581 (Health Stats) Randall Munroe. Alt text: live forever hearts, pushing little extra blood toward left hands now give squeeze. Cartoon reprinted CC -NC 2.5 license.\nillustrate point, Figure 2.3 shows five simple random samples size \\(n=10\\) drawn hypothetical population mean \\(\\mu\\) standard deviation \\(\\sigma\\), along sample mean \\(\\overline{y}\\). Thus, sampling variability implies sample means subgroups always differ even share characteristics. can view sampling variability noise: goal extract signal (typically differences means) accounting spurious results due background noise.\nFigure 2.3: Five samples size \\(n=10\\) drawn common population mean \\(\\mu\\) (horizontal line). colored segments show sample means sample.\ncan clearly see Figure 2.3 , even sample drawn population, sample mean varies one sample next result sampling variability. astute eye might even notice sample means less dispersed around full black horizontal line representing population average \\(\\mu\\) individual measurements. fundamental principle statistics: information accumulates get data.Values sample mean don’t tell whole picture studying differences mean (groups, relative postulated reference value) enough draw conclusions. settings, guarantee sample mean equal ’s true value changes one sample next: guarantee average equal population average repeated samples. Depending choice measurement variability population, may considerable differences one observation next means observed difference fluke.get idea certain something , consider variability observation \\(Y_i\\). variance observation drawn population typically denoted \\(\\sigma^2\\) ’s square root, standard deviation, \\(\\sigma\\).sample variance \\(S_n\\) estimator standard deviation \\(\\sigma\\), \n\\[\\begin{align*}\nS^2_n &= \\frac{1}{n-1} \\sum_{=1}^n (Y_i-\\overline{Y})^2\n\\end{align*}\\]\nsum squared difference observations sample average, scaled factor proportional sample size.standard deviation statistic termed standard error; confused standard deviation \\(\\sigma\\) population sample observations \\(Y_1, \\ldots, Y_n\\) drawn. standard deviation standard error expressed units measurements, easier interpret variance. Since standard error function sample size, however good practice report estimated standard deviation reports.\nFigure 2.4: Histograms 10 random samples size \\(n=20\\) discrete uniform distribution.\nEven drawn population, 10 samples Figure 2.4 look quite different. thing play sample variability: since \\(n=20\\) observations total, average 10% observations 10 bins, bins empty others counts expected. fluctuation due randomness, chance.can thus detect whether see compatible model think generated data? key collect observations: bar height sample proportion, average 0/1 values ones indicating observation bin zero otherwise.Consider now happens increase sample size: top panel Figure 2.5 shows uniform samples increasing samples size. histogram looks like true underlying distribution (flat, bin equal frequency) sample size increases. sample distribution points nearly indistinguishable theoretical one (straight line) \\(n=10 000\\).9 bottom panel, hand, isn’t uniform distribution larger samples come closer population distribution. couldn’t spotted difference first two plots, since sampling variability important; , lack data bins attributed chance, comparable graph data truly uniform. line practical applications, limited sample size restricts capacity disentangle real differences sampling variability. must embrace uncertainty: next section, outline hypothesis testing helps us disentangle signal noise.\nFigure 2.5: Histograms data uniform distribution (top) non-uniform (bottom) increasing sample sizes 10, 100, 1000 10 000 (left right).\n","code":""},{"path":"hypothesis-testing.html","id":"tests","chapter":"2 Hypothesis testing","heading":"2.3 Hypothesis testing","text":"hypothesis test binary decision rule (yes/) used evaluate statistical evidence provided sample make decision regarding underlying population. main steps involved :define model parametersformulate alternative null hypothesischoose calculate test statisticobtain null distribution describing behaviour test statistic \\(\\mathscr{H}_0\\)calculate p-valueconclude (reject fail reject \\(\\mathscr{H}_0\\)) context problem.good analogy hypothesis tests trial murder appointed juror.judge lets choose two mutually exclusive outcome, guilty guilty, based evidence presented court.presumption innocence applies evidences judged optic: evidence remotely plausible person innocent? burden proof lies prosecution avoid much possible judicial errors. null hypothesis \\(\\mathscr{H}_0\\) guilty, whereas alternative \\(\\mathscr{H}_a\\) guilty. reasonable doubt, verdict trial guilty.test statistic (choice test) represents summary proof. overwhelming evidence, higher chance accused declared guilty. prosecutor chooses proof best outline : choice evidence (statistic) ultimately maximize evidence, parallels power test.null distribution benchmark judge evidence (jurisprudence). Given proof, odds assuming person innocent? Since possibly different every test, common report instead p-value, gives level evidence uniform scale easily interpreted.final step verdict, binary decision outcomes: guilty guilty. hypothesis test performed level \\(\\alpha\\), one reject (guilty) p-value less \\(\\alpha\\). Even declare person guilty, doesn’t mean defendant innocent vice-versa.","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-1","chapter":"2 Hypothesis testing","heading":"2.3.1 Hypothesis","text":"statistical tests two hypotheses: null hypothesis (\\(\\mathscr{H}_0\\)) alternative hypothesis (\\(\\mathscr{H}_a\\)). Usually, null hypothesis (‘status quo’) single numerical value. alternative ’re really interested testing. Figure 2.3, consider whether five groups mean \\(\\mathscr{H}_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_5\\) alternative least two different. two outcomes mutually exclusive cover possible scenarios. statistical hypothesis test allows us decide whether data provides enough evidence reject \\(\\mathscr{H}_0\\) favor \\(\\mathscr{H}_a\\), subject pre-specified risk error: know differences just due sampling variability Figure 2.3 data simulated, practice need assess evidence using numerical summary.Example 2.3  (/B testing (continued)) follow-/B test experiment. Given \\(\\mu_1\\) population average click conversation rate current webpage \\(\\mu_2\\), redesign, interested one-sided hypothesis \\(\\mathscr{H}_0: \\mu_2 \\leq \\mu_1\\) alternative (trying prove) \\(\\mathscr{H}_a: \\mu_2 > \\mu_1\\). choosing null hypothesis new design better worst, putting weight make sure changes carry forward overwhelming evidence new design better allow us generate revenues, given costs associated changes interface resulting disruption.One-sided hypothesis directional: care specific direction, \\(\\mathscr{H}_a: \\mu_2 > \\mu_1\\). Indeed, experiment suggests conversion rate worst new webpage design, won’t go forward.Since neither population averages \\(\\mu_1\\) \\(\\mu_2\\) known us, can work instead \\(\\mathscr{H}_0: \\mu_2-\\mu_1 \\geq 0\\). can use estimator difference \\(\\mu_2-\\mu_1\\) difference sample average subgroup.null hypothesis interval, suffices consider beneficial scenario, \\(\\mu_2-\\mu_1=0\\). Indeed, can disprove difference see increase click rate updated version, extreme cases automatically discarded favour alternative new design better.One-sided tests evidence runs contrary hypothesis (say mean conversion rate higher current design new one) lead p-values 1, since proof null hypothesis old design (status quo) better.previous example illustrates fact , writing null alternative hypotheses, trying prove typically alternative.pairwise comparisons contrasts, can assign directionality. benefit , sure direction postulated effect, consider extreme scenarios run direction postulated10 However, empirical evidence runs contrary guess, support hypothesis.general statistical models, helps view null hypothesis simplification complex model: latter fit data better flexible, fail reject null unless improvement drastic. example, analysis variance model, compare different mean \\(K\\) groups single common average.","code":""},{"path":"hypothesis-testing.html","id":"test-statistic","chapter":"2 Hypothesis testing","heading":"2.3.2 Test statistic","text":"test statistic \\(T\\) function data takes data input outputs summary information contained sample characteristic interest, say population mean. order assess whether numerical value \\(T\\) unusual, need know potential values taken \\(T\\) relative probability \\(\\mathscr{H}_0\\) true. need know values expect , e.g., difference averages different groups: requires benchmark.Many statistics consider form11\n\\[\\begin{align*}\nT = \\frac{\\text{estimated effect}- \\text{postulated effect}}{\\text{estimated effect variability}} = \\frac{\\widehat{\\theta} - \\theta_0}{\\mathrm{se}(\\widehat{\\theta})}\n\\end{align*}\\]\n\\(\\widehat{\\theta}\\) estimator \\(\\theta\\), \\(\\theta_0\\) postulated value parameter \\(\\mathrm{se}(\\widehat{\\theta})\\) standard error test statistic \\(\\widehat{\\theta}\\). quantity designed , postulated value \\(\\theta_0\\) correct, \\(T\\) approximately mean zero variance one. standardization makes comparison easier; fact, form test statistic chosen doesn’t depend measurement units.example, interested mean differences treatment group control group, denoted \\(\\mu_T\\) \\(\\mu_C\\), \\(\\theta = \\mu_T-\\mu_C\\) \\(\\mathscr{H}_0: \\mu_T = \\mu_C\\) corresponds \\(\\mathscr{H}_0: \\theta = 0\\) difference. two-sample \\(t\\)-test numerator \\(\\widehat{\\theta} = \\overline{Y}_T - \\overline{Y}_C\\), \\(\\overline{Y}_T\\) sample average treatment group \\(\\overline{Y}_C\\) control group. postulated value mean difference zero.numerator thus consist difference sample means denominator standard error quantity, calculated using software.12","code":""},{"path":"hypothesis-testing.html","id":"null-distribution-and-p-value","chapter":"2 Hypothesis testing","heading":"2.3.3 Null distribution and p-value","text":"p-value allows us decide whether observed value test statistic \\(T\\) plausible \\(\\mathscr{H}_0\\). Specifically, p-value probability test statistic equal extreme estimate computed data, assuming \\(\\mathscr{H}_0\\) true. Suppose based random sample \\(Y_1, \\ldots, Y_n\\) obtain statistic whose value \\(T=t\\). two-sided test \\(\\mathscr{H}_0:\\theta=\\theta_0\\) vs. \\(\\mathscr{H}_a:\\theta \\neq \\theta_0\\), p-value \\(\\mathsf{Pr}_0(|T| \\geq |t|)\\).13How determine null distribution given true data generating mechanism unknown us? ask statistician! simple cases, might possible enumerate possible outcomes thus quantity degree outlyingness observed statistic. general settings, can resort simulations probability theory: central limit theorem says sample mean behaves like normal random variable mean \\(\\mu\\) standard deviation \\(\\sigma/\\sqrt{n}\\) \\(n\\) large enough. central limit theorem broader applications since statistics can viewed form average transformation thereof, fact used derive benchmarks commonly used tests. software use approximations proxy default: normal, Student’s \\(t\\), \\(\\chi^2\\) \\(F\\) distributions reference distributions arise often.\nFigure 2.6: Density p-values null hypothesis (left) alternative signal--noise ratio 0.5 (right). probability rejection obtained calculating area density curve zero \\(\\alpha=0.1\\), 0.1. null, model calibrated distribution p-values uniform (.e., flat rectangle height 1), meaning values unit interval equally likely. alternative (right), small p-values likely observed.\ngenerally three ways obtaining null distributions assessing degree evidence null hypothesisexact calculationslarge sample theory (aka ‘asymptotics’ statistical lingo)simulationWhile desirable, first method applicable simple cases (counting probability getting two six throw two fair die). second method commonly used due generality ease use (particularly older times computing power scarce), fares poorly small sample sizes (‘small’ context test-dependent). last approach can used approximate null distribution many scenarios, adds layer randomness extra computations costs sometimes worth .","code":""},{"path":"hypothesis-testing.html","id":"conclusion","chapter":"2 Hypothesis testing","heading":"2.3.4 Conclusion","text":"p-value allows us make decision null hypothesis. \\(\\mathscr{H}_0\\) true, p-value follows uniform distribution, shown Figure 2.6. Thus, p-value small, means observing outcome extreme \\(T=t\\) unlikely, ’re inclined think \\(\\mathscr{H}_0\\) true. ’s always underlying risk ’re making mistake make decision. statistic, two type errors:type error: reject null hypothesis \\(\\mathscr{H}_0\\) null true,type II error: fail reject null hypothesis \\(\\mathscr{H}_0\\) alternative true.two hypothesis judged equally: seek avoid error type (judicial errors, corresponding condamning innocent). prevent , fix level test, \\(\\alpha\\), captures tolerance risk commiting type error: higher level test \\(\\alpha\\), often reject null hypothesis latter true. value \\(\\alpha \\(0, 1)\\) probability rejecting \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) fact true,\n\\[\\begin{align*}\n\\alpha = \\mathsf{Pr}_0\\left(\\text{ reject } \\mathscr{H}_0\\right).\n\\end{align*}\\]\nlevel \\(\\alpha\\) fixed beforehand, typically \\(1\\)%, \\(5\\)% \\(10\\)%. Keep mind probability type error \\(\\alpha\\) null model \\(\\mathscr{H}_0\\) correct (sic) correspond data generating mechanism.focus type error best understood thinking costs moving away status quo: new website design branding costly implement, want make sure enough evidence proposal better alternative lead increased traffic revenues.make decision, compare p-value \\(P\\) level test \\(\\alpha\\):\\(P < \\alpha\\), reject \\(\\mathscr{H}_0\\);\\(P \\geq \\alpha\\), fail reject \\(\\mathscr{H}_0\\).mix level test (probability fixed beforehand researcher) p-value. test level 5%, probability type error (condemning innocent mistake) definition \\(\\alpha\\) depend p-value. latter conditional probability observing extreme statistic given null distribution \\(\\mathscr{H}_0\\) true.American Statistical Association (ASA) published \nlist principles guiding (mis)interpretation p-values, reproduced :P-values measure probability studied hypothesis true.Scientific conclusions business policy decisions based whether p-value passes specific threshold.P-values related analyses reported selectively.p-value, statistical significance, measure size effect importance result.Example 2.4  (Gender inequality permutation tests) consider data Rosen Jerdee (1974), look sex role stereotypes impacts promotion opportunities women candidates. experiment took place 1972 experimental units, consisted 95 male bank supervisors, submitted various memorandums asked provide ratings decisions based information provided.interested Experiment 1 related promotion employees: managers requested decide whether promote employee become branch manager based recommendations ratings potential customer employee relations.authors intervention focused description nature (complexity) manager’s job (either simple complex) sex candidate (male female): files otherwise similar.consider simplicity sex factor aggregate job \\(n=93\\) replies. Table 2.1 shows counts possibility.\nTable 2.1: Promotion recommandation branch manager based sex applicant.\nnull hypothesis interest sex impact, probability promotion men women. Let \\(p_{\\text{m}}\\) \\(p_{\\text{w}}\\) denote respective probabilities; can thus write mathematically null hypothesis \\(\\mathscr{H}_0: p_{\\text{m}} = p_{\\text{w}}\\) alternative \\(\\mathscr{H}_a: p_{\\text{m}} \\neq p_{\\text{w}}\\).test statistic typically employed contingency tables chi-square test14, compares overall proportions promoted subgroup. sample proportion male 32/42 = ~76%, compared 19/49 ~49% female. seems difference 16% large, spurious: standard error sample proportions roughly 3.2% male 3.4% female.discrimination based sex, expect proportion people promoted overall; 51/93 =0.55 pooled sample. simply test mean difference, rely instead Pearson contingency \\(X^2_p\\) (aka chi-square) test, compares expected counts (based equal promotion rates) observed counts, suitably standardized. discrepancy large expected observed, casts doubt validity null hypothesis.\nTable 2.2: Chi-square test experiment 1 Rosen Jerdee (1974)\ncounts cell large, null distribution chi-square test well approximated \\(\\chi^2\\) distribution. output test includes value statistic, degrees freedom \\(\\chi^2\\) approximation p-value, gives probability random draw \\(\\chi^2_1\\) distribution larger observed test statistic assuming null hypothesis true. p-value small, 0.001, means result quite unlikely happen chance sex-discrimination.alternative test statistics used, among Fisher’s test. latter assumes row sum totals fixed (, number promoted/withheld files male/female fixed design stage) uses derive exact probability observing particular configuration proportion success . numerical value Fisher’s exact test statistic different chi-square test contingency tables, null distribution15. contrary, p-value close one reported \\(\\chi^2\\) test Table 2.2.\nTable 2.3: Fisher’s exact test experiment 1 Rosen Jerdee (1974)\nYet another alternative obtain benchmark assess outlyingness observed odds ratio use simulations. Consider database containing raw data 93 rows, one manager, indicator action sex hypothetical employee presented task.\nTable 2.4: First five rows database long format experiment 1 Rosen Jerdee.\nnull hypothesis, sex incidence action manager. means get idea “-” world shuffling sex labels repeatedly. Thus, obtain benchmark repeating following steps multiple times:permute labels sex,recreate contingency table aggregating counts,calculate test statistic simulated table.test statistic, use odds ratio: odds event ratio number success failure: example, number promoted held files. odds promotion male 32/12, whereas female 19/30. odds ratio male versus female thus \\(\\mathsf{}=\\) (32/12) / (19/30)= 4.21. null hypothesis, \\(\\mathscr{H}_0: \\mathsf{}=\\) 1 (probability promoted) (?)\nFigure 2.7: Histogram simulated null distribution odds ratio statistic obtained using permutation test; vertical red line indicates sample odds ratio.\nhistogram Figure 2.7 shows distribution odds ratio based 10 000 permutations. Reassuringly, get roughly approximate p-value, 0.002.16The article concluded (light experiments)Results confirmed hypothesis male administrators tend discriminate female employees personnel decisions involving promotion, development, supervision.RecapModel parameters: probability promotion men women, respectively \\(p_{\\text{m}}\\) \\(p_{\\text{w}}\\).Hypotheses: discrimination based gender, meaning equal probability promotion (null hypothesis\n\\(\\mathscr{H}_0: p_{\\text{m}}=p_{\\text{w}}\\), versus alternative hypothesis \\(\\mathscr{H}_a: p_{\\text{m}}\\neq p_{\\text{w}}\\)).Test statistic: (1) chi-square test contingency tables, (2) Fisher exact test, (3) odds ratio.\\(p\\)-value: (1) .0010,.0016 (3) .0024 (permutation test).Conclusion: reject null hypothesis, evidence gender-discrimination different probability promotion men women.Following APA guidelines, \\(\\chi^2\\) statistic reported \\(\\chi^2(1, n = 93) = 10.79\\), \\(p = .001\\) along counts sample proportions.first experiment, managers also asked rank applications potential employee customer relations using Likert scale six items ranging (1) extremely unfavorable (6) extremely favorable. However, averages reported Table 1 along (Rosen Jerdee 1974)Mean rating male candidate 4.73 compared mean rating 4.25 female candidate (\\(F=4.76\\), \\(\\text{df} = 1/80\\), \\(p < .05\\))degrees freedom (80) much compared number observations, implying non-response isn’t discussed.Partial selective reporting statistical procedures hinders reproducibility. general, presentation explicitly state name test statistic employed, sample size, mean variance estimates, null distribution used assess significance parameters, . Without , left speculate.","code":""},{"path":"hypothesis-testing.html","id":"confidence-intervals","chapter":"2 Hypothesis testing","heading":"2.4 Confidence intervals","text":"confidence interval alternative way present conclusions hypothesis test performed significance level \\(\\alpha\\) giving range values null isn’t rejected chosen level. often combined point estimator \\(\\hat{\\theta}\\) give indication variability estimation procedure. Wald-based \\((1-\\alpha)\\) confidence intervals parameter \\(\\theta\\) form\n\\[\\begin{align*}\n\\widehat{\\theta} + \\text{critical value} \\; \\mathrm{se}(\\widehat{\\theta})\n\\end{align*}\\]\nbased Wald statistic \\(W\\),\n\\[\\begin{align*}\nW =\\frac{\\widehat{\\theta}-\\theta}{\\mathrm{se}(\\widehat{\\theta})},\n\\end{align*}\\]\n\\(\\theta\\) represents postulated value fixed, unknown value parameter. critical values quantile null distribution chosen probability extreme \\(\\alpha\\).bounds confidence intervals random variables, since estimators parameter standard error, \\(\\widehat{\\theta}\\) \\(\\mathrm{se}(\\widehat{\\theta})\\), random: values vary one sample next.generic random samples, \\(1-\\alpha\\) probability \\(\\theta\\) contained random confidence interval computed. obtain sample calculate confidence interval, notion probability: true value parameter \\(\\theta\\) either inside confidence interval . can interpret confidence interval’s follows: repeat experiment multiple times, calculate \\(1-\\alpha\\) confidence interval time, roughly \\(1-\\alpha\\) calculated confidence intervals contain true value \\(\\theta\\) repeated samples (way, flip coin, roughly 50-50 chance getting heads tails, outcome either). confidence procedure use calculate confidence intervals actual values obtain sample.\nFigure 2.8: 95% confidence intervals mean standard normal population 100 random samples. average, 5% intervals fail include true mean value zero (red).\ninterested binary decision rule reject/fail reject \\(\\mathscr{H}_0\\), confidence interval equivalent p-value since leads conclusion. Whereas \\(1-\\alpha\\) confidence interval gives set values test statistic doesn’t provide enough evidence reject \\(\\mathscr{H}_0\\) level \\(\\alpha\\), p-value gives probability null obtaining result extreme postulated value precise particular value. p-value smaller \\(\\alpha\\), null value \\(\\theta\\) outside confidence interval vice-versa.Example 2.5  (Surprise Reaching ) Liu et al. (2022+) studies social interactions impact surprise people reaching contact unexpected. Experiment 1 focuses questionnaires experimental condition perceived appreciation reaching someone (vs reached ). study used questionnaire administered 200 American adults recruited Prolific Academic platform. response index consists average four questions measured Likert scale ranging 1 7, higher values indicating higher appreciation.can begin inspecting summary statistics sociodemographic variables (gender age) assess whether sample representative general population whole. proportion (including non-binary people) much higher general census, population skews quite young according Table 2.5.\nTable 2.5: Summary statistics age participants, counts per gender (left) mean ratings, standard deviation number participants per experimental condition (right).\nSince two groups, initiator responder, dealing pairwise comparison. logical test one use two sample t-test, variant thereof. Using Welch two sample \\(t\\)-test statistic, group average standard deviation estimated using data provided latter used build statistic. explains non-integer degrees freedom.software returns \\(t(197.52) = -2.05\\), \\(p = .041\\), leads rejection null hypothesis difference appreciation depending role individual (initiator responder). estimated mean difference \\(\\Delta M = -0.37\\), 95% CI \\([-0.73, -0.01]\\); since \\(0\\) included confidence interval, also reject null hypothesis level 5%. estimate suggests initiators underestimate appreciation reaching .17RecapModel parameters: average expected appreciation score \\(\\mu_{\\mathrm{}}\\) \\(\\mu_{\\mathrm{r}}\\) initiators responder, respectivelyHypothesis: expected appreciation score initiator responders, \\(\\mathscr{H}_0: \\mu_{\\mathrm{}}=\\mu_{\\mathrm{r}}\\) alternative \\(\\mathscr{H}_0: \\mu_{\\mathrm{}} \\neq \\mu_{\\mathrm{r}}\\) different.Test statistic: Welch two sample \\(t\\)-test\\(p\\)-value: 0.041Conclusion: reject null hypothesis, average appreciation score differs depending roleExample 2.6  (Virtual communication curbs creative idea generation) Nature study performed experiment see virtual communications teamwork comparing output terms ideas generated brainstorming session pairs quality ideas, measured external referees. sample consisted 301 pairs participants interacted via either videoconference face--face.authors compared number creative ideas, subset ideas generated creativity score average. mean number number creative ideas face--face \\(7.92\\) ideas (sd \\(3.40\\)) relative videoconferencing \\(6.73\\) ideas (sd \\(3.27\\)).Brucks Levav (2022) used negative binomial regression model: model, expected number creative ideas generated \n\\[\\begin{align*}\n\\mathsf{E}(\\texttt{ncreative}) = \\exp(\\beta_0 + \\beta_1 \\texttt{video})\n\\end{align*}\\]\n\\(\\texttt{video}=0\\) pair room \\(\\texttt{video}=1\\) interact instead via videoconferencing.mean number ideas videoconferencing thus \\(\\exp(\\beta_1)\\) times face--face: estimate multiplicative factor \\(\\exp(\\beta_1)\\) \\(0.85\\) 95% CI \\([0.77, 0.94]\\).difference experimental conditions translates null hypothesis \\(\\mathscr{H}_0: \\beta_1=0\\) vs \\(\\mathscr{H}_0: \\beta_1 \\neq 0\\) equivalently \\(\\mathscr{H}_0: \\exp(\\beta_1)=1\\). likelihood ratio test comparing regression model without \\(\\texttt{video}\\) statistic \\(R=9.89\\) (\\(p\\)-value based \\(\\chi^2_1\\) \\(.002\\)). conclude average number ideas different, summary statistics suggesting virtual pairs generate fewer ideas.resorted two sample \\(t\\)-test, found mean difference number creative idea \\(\\Delta M = 1.19\\), 95% CI \\([0.43, 1.95]\\), \\(t(299) = 3.09\\), \\(p = .002\\).tests come slightly different sets assumptions, yield similar conclusions: evidence smaller number creative ideas people interact via videoconferencing.","code":""},{"path":"hypothesis-testing.html","id":"power","chapter":"2 Hypothesis testing","heading":"2.5 Power","text":"previous examples highlighted different test statistics gave broadly similar conclusions despite based different benchmark. Generally, however, tradeoff number assumptions make data model (fewer, better) ability draw conclusions truly something going null hypothesis false.two typically uses hypothesis test: either want show unreasonable assume null hypothesis (example, assuming equal variance), else want show beyond reasonable doubt difference effect significative: example, one wish demonstrate new website design (alternative hypothesis) leads significant increase sales relative status quo.ability make discoveries depends power test: larger power, greater ability reject null hypothesis \\(\\mathscr{H}_0\\) latter false.power test probability correctly rejecting null hypothesis \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) false, .e.,\n\\[\\begin{align*}\n\\mathsf{Pr}_a(\\text{reject} \\mathscr{H}_0)\n\\end{align*}\\]\nDepending alternative models, less easy detect null hypothesis false reject favour alternative. Power thus measure ability detect real effects.\nFigure 2.9: Comparison null distribution (full curve) specific alternative t-test (dashed line). power corresponds area curve density alternative distribution rejection area (white).\n\nFigure 2.10: Increase power due increase mean difference null alternative hypothesis. Power area rejection region (white) alternative distribution (dashed): latter shifted right relative null distribution (full line).\n\nFigure 2.11: Increase power due increase sample size decrease standard deviation population: null distribution (full line) concentrated. Power given area (white) curve alternative distribution (dashed). general, null distribution changes sample size.\nwant choose experimental design test statistic leads high power, power close possible one. various assumptions distribution original data, can derive optimal tests powerful, power comes imposing structure assumptions need satisfied practice.Minimally, power test \\(\\alpha\\) reject null hypothesis \\(\\alpha\\) fraction time even \\(\\mathscr{H}_0\\) true. Power depends many criteria, notablythe effect size: bigger difference postulated value \\(\\theta_0\\) \\(\\mathscr{H}_0\\) observed behaviour, easier detect departures \\(\\theta_0\\).\n(Figure 2.11); ’s easier spot elephant room mouse.variability: less noisy data, easier assess observed differences genuine, Figure 2.10 shows;sample size: observation, higher ability detect significative differences amount evidence increases gather observations.18 experimental designs, power also depends many observations allocated group.19the choice test statistic: plethora possible statistics choose summary evidence null hypothesis. Choosing designing statistics usually best left statisticians, may tradeoffs. example, rank-based statistics discard information observed values response, focusing instead relative ranking. resulting tests typically less powerful, also less sensible model assumptions, model misspecification outliers.Changing value \\(\\alpha\\) also impact power, since larger values \\(\\alpha\\) move cutoff towards bulk distribution. However, entails higher percentage rejection also alternative false. Since value \\(\\alpha\\) fixed beforehand control type error (avoid judicial mistakes), ’s parameter consider.","code":""},{"path":"hypothesis-testing.html","id":"conclusion-1","chapter":"2 Hypothesis testing","heading":"2.6 Conclusion","text":"chapter focused presenting tools trade examples outlining key ingredients common statistical procedure reporting latter. reader expected know test statistic adopt, rather understand stage ability (scientific) discoveries depends number factors.Richard McElreath first chapter book (McElreath 2020) draws parallel statistical tests golems (.e., robots): neitherdiscern context inapropriate answers. just knows procedure […] just ’s told.responsibility therefore lies user correctly use statistical procedures aware limitations. p-value indicate whether hypothesis reasonable, whether design proper, whether choice measurement adequate, etc.Pick journal paper (e.g., one dataset documented course webpage) particular study.Look ingredients testing procedure (parameters, hypotheses, test statistic name value, summary statistics, p-value, conclusion).may encounter measures, effect size, discussed later.","code":""},{"path":"CRT.html","id":"CRT","chapter":"3 Completely randomized designs","heading":"3 Completely randomized designs","text":"chapter focuses experiments potentially multiple factors interest manipulated experimenter study impact. allocation observational units treatment combination completely random, resulting experiment completely randomized design.one-way analysis variance describes simple experimental setup one can consider: completely randomized experiments one factor, solely interested effect single treatment variable multiple levels.","code":""},{"path":"CRT.html","id":"one-way-analysis-of-variance","chapter":"3 Completely randomized designs","heading":"3.1 One-way analysis of variance","text":"focus comparisons average single outcome variable \\(K\\) different treatments levels, defining sub-population differing experimental condition received. one-way analysis variance compares sample averages treatment group \\(T_1, \\ldots, T_K\\) try determine population averages . Since \\(K\\) groups, \\(K\\) averages (one per group) estimate.Let \\(\\mu_1, \\ldots, \\mu_K\\) denote theoretical (unknown) mean (aka expectation) \\(K\\) sub-populations defined different treatments. Lack difference treatments equivalent equality means, translates hypotheses\n\\[\\begin{align*}\n\\mathscr{H}_0: & \\mu_1 = \\cdots = \\mu_K \\\\\n\\mathscr{H}_a: & \\text{least two treatments different averages, }\n\\end{align*}\\]\nnull hypothesis , usual, single numerical value, \\(\\mu\\). alternative consists potential scenarios expectations equal. Going \\(K\\) averages one requires imposing \\(K-1\\) restrictions (number equality signs), value global mean \\(\\mu\\) left unspecified.","code":""},{"path":"CRT.html","id":"parametrizations-and-contrasts","chapter":"3 Completely randomized designs","heading":"3.1.1 Parametrizations and contrasts","text":"section can skipped first reading. focuses interpretation coefficients obtained linear model analysis variance model.natural parametrization terms group averages: (theoretical unknown) average treatment \\(T_j\\) \\(\\mu_j\\), obtain \\(K\\) parameters \\(\\mu_1, \\ldots, \\mu_K\\) whose estimates sample averages \\(\\widehat{\\mu}_1, \\ldots, \\widehat{\\mu}_K\\). One slight complication arising values population average unknown, formulation ill-suited hypothesis testing none \\(\\mu_i\\) values known practice need make comparisons terms known numerical value.common parametrization linear model terms differences baseline, say \\(T_1\\). theoretical average group written \\(\\mu_1 + a_i\\) treatment \\(T_i\\), \\(a_1=0\\) \\(T_1\\) \\(a_i = \\mu_i-\\mu_1\\) otherwise. parameters \\(\\mu_1, a_2, \\ldots, a_K\\).equivalent formulation writes treatment group average subpopulation \\(j\\) \\(\\mu_j = \\mu + \\delta_j\\), \\(\\delta_j\\) difference treatment average \\(\\mu_j\\) global average groups. Imposing constraint \\(\\delta_1 + \\cdots + \\delta_K=0\\) ensures average effects equals \\(\\mu\\). Thus, know \\(K-1\\) \\(\\{\\delta_1, \\ldots, \\delta_K\\}\\), automatically can deduce last one.Example 3.1  (Impact encouragement teaching) R, lm function fits linear model based formula form response ~ explanatory. explanatory categorical (.e., factor), parameters model intercept, sample average baseline group parameters simply contrasts, .e., \\(a_i\\)’s.sum--zero parametrization obtained contrasts = list(... = contr.sum), ellipsis replaced name categorical variable; easier alternative aov, enforces parametrization default. sum--zero parametrization, intercept average treatment average, \\((\\widehat{\\mu}_1 + \\cdots + \\widehat{\\mu}_5)/5\\); need coincide (overall) mean response \\(\\widehat{\\mu} = \\overline{y}\\) unless sample number observations group .20 coefficients sum--zero parametrization differences intercept group means.show function call fit one-way ANOVA different parametrizations along sample average arithmetic group (two controls taught separately groups praised, reproved ignored third class). Note omitted category changes depending parametrization.\nTable 3.1: Coefficients analysis variance model arithmetic scores using different parametrizations.\ncan still assess hypothesis comparing sample means group, noisy estimates population mean: inherent variability limit ability detect differences averages signal--noise ratio small.","code":"\nmod_contrast <- lm(score ~ group, \n                   data = arithmetic)\nmod_sum2zero <- lm(score ~ group, \n                   data = arithmetic,\n                   contrasts = list(group = contr.sum))"},{"path":"CRT.html","id":"sum-of-squares-decomposition","chapter":"3 Completely randomized designs","heading":"3.1.2 Sum of squares decomposition","text":"following section can safely skipped first reading: attempts shed light \\(F\\)-test statistic works summary evidence, isn’t straightforward way appears.usual notation sum squares decomposition follows: suppose \\(y_{ik}\\) represents \\(\\)th person \\(k\\)th treatment group (\\(k=1, \\ldots, K\\)) sample size \\(n\\) can split groups \\(n_1, \\ldots, n_K\\); case balanced sample, \\(n_1=\\cdots=n_K = n/K\\) number observations group . denote \\(\\widehat{\\mu}_k\\) sample average group \\(k\\) \\(\\widehat{\\mu}\\) overall average \\((y_{11} + \\cdots + y_{n_KK})/n = \\sum_k \\sum_i y_{ik}/n\\), \\(\\sum_i\\) denotes sum individuals group.null model, groups mean, natural estimator latter sample average pooled sample \\(\\widehat{\\mu}\\) likewise group averages \\(\\widehat{\\mu}_1, \\ldots, \\widehat{\\mu}_K\\) best estimators group averages group (potentially) different mean. complex model, parameters, always fit better possibility accommodate differences observed group, even spurious.\nsum squares measures (squared) distance observation fitted values, terminology total, within sum squares linked decomposition\n\\[\\begin{align*}\n\\underset{\\text{total sum squares} }{\\sum_{}\\sum_{k} (y_{ik} - \\widehat{\\mu})^2} &= \\underset{\\text{within sum squares} }{\\sum_i \\sum_k (y_{ik} - \\widehat{\\mu}_k)^2} +  \\underset{\\text{sum squares} }{\\sum_k n_i (\\widehat{\\mu}_k - \\widehat{\\mu})^2}.\n\\end{align*}\\]\nterm left measure variability null model \\((\\mu_1 = \\cdots = \\mu_K)\\) observations predicted overall average \\(\\widehat{\\mu}\\). within sum squares measures distance observations group mean, describes alternative model group (potentially) different average, variability.can measure much worst alternative model (different average per group) relative null calculating sum square. quantity varies sample size (observations, larger ) must standardize usual quantity suitable benchmark.\\(F\\)-statistic \n\\[\\begin{align}\nF &= \\frac{\\text{-group variability}}{\\text{within-group variability}} \\\\&= \\frac{\\text{sum squares}/(K-1)}{\\text{within sum squares}/(n-K)}\n\\tag{3.1}\n\\end{align}\\]mean difference (null hypothesis), numerator estimator population variance, denominator eq. (3.1) ratio two approximately 1 average. However, sum square variable induces skewness: large enough sample, null distribution F-statistic approximately F-distribution, whose shape governed two parameters named degrees freedom appear eq. (3.1) scaling factors ensure proper standardization. first degree freedom number restrictions imposed null hypothesis (\\(K-1\\), number groups minus one one-way analysis variance), second degree freedom number observations minus number parameters estimates mean (\\(n-K\\), \\(n\\) overall sample size \\(K\\) number groups).21Figure 3.1 shows difference distances can encompass information null wrong. sum squares obtained computing squared length vectors adding . left panel shows strong signal--noise ratio, , average, black segments much longer colored ones. indicates model obtained letting group mean much better . picture right panel clear: average, colored arrows shorter, difference length much smaller relative colored arrows.\nFigure 3.1: Observations drawn three groups model strong (left) weak (right) signal--noise ratio, along sample mean (colored horizontal segments) overall average (horizontal line). Arrows indicate magnitude difference observation (group/average) mean.\n\\(F\\)-distribution call large sample approximation behaviour statistic truly difference group averages (model assumptions satisfied): tells us expect nothing going . quality approximation depends sample size group: accurate observations group, average estimation becomes reliable22.alluded last chapter, large sample approximations option assessing null, cheap easy obtain. distributions null alternative except location shift, instead resort permutation-based approach generate alternative samples simply shuffling labels. see Figure 3.2 histogram \\(F\\)-statistic values obtained 10 000 permutations closely matches large-sample \\(F\\)-distribution average 20 observations per group (right), computational burden associated running simulation outweights benefits. However, smaller samples (left), large sample approximation appears underdispersed relative permutation based distribution; latter viewed accurate setting.\nFigure 3.2: One-way analysis variance sample size 20 (left) 100 (right), split five groups. histogram shows computed test values based 10 000 permutations, compared density large-sample F-distribution.\ninterestingly perhaps happens values taken statistic averages . can see Figure 3.3 , difference group means, values taken statistic random sample right null distribution: larger differences, curve shift right often obtain value rejection region (red).two groups, one can show \\(F\\)-statistic mathematically equivalent squaring \\(t\\)-statistic: null distributions \\(\\mathsf{St}(n-K)\\) \\(\\mathsf{F}(1, n-K)\\) lead \\(p\\)-values thus statistical inference conclusions.\nFigure 3.3: Distribution \\(F\\)-test statistic one-way analysis variance true group means equal (top) specific alternative (bottom). value falling within red region leads rejection null hypothesis level \\(\\alpha=0.05\\).\n","code":""},{"path":"CRT.html","id":"graphical-representation","chapter":"3 Completely randomized designs","heading":"3.2 Graphical representation","text":"represent data one-way analysis publication? purpose visualization provide intuition extends beyond reported descriptive statistics check model assumptions. time, interested averages dispersion, plotting raw data can insightful. also important keep mind summary statistics estimators population quantities perhaps unreliable (much variable) small samples meaningful quantities. Since mean estimates likely reported text, graphics used convey additional information data. samples extremely large, graphics typically used present salient features distributions.\nFigure 3.4: Two graphical representations arithmetic data: dynamite plot (left) showing sample average one standard error , dot plot sample mean (right).\none-way analysis variance, outcome continuous numerical variable, whereas treatment explanatory categorical variable. Basic graphics include dot plots, histograms density plots, rugs raw data.Typically, scatterplots good option observations get overlaid. multiple workarounds, involving transparency, bubble plots discrete data ties, adding noise (jitter) every observation drawing values using thin line (rugs) data continuous take distinct values.Journals plagued poor vizualisations, prime example infamous dynamite plot: consists bar plot one standard error interval. problem (summary statistics) hide precious information spread values taken data, many different data give rise average quite different nature. height bar sample average bars extend beyond one standard error: makes little sense end comparing areas, whereas mean single number. right panel Figure 3.4 shows instead dot plot data, .e., sample values ties stacked clarity, along sample average 95% confidence interval latter line underneath. example, enough observations per group produce histograms, five number summary nine observations isn’t really necessary boxplot useless. Weissgerber et al. (2015) discusses alternative solutions can referenced fighting reviewers insist bad visualizations.lot data, sometimes help represent selected summary statistics group data. box--whiskers plot (boxplot) commonly used graphic representing whole data distribution using five numbersThe box gives quartiles, say \\(q_1\\), \\(q_2\\) (median) \\(q_3\\) distribution: 50% observations smaller larger \\(q_2\\), 25% smaller \\(q_1\\) 75% smaller \\(q_3\\) sample.whiskers extend \\(1.5\\) times box width (\\(q_3-q_1\\)) (largest observation smaller \\(q_3+1.5(q_3-q_1)\\), etc.)Observations beyond whiskers represented dots circles, sometimes termed outliers. However, beware terminology: larger sample size, values fall outside whiskers (0.7% normal data). drawback boxplots, conceived time big data didn’t exist. want combine boxplots raw data, remove display outliers avoid artefacts.\nFigure 3.5: Box--whiskers plot\nWeissgerber et al. (2019) contains many examples build effective visualizations, including highlighting particular aspects using color, jittering, transparency adequately select display zone.","code":""},{"path":"CRT.html","id":"pairwise-tests","chapter":"3 Completely randomized designs","heading":"3.3 Pairwise tests","text":"global test equality mean one-way ANOVA leads rejection null, conclusion one group different mean. However, test indicate groups differ rest say many different. different options: one custom contrasts, special instance pairwise comparisons.interested looking difference (population) average group \\(\\) \\(j\\), say. null hypothesis difference translate \\(\\mu_i-\\mu_j=0\\), numerator statistic estimator \\(\\widehat{\\mu}_i - \\widehat{\\mu}_j\\) difference sample mean, minus zero.Assuming equal variances, two-sample \\(t\\)-test statistic \n\\[\\begin{align*}\nt_{ij} = \\frac{(\\widehat{\\mu}_i - \\widehat{\\mu}_j) - 0}{\\mathsf{se}(\\widehat{\\mu}_i - \\widehat{\\mu}_j)} =\\frac{\\widehat{\\mu}_i - \\widehat{\\mu}_j}{\\widehat{\\sigma} \\left(\\frac{1}{n_i} + \\frac{1}{n_j}\\right)^{1/2}},\n\\end{align*}\\]\n\\(\\widehat{\\mu}_i\\) \\(n_i\\) respectively sample average number observations group \\(\\), \\(\\widehat{\\sigma}\\) estimator standard deviation derived using whole sample (assuming equal variance). usual, denominator \\(t_{ij}\\) standard error \\(\\widehat{\\mu}_i - \\widehat{\\mu}_j\\), whose postulated difference zero. can compare value observed statistic Student-\\(t\\) distribution \\(n-K\\) degrees freedom, denoted \\(\\mathsf{St}(n-K)\\). two-sided alternative, reject \\(|t_{ij}| > \\mathfrak{t}_{1-\\alpha/2}\\), \\(\\mathfrak{t}_{1-\\alpha/2}\\) \\(1-\\alpha/2\\) quantile \\(\\mathsf{St}(n-K)\\).Figure 3.6 shows density benchmark distribution pairwise comparisons mean arithmetic data. blue area curve defines set values fail reject null hypothesis, whereas values test statistic falling red area lead rejection level \\(5\\)%.\nFigure 3.6: Student-t null distribution rejection region t-test.\nfail reject \\(\\mathscr{H}_0\\) \\(\\mathfrak{t}_{\\alpha/2} \\leq t_{ij} \\leq \\mathfrak{t}_{1-\\alpha/2}\\)23: gives us another way presenting conclusion terms set mean differences \\(\\delta_{ij} = \\mu_i - \\mu_j\\) \n\\[\\begin{align*}\n\\mathfrak{t}_{\\alpha/2} \\leq \\frac{\\widehat{\\delta}_{ij} - \\delta_{ij}}{\\mathsf{se}\\left(\\widehat{\\delta}_{ij}\\right)} \\leq \\mathfrak{t}_{1-\\alpha/2}\n\\end{align*}\\]\nequivalent upon rearranging \\((1-\\alpha)\\) confidence interval \\(\\delta_{ij}\\),24\n\\[\\begin{align*}\n\\mathsf{CI} = \\left[\\widehat{\\delta}_{ij} - \\mathfrak{t}_{1-\\alpha/2}\\mathsf{se}\\left(\\widehat{\\delta}_{ij}\\right), \\widehat{\\delta}_{ij} - \\mathfrak{t}_{\\alpha/2}\\mathsf{se}\\left(\\widehat{\\delta}_{ij}\\right)\\right].\n\\end{align*}\\]Example 3.2  (Pairwise comparison) consider pairwise average difference scores praised (group C) reproved (group D) arithmetic study. sample averages respectively \\(\\widehat{\\mu}_C = 27.4\\) \\(\\widehat{\\mu}_D = 23.4\\) estimated pooled standard deviation five groups \\(1.15\\). Thus, estimated average difference groups \\(C\\) \\(D\\) \\(\\widehat{\\delta}_{CD} = 4\\) standard error difference \\(\\mathsf{se}(\\widehat{\\delta}_{CD}) = 1.6216\\); calculated software.take null hypothesis \\(\\mathscr{H}_0: \\delta_{CD}=0\\), \\(t\\) statistic \n\\[\\begin{align*}t=\\frac{\\widehat{\\delta}_{CD} - 0}{\\mathsf{se}(\\widehat{\\delta}_{CD})} = \\frac{4}{1.6216}=2.467\n\\end{align*}\\]\n\\(p\\)-value \\(p=0.018\\). therefore reject null hypothesis level \\(\\alpha=0.05\\) conclude significant difference (level \\(\\alpha=0.05\\)) average scores students praised reproved.","code":""},{"path":"CRT.html","id":"model-assumptions","chapter":"3 Completely randomized designs","heading":"3.4 Model assumptions","text":"far, brushed model assumptions carpet. necessary requirements inference valid: statement related p-values, etc. approximately hold set assumptions met first place. section devoted discussion assumptions, showcasing examples things can go wrong.customary write \\(\\)th observation \\(k\\)th group one-way analysis variance model \n\\[\\begin{align}\n\\underset{\\text{observation}}{Y_{ik}} = \\underset{\\text{mean group $k$}}{\\mu_k} + \\underset{\\text{error term}}{\\varepsilon_{ik}},\n\\tag{3.2}\n\\end{align}\\]\nerror terms \\(\\varepsilon_{ik}\\), account unexplained variability individual differences, independent one mean zero variance \\(\\sigma^2\\).","code":""},{"path":"CRT.html","id":"additivity","chapter":"3 Completely randomized designs","heading":"3.4.1 Additivity","text":"basic assumption designs can decompose outcome two components (Cox 1958)\n\\[\\begin{align}\n\\begin{pmatrix} \\text{quantity depending} \\\\\n\\text{treatment used}\\end{pmatrix} +\n\\begin{pmatrix} \\text{quantity depending } \\\\\n\\text{particular unit}\n\\end{pmatrix}\n\\tag{3.3}\n\\end{align}\\]\nadditive decomposition assumes unit unaffected treatment units average effect treatment constant. Thus, justified use difference sample mean estimate treatment effect since average, individual effect zero.decomposition observations terms group average mean-zero noise (3.2) suggests plot error term \\(\\varepsilon_{ik}\\) observations, factors explanatories, see unusual structure unexplained model indicating problems randomization additivity. However, access \\(\\varepsilon_{ik}\\) since true group mean \\(\\mu_k\\) error \\(\\varepsilon_{ik}\\) unknown. However, good proxy ordinary residual \\(e_{ik} = y_{ik} - \\widehat{\\mu}_k\\) \\(\\widehat{\\mu}_k\\) sample mean observations experimental group \\(k\\). construction, sample mean residuals zero, local deviations may indicate violations analysis (example, plotting residuals time show learning effect).Many graphical diagnostics use residuals, .e., variant observations minus group mean \\(y_{ik} - \\widehat{\\mu}_k\\), look violation assumptions.\nFigure 3.7: Data satisfying assumptions one-way analysis variance model, additive effects, independent observations common variance.\ngenerally, test statistic may make assumptions. \\(F\\)-test global null \\(\\mu_1 = \\cdots \\mu_K\\) assumes \\(\\)th observation group \\(k\\), say \\(y_{ik}\\), average \\(\\mathsf{E}(Y_{ik}) = \\mu_k\\) variance \\(\\mathsf{Va}(Y_{ik}) = \\sigma^2\\). latter estimated using residuals, \\(\\widehat{\\sigma}^2 = \\sum_k\\sum_i (y_{ik} - \\widehat{\\mu}_k)^2/(n-K)\\). assumptions, \\(F\\)-test statistic global null \\(\\mu_1 = \\cdots = \\mu_K\\) powerful uses data get precise estimation variability. Generally, may considerations power may guide choice test statistic, including robustness (sensitivity extremes outliers). unequal variance, statistics \\(F\\)-test statistic may powerful.Example 3.3  (Additivity transformations) Chapter 2 Cox (1958) discusses assumption additivity provides useful examples showing taken granted. One , Example 2.3, scenario experimental units participants asked provide ranking different kindergarten students capacity interact others games, ranked scale 0 100. random group students receives additional orthopedagogical support, balance business--usual setting (control group). Since intrinsic differences student level, one consider paired experiment take outcome difference sociability scores beginning end school year.One can expect treatment impact people low sociability skills struggling make contacts: student scored 50 initially might see improvement 20 points support relative 10 business--usual scenario, whereas another well integrated scored high initially may see improvement 5 (s)assigned support group. implies treatment effects constant scale, violation additivity assumption. One way deal via transformations: Cox (1958) discusses transformation \\(\\log\\{(x+0.5)/(100.5-x)\\}\\) reduce warping due scale.Another example experiments effect treatment multiplicative, output form\n\\[\\begin{align*}\n\\begin{pmatrix} \\text{quantity depending } \\\\\n\\text{particular unit}\n\\end{pmatrix} \\times\n\\begin{pmatrix} \\text{quantity depending} \\\\\n\\text{treatment used}\\end{pmatrix}\n\\end{align*}\\]\nUsually, arises positive responses treatments, case taking natural logarithms sides, \\(\\log(xy) = \\log x + \\log y\\) yielding additive decomposition.Example 3.4  (Inadequacy additivity based context) example adapted Cox (1958), Example 2.2. Children suffering attention deficit hyperactivity disorder (ADHD) may receive medication increase attention span, measured scale 0 100, 0 indicating normal attention span. experiment can designed assess impact standardized dose laboratory comparing performances students series task , placebo. make case, suppose students ADHD fall two categories: low symptoms strong symptoms. low symptom group, average attention 8 per cent drug 12 per cent placebo, whereas people strong symptoms, average 40 per cent among treated 60 per cent placebo. two categories equally represented experiment population, estimate average reduction 12 percent score (thus higher attention span among treated). Yet, quantity artificial, better measure symptoms treatment 2/3 control (ratio proportions).Equation (3.3) also implies effect treatment constant individuals. often isn’t case: experimental study impact teaching delivery type (online, hybrid, person), may response choice delivery mode depends different preferences learning types (auditory, visual, kinestetic, etc.) Thus, recording additional measurements susceptible interact may useful; likewise, treatment allotment must factor variability wish make detectable. solution setup complex model (two-way analysis variance, general linear model) stratify explanatory variable (example, compute difference within level).\nFigure 3.8: Difference average response; treatment seems lead decrease response variable, stratification age group reveals occurs less 25 group, seemingly reversed effect adults. Thus, marginal model implied one-way analysis variance misleading.\n","code":""},{"path":"CRT.html","id":"heterogeneity","chapter":"3 Completely randomized designs","heading":"3.4.2 Heterogeneity","text":"one-way ANOVA builds fact variance group equal, upon recentering, can estimate variance residuals \\(y_{ik} - \\widehat{\\mu}_k\\). Specifically, unbiased variance estimator denominator \\(F\\)-statistic formula, .e., within sum squares divided \\(n-K\\) \\(n\\) total number observations \\(K\\) number groups comparison.time , consider hypothesis tests homogeneity (equal) variance assumption. commonly used tests Bartlett’s test25 Levene’s test (robust alternative, less sensitive outliers). tests, null distribution \\(\\mathscr{H}_0: \\sigma^2_1 = \\cdots = \\sigma^2_K\\) alternative least two differ. Bartlett test statistic \\(\\chi^2\\) null distribution \\(K-1\\) degrees freedom, whereas Levene’s test \\(F\\)-distribution (\\(K-1\\), \\(n-K\\)) degrees freedom: equivalent computing one-way ANOVA \\(F\\)-statistic absolute value centered residuals, \\(|y_{ik} - \\widehat{\\mu}_k|\\), observations.can see cases \\(p\\)-values large enough dismiss concern inequality variance. However, latter problem, can proceed test statistic require variances equal. common choice modification due Satterthwaite called Welch’s ANOVA. commonly encountered case two groups (\\(K=2\\)) default option R t.test oneway.test.happens example arithmetic data use instead usual \\(F\\) statistic? , evidence overwhelming changes conclusion. Generally, drawback using Welch’s ANOVA usual \\(F\\) statistic need enough observations group reliably estimate separate variance26. Welch’s ANOVA, estimate \\(2K\\) parameters (one mean one variance per group), rather \\(K+1\\) parameters one-way ANOVA (one mean per group, one overall variance).Notice degrees freedom denominator decreased. use pairwise.t.test argument pool.sd=FALSE, amounts running Welch \\(t\\)-tests separately pair variable.impacts unequal variance use \\(F\\)-test instead? one, pooled variance based weighted average variance group, weight function sample size. can lead size distortion (meaning proportion type error nominal level \\(\\alpha\\) claimed) potential loss power. following toy example illustrates .\nFigure 3.9: Histogram null distribution \\(p\\)-values obtained simulation using classical analysis variance \\(F\\)-test (left) Welch’s unequal variance alternative (right), based 10 000 simulations. simulated sample consist 50 observations \\(\\mathsf{}(0, 1)\\) distribution 10 observations \\(\\mathsf{}(0, 9)\\). uniform distribution 5% 20 bins used display.\nconsider simplicity problem \\(K=2\\) groups, two-sample \\(t\\)-test. simulated 50 observations \\(\\mathsf{}(0, 1)\\) distribution 10 observations \\(\\mathsf{}(0, 9)\\), comparing distribution \\(p\\)-values Welch \\(F\\)-test statistics. Figure 3.9 shows results. percentage \\(p\\)-values less \\(\\alpha=0.05\\) based 10 000 replicates estimated 4.76% Welch statistic, far level. contrast, reject 28.95% time one-way ANOVA global \\(F\\)-test: large share innocents sentenced jail based false premises! size distortion always striking, heterogeneity accounted design requiring sufficient sample sizes (whenever costs permits) group able estimate variance reliably using adequate statistic.alternative graphical ways checking assumption equal variance, many including standardized residuals \\(r_{ik} = (y_{ik} - \\widehat{\\mu}_k)/\\widehat{\\sigma}\\) fitted values \\(\\widehat{\\mu}_k\\). cover later sections.Oftentimes, unequal variance occurs model additive. use variance-stabilizing transformations (e.g., log multiplicative effects) ensure approximately equal variance group. Another option use model suitable type response (including count binary data). Lastly, may necessary explicitly model variance complex design (including repeated measures) learning effect time variability decreases result. Consult expert needed.","code":"\nbartlett.test(score ~ group,\n              data = arithmetic)\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  score by group\n#> Bartlett's K-squared = 2, df = 4, p-value = 0.7\n\ncar::leveneTest(score ~ group,\n                data = arithmetic,\n                center = mean)\n#> Levene's Test for Homogeneity of Variance (center = mean)\n#>       Df F value Pr(>F)\n#> group  4    1.57    0.2\n#>       40\n# compare with one-way ANOVA\nmod <- lm(score ~ group, data = arithmetic)\narithmetic$absresid <- abs(resid(mod)) #|y_{ik}-mean_k|\nanova(aov(absresid ~ group, data = arithmetic))\n#> Analysis of Variance Table\n#> \n#> Response: absresid\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> group      4   17.4    4.34    1.57    0.2\n#> Residuals 40  110.6    2.77\n# Welch ANOVA\noneway.test(score ~ group, data = arithmetic, \n            var.equal = FALSE)\n#> \n#>  One-way analysis of means (not assuming equal variances)\n#> \n#> data:  score and group\n#> F = 19, num df = 4, denom df = 20, p-value = 2e-06\n# Usual F-test statistic\noneway.test(score ~ group, data = arithmetic, \n            var.equal = TRUE)\n#> \n#>  One-way analysis of means\n#> \n#> data:  score and group\n#> F = 15, num df = 4, denom df = 40, p-value = 1e-07"},{"path":"CRT.html","id":"normality","chapter":"3 Completely randomized designs","heading":"3.4.3 Normality","text":"persistent yet incorrect claim literature data (either response, explanatory ) must normal order use (-called parametric) models like one-way analysis variance. normal data equal variances, eponymous distributions \\(F\\) \\(t\\) tests exact: knowing exact distribution harm convenient mathematical derivations. However, stressed condition unnecessary: results hold approximately large samples virtue central limit theorem. probability results dictates , general conditions nearly universally met, sample mean behaves like normal distribution large samples. applet lets explore impact underlying population data drawn interplay sample size central limit theorem kicks . can view Figure 3.2, simulated theoretical large-sample distributions undistinguishable approximately 20 observations per group.many authors may advocate rules thumbs (sample size \\(n>20\\) \\(n>30\\) per group, say), rules arbitrary: approximation much worst \\(n=19\\) \\(n=20\\). large must sample size approximation hold? largely depends distribution population: extremes, skewness, etc. , larger number observation must order approximation valid. Figure 3.10 shows skewed right bimodal distribution distribution sample mean repeated sampling. Even \\(n=5\\) observations (bottom left), approximation bad may still far \\(n=50\\) heavy-tailed data.\nFigure 3.10: Graphical representation central limit theorem. Top left: density underlying population samples drawn. Top right: sample 20 observations sample mean (vertical red). Bottom panels: histogram sample averages samples size 5 (left) 20 (right) normal approximation superimposed. sample size increases, normal approximation mean accurate standard error decreases.\nimportant keep mind statistical statements typically approximate reliability depends sample size: small sample may hampers strength conclusions. default graphic checking whether sample matches postulated distribution quantile-quantile plot.","code":""},{"path":"CRT.html","id":"independence","chapter":"3 Completely randomized designs","heading":"3.4.4 Independence","text":"allowed talk independence Quebecer27, simply means knowing value one observation tells us nothing value sample. Independence may fail hold case group structure (family dyads, cluster sampling) common characteristics simply case repeated measurements. Random assignment treatment thus key ensure measure holds, ensuring measurement phase spillover.Example 3.6  (Independence measurements) many hidden ways measurements can impact response. Physical devices need calibrated use (scales, microscope) require tuning: measurements done different experimenters different days, may impact add systematic shift means whole batch.Special care must taken whenever group testing used, blocking potential impacts can salvage analysis.impact dependence measurements? Heuristically, correlated measurements carry less information independent ones. extreme case, additional information measurements identical. reason makes difference following: denominator \\(F\\)-test sample variance, based within sum squares divided \\(n-K\\). observation counted 10 times, say, real number measurements \\(n\\) \\(F\\) statistic gets multiplied factor 10.28\nFigure 3.11: Percentage rejection null hypothesis \\(F\\)-test equality means one way ANOVA data generated equal mean variance equicorrelation model (within group observations correlated, group observations independent). nominal level test 5%.\nlack independence can also drastic consequences inference lead false conclusions: Figure 3.11 shows example correlated samples within group (equivalently repeated measurements individuals) 25 observations per group. \\(y\\)-axis shows proportion times null rejected shouldn’t . , since data generated null model (equal mean) equal variance, inflation number spurious discoveries, false alarm type error alarming inflation substantial even limited correlation measurements.","code":""},{"path":"contrasts-multiple-testing.html","id":"contrasts-multiple-testing","chapter":"4 Contrasts and multiple testing","heading":"4 Contrasts and multiple testing","text":"analysis variance model tests (global) null hypothesis average groups equal. experimental context, implies one manipulation different effect others mean response. Oftentimes, isn’t interesting : interested comparing different options relative status quo, determine whether specific combinations work better separately. scientific question interest warranted experiment may lead specific set hypotheses, can formulated researchers comparisons means different subgroups.","code":""},{"path":"contrasts-multiple-testing.html","id":"contrasts","chapter":"4 Contrasts and multiple testing","heading":"4.1 Contrasts","text":"can normally express contrasts. Dr. Lukas Meier puts , global \\(F\\)-test equality means equivalent dimly lit room, contrasts akin spotlight let one focus particular aspects differences treatments.formally speaking, contrast linear combination averages: plain English, means assign weight group average add . can build \\(t\\) statistic usual standardizing resulting weighted sum group means.\\(c_i\\) denotes weight group average \\(\\mu_i\\) \\((=1, \\ldots, K)\\), can write contrast \\(C = c_1 \\mu_1 + \\cdots + c_K \\mu_K\\) null hypothesis \\(\\mathscr{H}_0: C=\\) two-sided alternative, \\(\\) numeric value like 0. sample estimate linear contrast obtained replacing unknown population average \\(\\mu_i\\) sample average group, \\(\\widehat{\\mu}_i = \\overline{y}_{}\\). can easily obtain standard error linear combination \\(C\\).29Contrasts encode research question interest, taking form \\(\\mathscr{H}_0: c_1 \\mu_1 + \\cdots + c_K\\mu_K = \\), numerical value \\(\\) typically zero.","code":""},{"path":"contrasts-multiple-testing.html","id":"orthogonal-contrasts","chapter":"4 Contrasts and multiple testing","heading":"4.1.1 Orthogonal contrasts","text":"Sometimes, linear contrasts encode disjoint bits information sample: example, one contrast compares groups first two groups versus one compares third fourth effect using data two disjoint samples, contrasts based sample averages. Whenever contrasts vectors orthogonal, tests uncorrelated contain independent bits information population.30 Mathematically, let \\(c_{}\\) \\(c^{*}_{}\\) denote weights attached mean group \\(\\) comprising \\(n_i\\) observations, contrasts orthogonal \\(c_{1}c^{*}_{1}/n_1 + \\cdots + c_{K}c^{*}_K/n_K = 0\\); sample balanced number observations group, \\(n/K = n_1 =\\cdots = n_K\\)31. \\(K\\) groups, \\(K-1\\) contrasts pairwise differences, last one captured sample mean overall effect.care difference groups (opposed overall effect treatments), impose sum--zero constraint weights \\(c_1 + \\cdots + c_K=0\\). ensures32 Keep mind , although independent tests nice mathematically, contrasts encode hypothesis interest researchers: choose contrasts meaningful, orthogonal.Example 4.1  (Contrasts encouragement teaching) arithmetic data example considered five different treatment groups 9 individuals . Two control groups, one received praise, another reproved last ignored.Suppose researchers interested assessing whether experimental manipulation effect, whether impact positive negative feedback students.33Suppose five groups order (control 1, control 2, praised, reproved, ignored).\ncan express hypothesis \\(\\mathscr{H}_{01}\\): \\(\\mu_{\\text{praise}} = \\mu_{\\text{reproved}}\\)\\(\\mathscr{H}_{02}\\):\n\\[\\begin{align*}\n\\frac{1}{2}(\\mu_{\\text{control}_1}+\\mu_{\\text{control}_2}) = \\frac{1}{3}\\mu_{\\text{praised}} + \\frac{1}{3}\\mu_{\\text{reproved}} + \\frac{1}{3}\\mu_{\\text{ignored}}\n\\end{align*}\\]Note , hypothesis control vs experimental manipulation, look average different groups associated item. Using ordering, weights contrast vector \\((1/2, 1/2, -1/3, -1/3, -1/3)\\) \\((0, 0, 1, -1, 0)\\). many equivalent formulation: multiply weights number (different zero) get test statistic, latter standardized.Example 4.2  (Teaching read) consider data Baumann, Seifert-Kessell, Jones (1992). abstract paper provides brief description studyThis study investigated effectiveness explicit instruction think aloud means promote elementary students’ comprehension monitoring abilities. Sixty-six fourth-grade students randomly assigned one three experimental groups: () Think-Aloud (TA) group, students taught various comprehension monitoring strategies reading stories (e.g., self-questioning, prediction, retelling, rereading) medium thinking aloud; (b) Directed reading-Thinking Activity (DRTA) group, students taught predict-verify strategy reading responding stories; (c) Directed reading Activity (DRA) group, instructed control, students engaged noninteractive, guided reading stories.\nTable 4.1: Estimated group averages standard errors 95% confidence intervals post-test 1.\ncan see DRTA highest average, followed TA directed reading (DR).\npurpose Baumann, Seifert-Kessell, Jones (1992) make particular comparison treatment groups.\nabstract:primary quantitative analyses involved two planned orthogonal contrasts—effect instruction (TA + DRTA vs. 2 x DRA) intensity instruction (TA vs. DRTA)—three whole-sample dependent measures: () error detection test, (b) comprehension monitoring questionnaire, (c) modified cloze test.hypothesis Baumann, Seifert-Kessell, Jones (1992) \\(\\mathscr{H}_0: \\mu_{\\mathrm{TA}} + \\mu_{\\mathrm{DRTA}} = 2 \\mu_{\\mathrm{DRA}}\\)\n, rewritten slightly,\n\\[\\begin{align*}\n\\mathscr{H}_0: - 2 \\mu_{\\mathrm{DR}} + \\mu_{\\mathrm{DRTA}} + \\mu_{\\mathrm{TA}} = 0.\n\\end{align*}\\]\nweights \\((-2, 1, 1)\\); order levels treatment \n(\\(\\mathrm{DRA}\\), \\(\\mathrm{DRTA}\\), \\(\\mathrm{TA}\\)) must match coefficients.\nequivalent formulation \\((2, -1, -1)\\) \\((1, -1/2, -1/2)\\): either case, estimated differences different\n(constant multiple sign change).\nvector weights \\(\\mathscr{H}_0: \\mu_{\\mathrm{TA}} = \\mu_{\\mathrm{DRTA}}\\)\n(\\(0\\), \\(-1\\), \\(1\\)): zero appears first component, \\(\\mathrm{DRA}\\) doesn’t appear.\ntwo contrasts orthogonal since\n\\((-2 \\times 0) + (1 \\times -1) + (1 \\times 1) = 0\\).\nTable 4.2: Estimated contrasts post-test 1.\ncan look differences; since DRTA versus TA pairwise difference, obtained \\(t\\)-statistic directly pairwise contrasts\nusing pairs(emmeans_post). Note two different ways writing comparison DR average two methods yield different point estimates, inference (meaning \\(p\\)-values). contrast \\(C_{1b}\\), get half estimate (standard error also halved) likewise second contrasts get estimate \\(\\mu_{\\mathrm{DRTA}} - \\mu_{\\mathrm{TA}}\\) first case (\\(C_2\\)) \\(\\mu_{\\mathrm{TA}} - \\mu_{\\mathrm{DRTA}}\\): difference group averages sign.conclusion analysis contrasts?\nlooks like methods involving teaching aloud strong impact reading comprehension relative directed reading. evidence strong\ncompare method combines directed reading-thinking activity thinking aloud.","code":"\nlibrary(emmeans)\ndata(arithmetic, package = \"hecedsm\")\nlinmod <- aov(score ~ group, data = arithmetic)\nlinmod_emm <- emmeans(linmod, specs = 'group')\ncontrast_specif <- list(\n  controlvsmanip = c(0.5, 0.5, -1/3, -1/3, -1/3),\n  praisedvsreproved = c(0, 0, 1, -1, 0)\n)\ncontrasts_res <- \n  contrast(object = linmod_emm, \n                    method = contrast_specif)\n# Obtain confidence intervals instead of p-values\nconfint(contrasts_res)\nlibrary(emmeans) #load package\ndata(BSJ92, package = \"hecedsm\")\nmod_post <- aov(posttest1 ~ group, data = BSJ92)\nemmeans_post <- emmeans(object = mod_post, \n                        specs = \"group\")\n# Identify the order of the level of the variables\nwith(BSJ92, levels(group))\n#> [1] \"DR\"   \"DRTA\" \"TA\"\n# DR, DRTA, TA (alphabetical)\ncontrasts_list <- list(\n  \"C1: DRTA+TA vs 2DR\" = c(-2, 1, 1), \n  # Contrasts: linear combination of means, coefficients sum to zero\n  # 2xDR = DRTA + TA => -2*DR + 1*DRTA + 1*TA = 0 and -2+1+1 = 0\n  \"C1: average (DRTA+TA) vs DR\" = c(-1, 0.5, 0.5), \n  #same thing, but halved so in terms of average\n  \"C2: DRTA vs TA\" = c(0, 1, -1),\n  \"C2: TA vs DRTA\" = c(0, -1, 1) \n  # same, but sign flipped\n)\ncontrasts_post <- \n  contrast(object = emmeans_post,\n           method = contrasts_list)\ncontrasts_summary_post <- summary(contrasts_post)"},{"path":"contrasts-multiple-testing.html","id":"multiple-testing","chapter":"4 Contrasts and multiple testing","heading":"4.2 Multiple testing","text":"Beyond looking global null, interested set contrast statistics typically number can large-ish. however catch starting test multiple hypothesis .single hypothesis test testing procedure well calibrated (meaning model model assumptions hold), probability \\(\\alpha\\) making type error null true, meaning difference averages underlying population. problem approach look, higher chance finding something: 20 independent tests, expect , average, one yield \\(p\\)-value less 5%. , coupled tendency many fields dichotomise result every test depending whether \\(p \\leq \\alpha\\) (statistically significant level \\(\\alpha\\) leads selective reporting findings.tests interest, even standard software report possible pairwise comparisons. However, number tests performed course analysis can large. Dr. Yoav Benjamini investigated number tests performed study Psychology replication project (Nosek et al. 2015): number ranged 4 700, average 72 per study. natural ask many spurious findings correspond type errors. paramount (absurd) illustration xkcd cartoon Figure 4.1.tests interest, even standard software report possible pairwise comparisons. \\(K\\) groups compare comparison interest, performs \\(\\binom{K}{2}\\) pairwise comparisons \\(\\mathscr{H}_{0}: \\mu_i = \\mu_j\\) \\(\\neq j\\). \\(K=3\\), three comparisons, 10 pairwise comparisons \\(K=5\\) 45 pairwise comparisons \\(K=10\\). Thus, ‘discoveries’ bound spurious.number tests performed course analysis can large. Y. Benjamini investigated number tests performed study Psychology replication project (Nosek et al. 2015): number ranged 4 700, average 72 — studies account fact performing multiple tests selected model. natural ask many results spurious findings correspond type errors. paramount (absurd) illustration cartoon presented Figure 4.1: note little scientific backing theory (thus test shouldn’t interest begin ) likewise selective reporting made conclusions, despite nuanced conclusions.can also assess mathematically problem. Assume simplicity tests independent34 test conducted level \\(\\alpha\\). probability making least one type error, say \\(\\alpha^{\\star}\\), is35\n\\[\\begin{align}\\alpha^{\\star} &= 1 – \\text{probability making type error} \\\\\\ &= 1- (1-\\alpha)^m\\\\\n& \\leq m\\alpha\n  \\tag{4.1}\n\\end{align}\\]\\(\\alpha = 5\\)% \\(m=4\\) tests, \\(\\alpha^{\\star} \\approx 0.185\\) whereas \\(m=72\\) tests, \\(\\alpha^{\\star} \\approx 0.975\\): means almost guaranteed even nothing going find “statistically significant” yet meaningless results.\nFigure 4.1: xkcd 882: Significant. alt text ‘, uh, green study got link. probably –’ ‘RESEARCH CONFLICTED GREEN JELLY BEAN/ACNE LINK; STUDY RECOMMENDED!’\nsensible try reduce bound number false positive control probability getting spurious findings. consider family \\(m\\) null hypothesis \\(\\mathscr{H}_{01}, \\ldots, \\mathscr{H}_{0m}\\) tested. family simply collection \\(m\\) hypothesis tests: exact set depends context, comprises hypothesis scientifically relevant reported. comparisons called pre-planned comparisons: chosen experiment takes place pre-registered avoid data dredging selective reporting. number planned comparisons kept small relative number parameters: one-way ANOVA, general rule thumb make comparisons number groups, \\(K\\).Suppose perform \\(m\\) hypothesis tests study define binary indicators\n\\[\\begin{align}\nR_i &= \\begin{cases} 1 & \\text{reject null hypothesis }  \\mathscr{H}_{0i} \\\\\n0 & \\text{fail reject } \\mathscr{H}_{0i}\n\\end{cases}\\\\\nV_i &=\\begin{cases} 1 & \\text{type error } \\mathscr{H}_{0i}\\quad  (R_i=1 \\text{  }\\mathscr{H}_{0i} \\text{ true}) \\\\ 0 & \\text{otherwise}.\n\\end{cases}\n\\end{align}\\]\nnotation, \\(R=R_1 + \\cdots + R_m\\) simply encodes total number rejections (\\(0 \\leq R \\leq m\\)), \\(V = V_1 + \\cdots + V_m\\) number null hypothesis rejected mistake (\\(0 \\leq V \\leq R\\)).familywise error rate probability making least one type error per family,\n\\[\\begin{align*}\n\\mathsf{FWER} = \\Pr(V \\geq 1).\n\\end{align*}\\]\ncontrol familywise error rate, one must stringent rejecting null perform test smaller level \\(\\alpha\\) overall simultaneous probability less \\(\\mathsf{FWER}\\).","code":""},{"path":"contrasts-multiple-testing.html","id":"bonferronis-procedure","chapter":"4 Contrasts and multiple testing","heading":"4.2.1 Bonferroni’s procedure","text":"easiest way (one least powerful option) directly use inequality eq. (4.1). test performed level \\(\\alpha/m\\), family-wise error controlled level \\(\\alpha\\).Bonferroni adjustment also controls per-family error rate, expected (theoretical average) number false positive \\(\\mathsf{PFER} = \\mathsf{E}(V)\\). latter stringent criterion familywise error rate \\(\\Pr(V \\geq 1) \\leq \\mathsf{E}(V)\\): familywise error rate make distinction one multiple type errors.36Why Bonferroni’s procedure popular? conceptually easy understand simple, applies design regardless dependence tests. However, number tests adjust , \\(m\\), must prespecified procedure leads low power size family large. Moreover, sole objective control familywise error rate, procedures always better sense still control \\(\\mathsf{FWER}\\) leading increased capacity detection null false.raw (.e., unadjusted) \\(p\\)-values reported, reject hypothesis \\(\\mathscr{H}_{0i}\\) \\(m \\times p_i \\ge \\alpha\\): operationally, multiply \\(p\\)-value \\(m\\) reject result exceeds \\(\\alpha\\).","code":""},{"path":"contrasts-multiple-testing.html","id":"holm-bonferronis-procedure","chapter":"4 Contrasts and multiple testing","heading":"4.2.2 Holm-Bonferroni’s procedure","text":"idea Holm’s procedure use sharper inequality bound amounts performing tests different levels, stringent smaller \\(p\\)-values.Order \\(p\\)-values family \\(m\\) tests smallest largest\n\\(p_{(1)} \\leq \\cdots \\leq p_{(m)}\\) test sequentially hypotheses. Coupling Holm’s method Bonferroni’s procedure, compare \\(p_{(1)}\\) \\(\\alpha_{(1)} = \\alpha/m\\), \\(p_{(2)}\\) \\(\\alpha_{(2)}=\\alpha/(m-1)\\), etc.\\(p\\)-values less respective levels, still reject null hypothesis. Otherwise, reject tests whose \\(p\\)-values exceeds smallest nonsignificant one. \\(p_{(j)} \\geq \\alpha_{(j)}\\) \\(p_{()} \\leq \\alpha_{()}\\) \\(=1, \\ldots, j-1\\) (smaller \\(p\\)-values), reject associated hypothesis \\(\\mathscr{H}_{0(1)}, \\ldots, \\mathscr{H}_{0(j-1)}\\) fail reject \\(\\mathscr{H}_{0(j)}, \\ldots, \\mathscr{H}_{0(m)}\\).procedure doesn’t control per-family error rate, uniformly powerful thus leads increased detection Bonferroni’s method.see , consider family \\(m=3\\) \\(p\\)-values values \\(0.01\\), \\(0.04\\) \\(0.02\\). Bonferroni’s adjustment lead us reject second third hypotheses level \\(\\alpha=0.05\\), Holm-Bonferroni.continued…","code":""},{"path":"complete-factorial-designs.html","id":"complete-factorial-designs","chapter":"5 Complete factorial designs","heading":"5 Complete factorial designs","text":"next consider experiments designs multiple factors manipulated experimenter simultaneously.jumping statistical analysis, let us discuss briefly examples covered sequel.Example 5.1  (Psychological ownership borrowed money) Supplemental Study 5 Sharma, Tully, Cryder (2021) checks psychological perception borrowing money depending label. authors conducted 2 2 -subject comparison (two-way ANOVA) varying type debt (whether money advertised credit loan) type purchase latter used (discretionary spending need). response average likelihood interest product, measured using 9 point Likert scale 1 9.Example 5.2  (Spatial orientation shrinks expands psychological distance) Maglio Polman (2014) measured subjective distance travel based direction travel. conducted experiment Toronto subway green line, asking commuters Bay station answer question “far away [name] station feel ?” using 7 point Likert scale ranging close (1) far (7). stations name one Spadina, St. George, Bloor–Yonge Sherbourne (West East).four stations two directions travel (4 2 design), scientific question interest subjective measures distance consist perceiving differently distance depending direction travel. also wonder whether destinations two stations away Bay (Spadina Sherbourne) considered equidistant, similarly two.","code":""},{"path":"complete-factorial-designs.html","id":"efficiency-of-multiway-analysis-of-variance.","chapter":"5 Complete factorial designs","heading":"5.1 Efficiency of multiway analysis of variance.","text":"Consider setting Sharma, Tully, Cryder (2021) suppose want check impact debt collect certain number observations group. suspected label influence, run one-way analysis variance spending type separately (thus, two one-way ANOVA two groups). likewise wanted instead focus whether spending discretionary nature , label: together, give total eight sets observations. Combining two factors allows us halve number groups/samples collect simple setting: highlights efficiency running experiment modifying instances , series one-way analysis variance. concept extends higher dimension manipulate two factors. Factorial designs allow us study impact multiple variables simultaneously fewer overall observations.drawback increase number factors, total number subgroups increases: complete design37 factors \\(\\), \\(B\\), \\(C\\), etc. \\(n_a\\), \\(n_b\\), \\(n_c\\), \\(\\ldots\\) levels, total \\(n_a\\times n_b \\times n_c \\times \\cdots\\) combinations number observations needed efficiently measure group means increases quickly. curse dimensionality: larger number experimental treatments manipulated together, larger sample size needed. efficient approach, cover later section, relies measuring multiple observations experimental units, example giving multiple tasks (randomly ordered) participants.Intrinsically, multiway factorial design model description change relative one-way design: analysis variance describes sample mean response subgroup,Consider two-way analysis variance model. linear model two factors, \\(\\) \\(B\\), respectively \\(n_a\\) \\(n_b\\) levels. response \\(Y_{ijk}\\) \\(k\\)th measurement group \\((a_i, b_j)\\) \n\\[\\begin{align}\n\\underset{\\text{response}\\vphantom{b}}{Y_{ijk}} = \\underset{\\text{subgroup mean}}{\\mu_{ij}} + \\underset{\\text{error term}}{\\varepsilon_{ijk}}\n\\tag{5.1}\n\\end{align}\\]\n\\(Y_{ijk}\\) \\(k\\)th replicate \\(\\)th level factor \\(\\) \\(j\\)th level factor \\(B\\)\\(\\mu_{ij}\\) average response measurements group \\((a_i, b_j)\\)\\(\\varepsilon_{ijk}\\) independent error terms mean zero standard deviation \\(\\sigma\\)., turns , special case linear regression model. build contrasts comparing group averages, convenient reparametrize model hypotheses interest directly expressed terms parameters.example, Maglio Polman (2014) study, gather observations factor combination table, direction row station column.Table 5.1:  Conceptual depiction cell average two two design Maglio Polman (2014)\\(\\)th row mean represents average response across levels \\(B\\),\n\\(\\mu_{.} = (\\mu_{i1} + \\cdots + \\mu_{in_b})/n_b\\) similarly average \\(j\\)th column, \\(\\mu_{.j} = (\\mu_{1j} + \\cdots + \\mu_{n_aj})/n_a.\\) Finally, overall average \n\\[\\mu = \\frac{\\sum_{=1}^{n_a} \\sum_{j=1}^{n_b} \\mu_{ij}}{n_an_b}.\\]subgroup average \\(\\mu_{ij}\\) estimated sample mean observations group use formulae obtain estimates row, column overall means \\(\\widehat{\\mu}_{.}\\), \\(\\widehat{\\mu}_{.j}\\) \\(\\widehat{\\mu}\\). sample balanced, meaning number observations , summing observations row, column table averaging. general setup, however, give equal weight subgroup average.","code":""},{"path":"complete-factorial-designs.html","id":"interactions","chapter":"5 Complete factorial designs","heading":"5.2 Interactions","text":"Table 5.1 shows individual mean subgroup. , may interested looking experiment single one-way analysis variance model eight subgroups, series one-way analysis variance either direction station sole factor.use particular terminology refer :simple effects: difference levels one fixed combination others. Simple effects comparing cell averages within given row column.main effects: differences relative average condition factor. Main effects row/column averages.interaction effects: simple effects differ depending levels another factor. Interactions effects difference relative row column average.words, interaction occurs experimental factors, coupled together, different impacts superposition . interaction two factors occurs average effect one independent variable depends level .significant interaction, main effects interest since misleading. Rather, compute simple effects making comparison one level time.example Maglio Polman (2014), simple effect comparing distance Spadina Sherbourne east. main effect direction average perceived distance east west. Finally, interaction measure much differ station depending direction.\nFigure 5.1: Line graph example patterns means possible kinds general outcomes 2 2 design. Illustration adapted Figure 10.2 Crump, Navarro, Suzuki (2019) Matthew Crump (CC -SA 4.0 license).\nbetter understand, consider average response suppose access true population average sub-treatment. can represent population using line graph two factors, one mapped color another \\(x\\)-axis. Figure 5.1 shows happens possible scenarios 2 2 design. overall effect, mean constant. isn’t main effect \\(\\), average two mean response \\(a_1\\) \\(a_2\\) , etc. Interactions depicted non-parallel lines.’s clear Figure 5.1 looking average \\(\\) alone (main effect) isn’t instructive presence interaction: rather, comparing values \\(\\) \\(b_1\\) separately \\(b_2\\), vice-versa using simple effects, otherwise conclusions may misleading.Example 5.3  (Interaction plots Maglio Polman (2014)) hypothesis interest interaction; time , can simply plot average per group. Since summary statistics can hide important information uncertainty, add 95% confidence intervals subgroup averages superimpose jittered observations show spread data. Based Figure 5.2, appears least interaction station direction travel, addition main effect station. Formal hypothesis testing can help check intuition.\nFigure 5.2: Interaction plot Study 1 Maglio Polman (2014), showing group averages 95% confidence intervals means. Observations overlaid graph.\n","code":""},{"path":"complete-factorial-designs.html","id":"model-parametrization","chapter":"5 Complete factorial designs","heading":"5.3 Model parametrization","text":"model parametrized terms subgroup cell average okay Equation (5.1), doesn’t help us want check presence main effects interaction, even possible specify contrasts required test hypotheses. can however express model terms main effects interactions.consider alternative formulation\n\\[\\begin{align*}\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk},\n\\end{align*}\\]\n\\(\\mu\\) average subgroup averages, termed overall mean.\\(\\alpha_i = \\mu_{.} - \\mu\\) mean level \\(A_i\\) minus overall mean.\\(\\beta_j = \\mu_{.j} - \\mu\\) mean level \\(B_j\\) minus overall mean.\\((\\alpha\\beta)_{ij} = \\mu_{ij} - \\mu_{.} - \\mu_{.j} + \\mu\\) interaction term \\(A_i\\) \\(B_j\\) encodes effect variable already captured main effects.rapid calculation shows coefficients number cells subgroups (\\(n_an_b\\) cells overall) table. model overparametrized: get away , impose constraints remove redundancies. idea know \\(n_a-1\\) mean factor \\(\\) global average combination , can deduce value last row mean. model formulation terms difference global average main effect ensures can test main effects factor \\(\\) setting \\(\\mathscr{H}_0: \\alpha_1 = \\cdots = \\alpha_{n_a-1}=0\\). sum zero constraints,\n\\[\\sum_{=1}^{n_a} \\alpha_i=0, \\quad \\sum_{j=1}^{n_b} \\beta_j=0, \\quad  \\sum_{j=1}^{n_b} (\\alpha\\beta)_{ij}=0, \\quad \\sum_{=1}^{n_a} (\\alpha\\beta)_{ij}=0,\\] restore identifiability imposes\nimposes \\(1 + n_a + n_b\\) constraints.redundancy information, due fact main effects expressible row column averages, overall mean average observations, arise consider degrees freedom tests.continued…Example 5.4  (Testing Psychological ownership borrowed money) Sharma, Tully, Cryder (2021) first proceeded test interaction. Since one global average two main effect (additional difference average factors debttype purchase), interaction involves one degree freedom since go model three parameters describing mean one different average four subgroups.reason first test carry effect one factor depends level , shown 5.1, need compare label debt type separately type purchase vice-versa using simple effects. interaction contrary isn’t significant, pool observations instead average across either two factors, resulting marginal comparisons main effects.Fitting model including interaction factors ensures keep additivity assumption conclusions aren’t misleading: price pay additional mean parameters estimated, isn’t issue collect enough data, can critical data collection extremely costly runs allowed.R, include factors formula \nresponse ~ factorA * factorB, * symbol indicating allowed interact; main effect model, use instead + reflect effects factors add .analysis variance table, focus last line sum squares purchase:debttype. \\(F\\) statistic 1.785; using \\(\\mathsf{F}\\) (1, 1497) distribution benchmark, obtain \\(p\\)-value 0.18 evidence effect purchase depends debt type.can thus pool data look effect debt type (loan credit) overall combining results purchase types, one planned comparison reported Supplementary material. R emmeans package, use emmeans function quote factor interest (.e., one want keep) specs. default, compute estimate marginal means: contr = \"pairwise\" indicates want difference two, gives us contrasts.get simple effects, give variables specs factors compute subgroup means, set additionally command specify variable want separate results . get difference average credit loan labels purchase type along \\(t\\) statistics marginal contrast \\(p\\)-value. simple effects suggest label impact perception discretionary expenses rather needed ones, runs counter-intuitively lack interaction.Summary:Complete factorial designs consist experiments manipulate multiple experimental factors collect observations subgroup.Factorial designs efficient running repeatedly one-way analysis variance sample size per group.Interactions occur effect variable depends levels others.Interaction plots (group average per group) can help capture difference, beware overinterpretation small samples.interaction, consider differences contrasts level factor (simple effects).interaction, can pool observations look main effects.multiway analysis variance can treated one-way analysis variance collapsing categories; however, specific contrasts interest.number observations increases quickly dimension increase number factors considered.","code":"\n# Analysing Supplementary Study 5\n# of Sharma, Tully, and Cryder (2021)\ndata(STC21_SS5, package = \"hecedsm\")\nmod <- aov(likelihood ~ purchase*debttype, \n           data = STC21_SS5)\nmodel.tables(mod, type = \"means\")\n#> Tables of means\n#> Grand mean\n#>      \n#> 4.88 \n#> \n#>  purchase \n#>     discretionary    need\n#>             4.182   5.579\n#> rep       751.000 750.000\n#> \n#>  debttype \n#>      credit    loan\n#>       5.127   4.631\n#> rep 753.000 748.000\n#> \n#>  purchase:debttype \n#>                debttype\n#> purchase        credit loan \n#>   discretionary   4.5    3.8\n#>   rep           392.0  359.0\n#>   need            5.7    5.4\n#>   rep           361.0  389.0\n# Analysis of variance reveals \n# non-significant interaction\n# of purchase and type\ncar::Anova(mod, type = 3)\n#> Anova Table (Type III tests)\n#> \n#> Response: likelihood\n#>                   Sum Sq   Df F value  Pr(>F)    \n#> (Intercept)         7974    1 1040.96 < 2e-16 ***\n#> purchase             283    1   36.91 1.6e-09 ***\n#> debttype              88    1   11.55  0.0007 ***\n#> purchase:debttype     14    1    1.79  0.1817    \n#> Residuals          11467 1497                    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Main effects\nemmeans::emmeans(mod, \n                 specs = \"debttype\",\n                 contr = \"pairwise\")\n#> $emmeans\n#>  debttype emmean    SE   df lower.CL upper.CL\n#>  credit     5.12 0.101 1497     4.93     5.32\n#>  loan       4.63 0.101 1497     4.43     4.83\n#> \n#> Results are averaged over the levels of: purchase \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast      estimate    SE   df t.ratio p.value\n#>  credit - loan    0.496 0.143 1497   3.470  0.0005\n#> \n#> Results are averaged over the levels of: purchase\n  \n# Pairwise comparisons within levels of purchase\n# Simple effect\nemmeans::emmeans(mod, \n                 specs = c(\"purchase\", \"debttype\"),\n                 by = \"purchase\",\n                 contr = \"pairwise\")\n#> $emmeans\n#> purchase = discretionary:\n#>  debttype emmean    SE   df lower.CL upper.CL\n#>  credit     4.51 0.140 1497     4.24     4.78\n#>  loan       3.82 0.146 1497     3.54     4.11\n#> \n#> purchase = need:\n#>  debttype emmean    SE   df lower.CL upper.CL\n#>  credit     5.74 0.146 1497     5.45     6.02\n#>  loan       5.43 0.140 1497     5.16     5.71\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#> purchase = discretionary:\n#>  contrast      estimate    SE   df t.ratio p.value\n#>  credit - loan    0.687 0.202 1497   3.400  0.0007\n#> \n#> purchase = need:\n#>  contrast      estimate    SE   df t.ratio p.value\n#>  credit - loan    0.305 0.202 1497   1.510  0.1318"},{"path":"designs-to-reduce-the-error.html","id":"designs-to-reduce-the-error","chapter":"6 Designs to reduce the error","heading":"6 Designs to reduce the error","text":"previous chapter dealt factorial experiments experimental factors interest. many instances, characteristics observational units interest: example, EEG measurements participants lab may differ due time day, lab technician, etc. instances blocking factors: variables impact measurements’s variability, direct interest. filtering effect looking residual variability unexplained blocking factors. Block designs reduce error term, cost including estimating additional parameters (group average slope).analyse block designs multi-way analysis variance model, one notable exception. Typically, assume interaction experimental factor blocking factors.38 Thus, interested mostly marginal effects.related design includes continuous covariate analysis variance, whose slope governs relationship response. strict inclusion isn’t necessary draw valid causal conclusion, adding term helps reduce residual variability. design historically called analysis covariance, instance linear model.Including blocking factor covariates principle increase power ability detect real differences due experimental manipulations, provided variables used control correlated response. Generally, needed valid inference, guaranteed randomization, shouldn’t used assign treatment.","code":""},{"path":"designs-to-reduce-the-error.html","id":"ancova","chapter":"6 Designs to reduce the error","heading":"6.1 Analysis of covariance","text":"analysis covariance, include linear component (continuous) covariate, purpose reduce residual error. prime example prior/post experiment measurements, whereby monitor change outcome due manipulation.setting, may seem logical take difference post prior score response: showcased Example 6.2 Baumann, Seifert-Kessell, Jones (1992), analysis presented course website.add covariate, need latter strong linear correlation inclusion make sense. can assess graphically whether relationship linear, whether slopes experimental condition .39\nFigure 6.1: Simulated data two groups analysis covariance model.\nleft panel Figure 6.1 shows ideal situation analysis covariate: relationship response covariate linear strong correlation, slope overlapping support. Since slopes , can compare difference average (vertical difference slopes level covariate) latter constant, depiction useful. contrast, right-hand panel Figure 6.1 shows interaction covariate experimental groups, different slopes: , effect experimental condition increases level covariate. One may also note lack overlap support, set values taken covariate, two experimental conditions, makes comparison hazardous best right-hand panel.Figure 6.2 shows , due strong correlation, variability measurements smaller right-hand panel (corresponding analysis covariance model) centred response left-hand panel; note \\(y\\)-axes different scales.\nFigure 6.2: Response subtracting mean (left) detrending (right).\npresent two examples analysis covariance, showing inclusion covariates helps disentangle differences experimental conditions.Example 6.1  (Inconsistency product description image online retailing) Lee Choi (2019) measured impact discrepancies descriptions visual depiction items online retail. performed experiment participants presented descriptions product (set six toothbrushes) either consistent inconsistent description. authors postulated discrepancy lead lower appreciation score, measured using three Likert scales. also suspected familiarity product brand impact ratings, controlled latter using another question.One way account familiarity comparing mean use linear regression familiarity another explanatory variable. expected value product evaluation \n\\[\\begin{align}\n\\mathsf{E}(\\texttt{prodeval}) = \\beta_0 + \\beta_1 \\texttt{familiarity} + \\beta_2 \\texttt{consistency}, \\tag{6.1}\n\\end{align}\\]\n\\(\\texttt{familiarity}\\) score 1 7 \\(\\texttt{consistency}\\) binary indicator equal one output inconsistent zero otherwise.\ncoefficient \\(\\beta_2\\) thus measures difference product evaluation rating consistent vs inconsistent displays, familiarity score.can look coefficient (standard error) estimates \\(\\widehat{\\beta}_2 = -0.64 (0.302)\\).\ndifference groups mean \\(\\beta_2=0\\) can build test statistic looking standardized regression coefficient \\(t = \\widehat{\\beta}_2/\\mathsf{se}(\\widehat{\\beta}_2)\\). result output \\(b = -0.64\\), 95% CI \\([-1.24, -0.04]\\), \\(t(93) = -2.12\\), \\(p = .037\\). reject null hypothesis equal product evaluation display level 5%: evidence small difference, people giving average score 0.64 points smaller (scale 1 9) presented conflicting descriptions images.can compare analysis variance table obtained fitting model without \\(\\texttt{familiarity}\\). Table 6.1 shows effect consistency small significant two-sample t-test shows evidence difference average familiarity score experimental conditions (\\(p\\)-value .532). However, can explain roughly one fifth residual variability familiarity brand (see sum squares Table 6.1): removing latter leads higher signal--noise ratio impact consistency, expense loss one degree freedom. Thus, appears manipulation successful.\nTable 6.1: Analysis variance tables models without \\(\\texttt{familiarity}\\).\n\nTable 6.1: Analysis variance tables models \\(\\texttt{familiarity}\\).\n\nFigure 6.3: Scatterplot product evaluation function familiarity score, split experimental manipulation.\nFigure 6.3 shows people familiar product brand tend positive product evaluation, postulated authors. graph also shows two straight lines corresponding fit linear model different intercept slope display group: little material difference, one needs assess formally whether single linear relationship one postulated eq.(6.1) can adequately characterize relation groups.effect, fit linear model different slopes group, compare fit latter analysis covariance model includes single slope groups: can test slopes , alternatively difference slopes zero. t-statistic indicates difference slope (\\(p\\)-value .379), thus assumption reasonable. Levene’s test homogeneity variance indicates discernible difference groups. Thus, appears difference perception product quality due manipulation.Example 6.2  (Effect scientific consensus false beliefs) consider Study 3 Stekelenburg et al. (2021), studied changes perception people holding false beliefs denying (extent) scientific consensus presenting news article showcasing information various phenomena. experimental manipulation consisted presenting boosting, form training help readers identify establish whether scientifists truly expert domain interest, strong consensus, etc.40The third final experiment paper focused genetically modified organisms: replication Study 2, control group (since detectable difference experimental conditions Boost BoostPlus) larger sample size (Study 2 underpowered).data include 854 observations prior, negative prior belief score participant, post experiment score veracity claim. measured using visual scale ranging -100 (100% certain false) 100 (100% certain true), 0 (don’t know) middle. people negative prior beliefs recruited study. three experimental conditions BoostPlus, consensus control group. Note scores data negated, meaning negative posterior scores indicate agreement consensus GMO.Preliminary checks suggest , although slopes prior beliefs plausibly group data properly randomized, evidence unequal variance changes score. , fit model mean\n\\[\\begin{align*}\n\\mathsf{E}(\\texttt{post}) &= \\begin{cases}\n\\beta_0 + \\beta_1 \\texttt{prior} + \\alpha_1 &  \\texttt{condition} = \\texttt{BoostPlus}\\\\\n\\beta_0 + \\beta_1 \\texttt{prior} + \\alpha_2 &\\texttt{condition} = \\texttt{consensus}\\\\\n\\beta_0 + \\beta_1 \\texttt{prior} + \\alpha_3 &\\texttt{condition} = \\texttt{control}\n\\end{cases}\n\\end{align*}\\]\n\\(\\alpha_1 + \\alpha_2 + \\alpha_3=0\\), using sum--zero parametrization, different variance experimental condition,\n\\[\\begin{align*}\n\\mathsf{Va}(\\texttt{post}) = \\begin{cases}\n\\sigma^2_1, &  \\texttt{condition} = \\texttt{BoostPlus},\\\\\n\\sigma^2_2, &  \\texttt{condition} = \\texttt{consensus},\\\\\n\\sigma^2_3, & \\texttt{condition} = \\texttt{control}.\n\\end{cases}\n\\end{align*}\\]\nunequal variances, use multiple testing procedures reserved analysis variance resort instead Holm–Bonferroni control familywise error rate. look pairwise differences conditions.41\nTable 6.2: Analysis variance tables model (left) without (right) \\(\\texttt{prior}\\) belief score.\nRepeating exercise comparing amount evidence comparison without inclusion covariate shows value test statistic larger (Table 6.2), indicative stronger evidence analysis covariance model: conclusion unaffected large sample sizes. course care little global \\(F\\) test equality mean, previous study shown large differences. interesting quantifying change conditions.\nTable 6.3: Pairwise contrasts \\(p\\)-values adjusted using Holm–Bonferroni ANOVA model (without \\(\\texttt{prior}\\) belief score).\n\nTable 6.3: Pairwise contrasts \\(p\\)-values adjusted using Holm–Bonferroni ANCOVA model (\\(\\texttt{prior}\\) belief score).\nTable 6.3 shows pairwise contrasts, measure two different things: analysis variance model compares average group, whereas analysis covariance (linear model prior) uses detrended values focuses change perception. data unbalanced estimate group mean variance separately, degrees freedom change one pairwise comparison next. , using covariate prior, somewhat strongly correlated post seen Figure 6.4, helps decrease background noise.\nTable 6.4: Summary statistics belief function time measurement experimental condition.\n\nFigure 6.4: Difference prior post experiment beliefs genetically engineered food.\nStekelenburg et al. (2021) split data pairwise comparisons two time (thus taking roughly two-third data perform two sample t-test pair). Although impact conclusion, approach conceptually incorrect: variance equal, want use observations estimate (approach suboptimal, since estimate variance three times smaller samples).contrary, using model assumes equal variance case leads distortion: variance estimate sort average variability \\(\\sigma_i\\) \\(\\sigma_j\\) experimental condition \\(\\) \\(j\\), potentially leading distortions. large samples, may unconsequential, illustrates caveats subsample analyses.Figure 6.5 shows relationship prior posterior score. data show clear difference individuals: many start completely disbelieving genetically engineered food change mind (sometimes drastically), many people change idea similar scores, many give posterior score zero. heterogeneity data illustrates danger looking summary statistics comparing averages. tell whole picture! One investigate whether strength religious political beliefs, much participants trust scientists, explain observed differences.\nFigure 6.5: Scatterplot negated prior posterior belief score.\nSummary:Inclusion blocking factor continuous covariates may help filtering unwanted variability.typically concomitant variables (measured alongside response variable).designs reduce residual error, leading increase power (ability detect differences average experimental conditions).interested differences due experimental condition (marginal effects).general, interaction covariates/blocking factors experimental conditions.hypothesis can assessed comparing models without interaction, enough units (e.g., equality slope ANCOVA).Box, Hunter, Hunter (1978) write page 103 following moto:Block can randomize .Explain main benefit blocking confounding variables (possible) randomization.","code":""},{"path":"effect-sizes-and-power.html","id":"effect-sizes-and-power","chapter":"7 Effect sizes and power","heading":"7 Effect sizes and power","text":"social studies, common write paper containing multiple studies similar topic. may use different designs, varying sample size. studies uses different questionnaires, change Likert scale, results mean difference groups directly comparable experiments.Another related task replication study, whereby researchers use material setting re-run experiment different data. replication somewhat successful (least reliable), one needs determine beforehand many participants recruited study. order , one needs measure effect size set desired power.think example comparing statistics \\(p\\)-values, construction standardized unitless measures, making comparable across study.\nTest statistics show outlying observed differences experimental conditions relative null hypothesis, typically effect (equal mean subgroup). However, statistics usually function sample size (number observations experimental condition) effect size (large standardized differences groups ), making unsuitable describing differences.\nFigure 7.1: True sampling distribution two-sample \\(t\\)-test alternative (rightmost curve) null distribution (leftmost curve) small (left panel) large (right panel) sample sizes.\nFigure 7.1 shows example sampling distributions difference mean null (curve centered zero) true alternative (mean difference two). area white curve represents power, larger larger sample size coincides smaller average \\(p\\)-values testing procedure.One argue , surface, every null hypothesis wrong , sufficiently large number observation, observed differences eventually become “statistically significant”. fact become certain estimated means experimental sub-condition. Statistical significance testing procedure translate practical relevance, depends scientific question hand.\nexample, consider development new drug commercialization Health Canada: minimum difference two treatments large enough justify commercialization new drug? effect small leads many lives saved, still relevant? decision involve trade-efficacy new treatment relative status quo, cost drug, magnitude improvement, etc.Effect size summaries inform standardized magnitude differences; used combine results multiple experiments using meta-analysis, calculate sample size requirements replicate effect power studies.","code":""},{"path":"effect-sizes-and-power.html","id":"effect-sizes","chapter":"7 Effect sizes and power","heading":"7.1 Effect sizes","text":"two main classes effect size: standardized mean differences ratio (percentages) explained variance. latter used analysis variance multiple groups compare.Unfortunately, literature effect size quite large. Researchers often fail distinguish estimand (unknown target) estimator used, frequent notational confusion arising due conflicting standards definitions. Terms also overloaded: notation may used denote effect size, calculated differently depending whether design -subject within-subject (repeated correlated measures per participant), whether blocking factors.","code":""},{"path":"effect-sizes-and-power.html","id":"standardized-mean-differences","chapter":"7 Effect sizes and power","heading":"7.1.1 Standardized mean differences","text":"gather intuition, begin task comparing means two groups using two-sample \\(t\\)-test, null hypothesis equality means \\(\\mathscr{H}_0: \\mu_1 = \\mu_2\\). test statistic \n\\[\\begin{align*}\nT =  \\frac{\\widehat{\\mu}_2 - \\widehat{\\mu}_1}{\\widehat{\\sigma}} \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)^{-1/2}\n\\end{align*}\\]\n\\(\\widehat{\\sigma}\\) pooled sample size estimator. first term, \\(\\widehat{d}_s = (\\widehat{\\mu}_2 - \\widehat{\\mu}_1)/\\widehat{\\sigma}\\), termed Cohen’s \\(d\\) (Cohen 1988) measures standardized difference groups, form signal--noise ratio. sample size gets larger larger, sample mean pooled sample variance become closer closer true population values \\(\\mu_1\\), \\(\\mu_2\\) \\(\\sigma\\); time, statistic \\(T\\) becomes bigger \\(n\\) becomes larger second term.42The difference \\(d=(\\mu_1-\\mu_2)/\\sigma\\) obvious interpretation: distance \\(\\) indicates means two groups \\(\\) standard deviation apart. Cohen’s \\(d\\) sometimes loosely categorized terms weak (\\(d = 0.2\\)), medium (\\(d=0.5\\)) large (\\(d=0.8\\)) effect size; , much like arbitrary \\(p\\)-value cutoffs, rules thumbs. Alongside \\(d\\), many commonly reported metrics simple transformations \\(d\\) describing observed difference. interactive applet Kristoffer Magnusson (Magnusson 2021) shows visual impact changing value \\(d\\) along.\ndifferent estimators \\(d\\) depending whether pooled variance estimator used. Cohen’s \\(d\\), upward biased, meaning gives values average larger truth. Hedge’s \\(g\\) (Hedges 1981) offers bias-correction always preferred estimator.different estimators, possible obtain (asymmetric) confidence intervals tolerance intervals.[using pivot method (Steiger 2004) relating effect size noncentrality parameter null distribution, whether \\(\\mathsf{St}\\), \\(\\mathsf{F}\\) \\(\\chi^2\\).]Example 7.1  (Surprise Reaching ) consider two-sample \\(t\\)-test study Liu et al. (2022+) discussed Example 2.5. difference average response index 0.371, indicating responder higher score. \\(p\\)-value 0.041, showing small effect.consider standardized difference \\(d\\), group means -0.289 standard deviations apart based Hedge’s \\(g\\), associated 95% confidence interval [-0.567, -0.011]: thus, difference found small (using Cohen (1988)’s convention) large uncertainty surrounding .42% probability observation drawn random responder condition exceed mean initiator group (probability superiority) 41.9% responder observations exceed mean initiator.","code":"\ndata(LRMM22_S1, package = \"hecedsm\")\nttest <- t.test(\n  appreciation ~ role, \n  data = LRMM22_S1,\n  var.equal = TRUE)\neffect <- effectsize::hedges_g(\n  appreciation ~ role, \n  data = LRMM22_S1, \n  pooled_sd = TRUE)\neffectsize::d_to_cles(effect)"},{"path":"effect-sizes-and-power.html","id":"ratio-and-proportion-of-variance","chapter":"7 Effect sizes and power","heading":"7.1.2 Ratio and proportion of variance","text":"Another class effect sizes obtained considering either ratio variance due effect (say differences means relative overall mean) relative background level noise measured variance.One common measure employed software Cohen’s f (Cohen 1988), one-way ANOVA (equal variance \\(\\sigma^2\\)) two groups,\n\\[\nf^2 = \\frac{1}{\\sigma^2} \\sum_{j=1}^k \\frac{n_j}{n}(\\mu_j - \\mu)^2 = \\frac{\\sigma^2_{\\text{effect}}}{\\sigma^2},\n\\]\nweighted sum squared difference relative overall mean \\(\\mu\\). \\(\\sigma^2_{\\text{effect}}\\) measure variability due difference mean, standardizing measurement variance gives us ratio variance values higher one indicating variability explainable, leading higher effect sizes. means every subgroup , \\(f=0\\). \\(k=2\\) groups, Cohen’s \\(f\\) Cohen’s \\(d\\) related via \\(f=d/2\\).Cohen’s \\(f\\) can directly related behaviour \\(F\\) statistic alternative, explained Section 7.2.1. However, since interpretation isn’t straightforward, typically consider proportions variance (rather ratios variance).build effect size, break variability explained experimental manipulation (\\(\\sigma^2_\\text{effect}\\)), denoted effect, leftover unexplained part, residual (\\(\\sigma^2_\\text{resid}\\)). one-way analysis variance, \\[\\sigma^2_{\\text{total}} = \\sigma^2_{\\text{resid}} + \\sigma^2_{\\text{effect}}\\] percentage variability explained \\(\\text{effect}\\).\n\\[\\eta^2 = \\frac{\\text{explained variability}}{\\text{total variability}}= \\frac{\\sigma^2_{\\text{effect}}}{\\sigma^2_{\\text{resid}} + \\sigma^2_{\\text{effect}}} = \\frac{\\sigma^2_{\\text{effect}}}{\\sigma^2_{\\text{total}}}.\\]\nSimple arithmetic manipulations reveal \\(f^2 = \\eta^2/(1-\\eta^2)\\), can relate proportion variance terms ratio vice-versa.effect size depends unknown population quantities (true means subgroup, overall mean variance). multiple alternative estimators estimate \\(\\eta^2\\), researchers often carefree reporting used. disambiguate, put \\(\\hat{\\eta}^2\\) denote estimator. make analogy, many different recipes (estimators) can lead particular cake, may lead mixing average wet well calibrated.default estimator \\(\\eta^2\\) coefficient determination linear regression, denoted \\(\\widehat{R}^2\\) \\(\\widehat{\\eta}^2\\). latter can reconstructed analysis variance table using formula\n\\[\n\\widehat{R}{}^2 = \\frac{F\\nu_1}{F\\nu_1 + \\nu_2}\n\\]\none-way ANOVA \\(\\nu_1 = K-1\\) \\(\\nu_2 = n-K\\) degrees freedom design \\(n\\) observations \\(K\\) experimental conditions.Unfortunately, \\(\\widehat{R}{}^2\\) upward biased estimator (large average), leading optimistic measures. Another estimator \\(\\eta^2\\) recommended Keppel Wickens (2004) power calculations \\(\\widehat{\\omega}^2\\), \n\\[\\widehat{\\omega}^2 = \\frac{\\nu_1 (F-1)}{\\nu_1(F-1)+n}.\\]\nSince \\(F\\) statistic approximately 1 average, measure removes mode. \\(\\widehat{\\omega}^2\\) \\(\\widehat{\\epsilon}^2\\) reported less biased thus preferable estimators true proportion variance (Lakens 2013).","code":""},{"path":"effect-sizes-and-power.html","id":"partial-effects-and-variance-decomposition","chapter":"7 Effect sizes and power","heading":"7.1.3 Partial effects and variance decomposition","text":"multiway design several factors, may want estimate effect separate factors interactions. cases, can break variability explained manipulations per effect. effect size models build comparing variance explained effect \\(\\sigma^2_{\\text{effect}}\\).example, say completely randomized balanced design two factors \\(\\), \\(B\\) interaction \\(AB\\). can decompose total variance \n\\[\\sigma^2_{\\text{total}} = \\sigma^2_A + \\sigma^2_B + \\sigma^2_{AB} + \\sigma^2_{\\text{resid}}.\\]\ndesign balanced, variance terms can estimated using mean squared error analysis variance table output. design unbalanced, sum square decomposition unique get different estimates using Type II Type III sum squares.can get formula similar one-sample case now termed partial effect sizes, e.g.,\n\\[\\widehat{\\omega}^2_{\\langle \\text{effect} \\rangle} = \\frac{\\text{df}_{\\text{effect}}(F_{\\text{effect}}-1)}{\\text{df}_{\\text{effect}}(F_{\\text{effect}}-1) + n},\\]\n\\(n\\) overall sample size \\(F_\\text{effect}\\) corresponding degrees freedom statistic associated main effects \\(\\) \\(B\\), interaction term \\(AB\\). R, effectsize package reports estimates one-sided confidence intervals derived using pivot method (Steiger 2004).43Software typically return estimates effect size alongside designs, small things keep mind. One decomposition variance unique unbalanced data. second , using repeated measures mixed models, notation used denote different quantities.Lastly, customary report effect sizes include variability blocking factors random effects, leading -called generalized effect sizes. Include variance blocking factors interactions (effect!) denominator.44For example, \\(\\) experimental factor whose main effect interest, \\(B\\) blocking factor \\(C\\) another experimental factor, use\n\\[\\eta_{\\langle \\rangle}^2 = \\frac{\\sigma^2_A}{\\sigma^2_A + \\sigma^2_B + \\sigma^2_{AB} + \\sigma^2_{\\text{resid}}}.\\]\ngeneralized partial effect. R, effect sizes variance proportion generalized argument vector names blocking factor can passed. reason including blocking factors random effects necessarily available replication.\ncorrect effect size measure calculate report depends design, numerous estimators can utilized. Since related one another, oftentimes possible compute directly output convert. formula highlight importance reporting (enough precision) exactly values test statistic.","code":""},{"path":"effect-sizes-and-power.html","id":"power-1","chapter":"7 Effect sizes and power","heading":"7.2 Power","text":"power probability correctly rejecting null hypothesis isn’t true. However, whereas null alternative corresponds single value (equality mean), infinitely many alternatives…intricate relation effect size, power sample size. Journals grant agencies oftentimes require estimate latter funding study, one needs ensure sample size large enough pick-effects scientific interest (good signal--noise), also overly large minimize time money make efficient allocation resources. Goldilock’s principle, never hurts.run pilot study estimate background level noise estimated effect, wish perform replication study, come similar question cases: many participants needed reliably detect difference? Setting minimum value power (least 80%, typically 90% 95% feasible) ensures study reliable ensures high chance success finding effect least size specified. power 80% ensures , average, 4 5 experiments study phenomenon specified non-null effect size lead rejecting null hypothesis.order better understand interplay power, effect size sample size, consider theoretical example. purpose displaying formula (hopefully) transparently confirm intuitions leads higher power. many things can influence power:experimental design: blocking design repeated measures tend filter unwanted variability population, thus increasing power relative completely randomized designthe background variability \\(\\sigma\\):noise level oftentimes intrinsic measurement. depends phenomenon study, instrumentation choice scale, etc. can impact. Running experiments controlled environment helps reduce , researchers typically limited control variability inherent observation.sample size: data gathered, information accumulates. precision measurements (e.g., differences mean) normally determined group smallest sample size, (approximate) balancing increases power variance group .size effect: bigger effect, easier accurately detect (’s easier spot elephant mouse hiding classroom).level test, \\(\\alpha\\): increase rejection region, technically increase power run experiment alternative regime. However, level oftentimes prespecified avoid type errors.\nmay consider multiplicity correction within power function, Bonferonni’s method, equivalent reducing \\(\\alpha\\).","code":""},{"path":"effect-sizes-and-power.html","id":"power-oneway","chapter":"7 Effect sizes and power","heading":"7.2.1 Power for one-way ANOVA","text":"fix ideas, consider one-way analysis variance model. usual setup, consider \\(K\\) experimental conditions \\(n_k\\) observations group \\(k\\), whose population average denote \\(\\mu_k\\). can parametrize model terms overall sample average,\n\\[\\begin{align*}\n\\mu = \\frac{1}{n}\\sum_{j=1}^K\\sum_{=1}^{n_j} \\mu_j = \\frac{1}{n}\\sum_{j=1}^K n_j \\mu_j,\n\\end{align*}\\]\n\\(n=n_1 + \\cdots +n_K\\) total sample size.\n\\(F\\)-statistic one-way ANOVA \n\\[\\begin{align*}\nF =  \\frac{\\text{sum squares}/(K-1)}{\\text{within sum squares}/(n-K)}\n\\end{align*}\\]\nnull distribution \\(F(K-1, n-K)\\). interest understanding F-statistic behaves alternative.construction, stressed denominator estimator \\(\\sigma^2\\) null alternative. happens numerator? can write population average \n\\[\n\\mathsf{E}(\\text{sum squares}) = \\sigma^2\\{(K-1) + \\Delta\\}.\n\\]\n\n\\[\n\\Delta = \\dfrac{\\sum_{j=1}^K n_j(\\mu_j - \\mu)^2}{\\sigma^2} = nf^2.\n\\]\n\\(f^2\\) square Cohen’s \\(f\\). null hypothesis, \\(\\mu_j=\\mu\\) \\(j=1, \\ldots, K\\) \\(\\Delta=0\\), groups different average displacement non-zero. greater \\(\\Delta\\), mode (peak distribution) unity greater power.Closer examination reveals \\(\\Delta\\) increases \\(n_j\\) (sample size) true squared mean difference \\((\\mu_j-\\mu)^2\\) increases effect size represented difference mean, decreases observation variance increases.alternative, distribution \\(F\\) statistic noncentral Fisher distribution, denoted \\(\\mathsf{F}(\\nu_1, \\nu_2, \\Delta)\\) degrees freedom \\(\\nu_1\\) \\(\\nu_2\\) noncentrality parameter \\(\\Delta\\).45 calculate power test, need single specific alternative hypothesis.\nFigure 7.2: Density curves null distribution (full line) true distribution (dashed line) noncentrality parameter \\(\\Delta=3\\). area white curve denotes power alternative.\nplot Figure 7.2 shows null (full line) distribution true distribution (dashed line) particular alternative. noncentral \\(\\mathsf{F}\\) shifted right right skewed, mode (peak) away 1.Given value \\(\\Delta=nf^2\\) information effect interest (degrees freedom effect residuals), can compute tail probability followsCompute cutoff point: value \\(\\mathscr{H}_0\\) leads rejection level \\(\\alpha\\)Compute probability alternative curve, cutoff onwards.practice, software return quantities inform us power. Note results trustworthy provided model assumptions met, otherwise may misleading.difficult question trying estimate sample size study determining value use effect size. One opt value reported elsewhere similar scale estimate variability provide educated guesses mean differences. Another option run pilot study use resulting estimates inform sensible values, perhaps using confidence intervals see range plausible effect sizes.Reliance estimated effect sizes reported literature debatable: many effects inflated result file-drawer problem , , can lead unreasonably high expectations power.WebPower package R offers comprehensive solution conducting power studies, free software G*Power.","code":"\ncutoff <- qf(p = 1-alpha, df1 = df1, df2 = df2)\npf(q = cutoff,  df1 = df1, df2 = df2, \n    ncp = Delta, lower.tail = FALSE)"},{"path":"effect-sizes-and-power.html","id":"power-in-complex-designs","chapter":"7 Effect sizes and power","heading":"7.2.2 Power in complex designs","text":"cases analytic derivations isn’t possible, can resort simulations approximate power. given alternative, wesimulate repeatedly samples model hypothetical alternative worldwe compute test statistic new sampleswe transform associated p-values based postulated null hypothesis.end, calculate proportion tests lead rejection null hypothesis level \\(\\alpha\\), namely percentage p-values smaller \\(\\alpha\\). can vary sample size see many observations need per group achieve desired level power.Summary:Effect sizes used provide standardized measure strength result, independent design sample size.two classes: standardized differences proportions variance.Multiple estimators exists: report latter along software used compute confidence intervals.adequate measure variability use effect size depends design: normally include variability blocking factors residual variance.Given design, can deduce either sample size, power effect size two metrics. allows us compute sample size study replication.","code":""},{"path":"replication-crisis.html","id":"replication-crisis","chapter":"8 Replication crisis","heading":"8 Replication crisis","text":"recent years, many team efforts performed -called replications existing methodological papers assess robustness findings. Perhaps unsurprisingly, many replications failed yield anything like authors used claim, found much weaker findings. chapter examines causes lack replicability.Defining replicability reproducibility.Understanding scale replication crisis.Recognizing common statistical fallacies.Listing strategies enhancing reproducibility.adopt terminology Claerbout Karrenbach (1992): study said reproducible external person data enough indications procedure (example, code software versions, etc.) can obtain consistent results match paper. related scientific matter replicability, process new data collected test hypothesis, potentially using different methodology. Reproducibility important enhances credibility one’s work. Extensions deal different analyses leading conclusion described Turing Way presented 8.1.\nFigure 8.1: Definition different dimensions reproducible research (Turing Way project, illustration Scriberia).\nreproducibility replicability important? thought provoking paper, Ioannidis (2005) claimed research findings wrong. abstract paper statedThere increasing concern current published research findings false. […] framework, research finding less likely true studies conducted field smaller; effect sizes smaller; greater number lesser preselection tested relationships; greater flexibility designs, definitions, outcomes, analytical modes; greater financial interest prejudice; teams involved scientific field chase statistical significance.Since publication, collaborative efforts tried assess scale problem reanalysing data trying replicate findings published research. example, “Reproducibility [sic] Project: Psychology” (Nosek et al. 2015)conducted replications 100 experimental correlational studies published three psychology journals using high powered designs original materials available. Replication effects half magnitude original effects, representing substantial decline. Ninety seven percent original studies significant results. Thirty six percent replications significant results; 47% original effect sizes 95% confidence interval replication effect size; 39% effects subjectively rated replicated original result; , bias original results assumed, combining original replication results left 68% significant effects. […]large share findings review replicable effects much smaller claimed, shown Figure 2 study.\nfindings show peer-review procedure foolproof: “publish--perish” mindset academia leading many researchers try achieve statistical significance costs meet 5% level criterion, whether involuntarily . problem many names: \\(p\\)-hacking, harking paraphrase story Jorge Luis Borges, garden forking paths. many degrees freedom analysis researchers refine hypothesis viewing data, conducting many unplanned comparisons reporting selected results.\nFigure 8.2: Figure 2 Nosek et al. (2015), showing scatterplot effect sizes original replication study power, rugs density plots significance 5% level.\nAnother problem selective reporting. large emphasis placed statistical significance, many studies find small effects never published, resulting gap. Figure 8.3 Zwet Cator (2021) shows \\(z\\)-scores obtained transforming confidence intervals reported Barnett Wren (2019). authors used data mining techniques extract confidence intervals abstracts nearly one million publication Medline published 1976 2019. experiments yielded effect due natural variability, \\(z\\)-scores normally distributed, Figure 8.3 shows big gap bell curve approximately \\(-2\\) \\(2\\), indicative selective reporting. fact results lead \\(p < 0.05\\) published called file-drawer problem.\nFigure 8.3: Figure Zwet Cator (2021) based results Barnett Wren (2019); histogram \\(z\\)-scores one million studies Medline.\nongoing debate surrounding reproducibility crisis sparked dramatic changes academic landscape: enhance quality studies published, many journal now require authors provide code data, pre-register studies, etc. Teams lead effort (e.g., Experimental Economics Replication Project) try replicate studies, mitigated success far. inside recollection graduate student shows extent problem.course place strong emphasis identifying avoiding statistical fallacies showcasing methods enhance reproducibility. can reproducible research enhance work? one thing, workflow facilitates publication negative research, forces researchers think ahead time (receive feedback). Reproducible research data availability also leads additional citations increased credibility scientist.Among good practices arepre-registration experiments use logbook.clear reporting key aspects experiment (choice metric, number items Likert scale, etc.)version control systems (e.g., Git) track changes files records.archival raw data proper format accompanying documentation.Keeping logbook documenting progress helps collaborators, reviewers future-self understand decisions may seem unclear arbitrary future, even result careful thought process time made . Given pervasiveness garden forking paths, pre-registration helps prevents harking limits selective reporting unplanned tests, panacea. Critics often object pre-registration claiming binds people. misleading claim view: pre-registration doesn’t mean must stick plan exactly, merely requires explain go planned anything.Version control keeps records changes file can help retrieve former versions make mistakes point.\nFigure 8.4: Tweet showing widespread problems related unintentional changes raw data software.\nArchival data helps avoid unintentional irreversible manipulations original data, examples can large scale consequences illustrated Figure 8.4 (Ziemann El-Osta 2016), report flaws genetic journals due automatic conversion gene names dates Excel. problems far unique. sensitive data shared “” confidentiality issues, many instances data can made available licence DOI allow people reuse , cite credit work.enforce reproducibility, many journals now policy regarding data, material code availability. journals encourage , trend recent years enforce. example, Nature require following reported published papers:\nFigure 8.5: Screenshot Nature Reporting summary statistics, reproduced CC 4.0 license.\n","code":""},{"path":"replication-crisis.html","id":"causes-of-the-replication-crisis","chapter":"8 Replication crisis","heading":"8.1 Causes of the replication crisis","text":"multiple (non-exclusive) explanations lack replication study findings.","code":""},{"path":"replication-crisis.html","id":"the-garden-of-forking-paths","chapter":"8 Replication crisis","heading":"8.1.1 The garden of forking paths","text":"garden forking paths, named novel Borges, term coined Andrew Gelman refer researchers’ degrees freedom. vague hypothesis data collection rules, easy researcher adapt interpret conclusions way fits chosen narratives. words Gelman Loken (2014)Given particular data set, can seem entirely appropriate look data construct reasonable rules data exclusion, coding, analysis can lead statistical significance. case, researchers need perform one test, test conditional data.user case accomodated classical testing theory. Research hypothesis often formulated vague way, different analysis methods, tests may compatible. Abel et al. (2022) recent preprint found preregistration alone solve problem, publication bias randomized control trial alleviated publication pre-analysis plans. directly related garden forking path.","code":""},{"path":"replication-crisis.html","id":"selective-reporting","chapter":"8 Replication crisis","heading":"8.1.2 Selective reporting","text":"Also known file-drawer problem, selective reporting occurs publication results fail reach statistical significance (sic) harder publish. much way multiple testing, 20 researchers perform study one writes paper result fluke, indicates. widespread indications publication bias, evidence distribution \\(p\\)-values reported papers. recent preprint study found prevalance higher online experiments Amazon MTurks.P-hacking replication crisis lead many leading statisticians advocate much stringent cutoff criterion \\(p < 0.001\\) instead usual \\(p<0.05\\) criterion level test.\nlevel \\(\\alpha=5\\)% essentially arbitrary dates back Fisher (1926), wroteIf one twenty seem high enough odds, may, prefer , draw line one fifty one hundred. Personally, writer prefers set low standard significance 5 per cent point, ignore entirely results fails reach level.Methods pool together results, meta-analysis, sensitive selective reporting. matter?","code":""},{"path":"replication-crisis.html","id":"non-representative-samples","chapter":"8 Replication crisis","heading":"8.1.3 Non-representative samples","text":"Many researchers opt convenience samples using online panels Qualtrics, Amazon MTurks, etc. quality observations best dubious: ask whether answer survey small amount. Manipulation checks ensure participants following, information completed bots, threshold minimal time required complete study, etc. necessary (sufficient) conditions ensure data rubbish.important criticism people answer surveys representative population whole: sampling bias thus plays important role conclusions , even summary statistics different general population, may exhibit different opinions, levels skills, etc. .\nFigure 8.6: Sampling bias. Artwork Jonathan Hey (Sketchplanations) shared CC -NC 4.0 license.\ncan said panels students recruited universities classes, young, educated perhaps may infer backward induction purpose study answer accordingly.","code":""},{"path":"replication-crisis.html","id":"summary","chapter":"8 Replication crisis","heading":"8.2 Summary","text":"Operating open-science environment seen opportunity make better science, offer opportunities increase impact increase likelihood work gets published regardless whether results turn negative. right thing increases quality research produced, collateral benefits forces researchers validate methodology , double-check data analysis adopt good practice.many platforms preregistering studies sharing preanalysis plans, scripts data, different level formality. One Research Box.Reflect workflow applied researcher designing undertaking experiments. practical aspects improve upon improve reproducibility study?","code":""},{"path":"repeated-measures-and-multivariate-models.html","id":"repeated-measures-and-multivariate-models","chapter":"9 Repeated measures and multivariate models","heading":"9 Repeated measures and multivariate models","text":"far, experiments considered can classified -subject designs, meaning experimental unit assigned single experimental (sub)-condition. many instances, may possible randomly assign multiple conditions experimental unit. example, individual coming lab perform tasks virtual reality environment may assigned treatments, latter presented random order avoid confounding. obvious benefit , participants can act control group, leading greater comparability among treatment conditions.example, consider study performed Tech3Lab looks reaction time people texting talking cellphone walking. may wish determine whether disengagement slower people texting, yet may also postulate elderly people slower reflexes.-subjects design, subjects nested within experimental condition, subject can assigned single treatment. within-subjects designs, experimental factors subjects crossed: possible observed combination subject experimental conditions.including multiple conditions, can filter effect due subject, much like blocking: leads increased precision effect sizes increased power (see, hypothesis tests based within-subject variability). Together, translates need gather fewer observations participants detect given effect population thus experiments cheaper run.course drawbacks gathering repeated measures individuals. subjects confronted multiple tasks, may carryover effects (one task influences response subsequent ones, example becoming fluent manipulations go ), period effects (practice fatigue, e.g., leading decrease acuity), permanent changes subject condition treatment attrition (loss subjects time).minimize potential biases, multiple strategies one can use. can randomize order treatment conditions among subjects reduce confounding, use balanced crossover design include period carryover effect statistical model via control variables better isolate treatment effect. experimenter also allow enough time treatment conditions reduce eliminate period carryover effects plan tasks accordingly.multiple approaches handling repeated measures. first option take averages experimental condition per subject treat additional blocking factors, may necessary adjust resulting statistics. second approach consists fitting multivariate model response explicitly account correlation. Multivariate analysis variance (MANOVA) leads procedures analogous univariate analysis variance, now need estimate correlation variance parameters measurement separately multiple potential statistics can defined testing effects. can benefit correlation find differences wouldn’t detected univariate models, additional parameters estimate lead loss power. Finally, popular method nowadays handling repeated measures fit mixed model, random effects accounting subject-specific characteristics. , assume levels factor (subject identifiers) form random sample large population. models can difficult fit one needs take great care specifying model.","code":""},{"path":"repeated-measures-and-multivariate-models.html","id":"repeated-measures","chapter":"9 Repeated measures and multivariate models","heading":"9.1 Repeated measures","text":"introduce concept repeated measure within-subject ANOVA example.Example 9.1  (Happy fakes) consider experiment conducted graduate course HEC, Information Technologies Neuroscience, PhD students gathered electroencephalography (EEG) data. project focused human perception deepfake image created generative adversarial network: Amirabdolahian Ali-Adeeb (2021) expected attitude towards real computer generated image people smiling change.response variable amplitude brain signal measured 170 ms participant exposed different faces. Repeated measures collected 9 participants given database AA21, expected look 120 faces. participants completed full trial, can checked looking cross-tabs countsThe experimental manipulation encoded stimuli, levels control (real) real facial images, whereas others generated using generative adversarial network (GAN) slightly smiling (GAN1) extremely smiling (GAN2); latter looks fake. presentation order randomized, order presentation faces within type recorded using epoch variable: allows us measure fatigue effect.Since research question whether images generated generative adversarial networks trigger different reactions, looking pairwise differences control.\nFigure 9.1: Example faces presented Amirabdolahian Ali-Adeeb (2021): real, slightly modified extremely modified (left right).\nbegin grouping data computing average experimental condition stimulus per participant set id blocking factor. analysis variance table obtained aov correct, fails account correlation.one-way analysis variance \\(n_s\\) subjects, exposed \\(n_a\\) experimental conditions, can written\n\\[\\begin{align*}\\underset{\\text{response}\\vphantom{l}}{Y_{ij}} = \\underset{\\text{global mean}}{\\mu_{\\vphantom{j}}} + \\underset{\\text{mean difference}}{\\alpha_j} + \\underset{\\text{subject difference}}{s_{\\vphantom{j}}} + \\underset{\\text{error}\\vphantom{l}}{\\varepsilon_{ij}}\\end{align*}\\]Since design balanced averaging, can use aov R: need specify subject identifier within Error term. approach drawback, variance components can negative variability due subject negligible. aov fast, works simple balanced designs.","code":"#>         id\n#> stimulus  1  2  3  4  5  6  7  8  9 10 11 12\n#>     real 30 32 34 32 38 29 36 36 40 30 39 33\n#>     GAN1 32 31 40 33 38 29 39 31 39 28 35 34\n#>     GAN2 31 33 37 34 38 29 34 36 40 33 35 32\n# Compute mean for each subject + \n# experimental condition subgroup\nAA21_m <- AA21 |>\n  dplyr::group_by(id, stimulus) |>\n  dplyr::summarize(latency = mean(latency))\n# Use aov for balanced sample\nfixedmod <- aov(\n  latency ~ stimulus + Error(id/stimulus), \n  data = AA21_m)\n# Print ANOVA table\nsummary(fixedmod)\n#> \n#> Error: id\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> Residuals 11    188    17.1               \n#> \n#> Error: id:stimulus\n#>           Df Sum Sq Mean Sq F value Pr(>F)\n#> stimulus   2    1.9    0.97     0.5   0.62\n#> Residuals 22   43.0    1.96"},{"path":"repeated-measures-and-multivariate-models.html","id":"contrasts-1","chapter":"9 Repeated measures and multivariate models","heading":"9.1.1 Contrasts","text":"balanced data, estimated marginal means coincide row averages. single replication average subject/condition, create new column contrast fit model intercept-(global mean) check whether latter zero. 12 participants, thus expect test statistic 11 degrees freedom, since one unit spent estimating mean parameter 12 participants.Unfortunately, emmeans package analysis object fitted using aov incorrect: can seen passing contrast vector inspecting degrees freedom. afex package includes functionalities tailored within-subject -subjects interface emmeans.afex package different functions computing within-subjects design aov_ez specification, allow people list within -subjects factor separately subject identifiers may easier understand. also argument, fun_aggregate, automatically average replications.","code":"#> $emmeans\n#>  stimulus emmean    SE df lower.CL upper.CL\n#>  real      -10.8 0.942 11    -12.8    -8.70\n#>  GAN1      -10.8 0.651 11    -12.3    -9.40\n#>  GAN2      -10.3 0.662 11    -11.8    -8.85\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast    estimate    SE df t.ratio p.value\n#>  real vs GAN   -0.202 0.552 11  -0.366  0.7210\n#> $emmeans\n#>  stimulus emmean    SE   df lower.CL upper.CL\n#>  real      -10.8 0.763 16.2    -12.4    -9.15\n#>  GAN1      -10.8 0.763 16.2    -12.4    -9.21\n#>  GAN2      -10.3 0.763 16.2    -11.9    -8.69\n#> \n#> Warning: EMMs are biased unless design is perfectly balanced \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>  contrast    estimate    SE df t.ratio p.value\n#>  real vs GAN   -0.202 0.494 22  -0.409  0.6870"},{"path":"repeated-measures-and-multivariate-models.html","id":"sphericity-assumption","chapter":"9 Repeated measures and multivariate models","heading":"9.1.2 Sphericity assumption","text":"validity \\(F\\) statistic null distribution relies model correct structure.repeated-measure analysis variance, assume variance variance. equally require correlation measurements subject , assumption corresponds -called compound symmetry model.46What measurements unequal variance different correlations? fit fully multivariate model accounts \nautomatic two measurements (single correlation), can check comparing fit model unstructured covariance (difference variances correlations pair variable)Since care differences treatment, can get away weaker assumption compound symmetry (equicorrelation) relying instead sphericity, holds variance difference treatment constant.popular approach handling correlation tests two-stage approach: first, check sphericity (using, e.g., Mauchly’s test sphericity). null hypothesis sphericity rejected, one can use correction \\(F\\) statistic modifying parameters Fisher \\(\\mathsf{F}\\) null distribution used benchmark.idea due Box correct degrees freedom \\(\\mathsf{F}(\\nu_1, \\nu_2)\\) distribution multiplying common factor \\(\\epsilon<1\\) use \\(\\mathsf{F}(\\epsilon\\nu_1, \\epsilon\\nu_2)\\). Since \\(F\\) statistic ratio variances, \\(\\epsilon\\) term cancel. Using scaled \\(\\mathsf{F}\\) distribution leads larger \\(p\\)-values, thus accounting correlation.three widely used corrections: Greenhouse–Geisser, Huynh–Feldt Box correction, divides \\(\\nu_1\\) degrees freedom gives conservative option. Huynh–Feldt method reported powerful preferred, estimated value \\(\\epsilon\\) can larger 1.Using afex functions, get result Mauchly’s test sphericity \\(p\\) values using either correction methodExample 9.2  (Visual acuity) consider model within-subject -subject factors. Data study visual acuity participants. data represent number words correctly detected different font size; interest effect illusory contraction detection. mixed analysis variance includes experimental factors adaptation (2 levels, within), fontsize (4 levels, within), position (5 levels, within) visual acuity (2 levels, within). total 1760 measurements 44 participants LBJ17_S1A, balanced.\nwithin-subject factors give total 40 measurements (\\(2 \\times 4 \\times 5\\)) per participant; factors crossed can estimate interactions . subjects nested within visual acuity groups, participants dichotomized two groups based visual acuity, obtained preliminary checks, using median split.fit model, rely aov_ez function afex. default, latter includes interactions.complicated model tested far: four experimental factor manipulated , interactions order two, three four included!fourth order interaction isn’t statistically significant: means can legitimately marginalize look four three-way ANOVA designs turn. can also see third order interaction adaptation:fontsize:position acuity:adaptation:position really meaningful.following paragraph technical can skipped. One difficult bit designs including within-subject -subject factors degrees freedom correct sum square terms use calculate \\(F\\) statistics hypothesis interest. correct setup use next sum square (associated degrees freedom) . main effect interaction, count number instances particular (e.g., 10 interaction position adaptation). subtract number mean parameter used estimate means differences mean (1 global mean, 4 means position, 1 adaptation), gives \\(4=10-6\\) degrees freedom. Next, term compared mean square contains subject (via acuity levels, since subjects nested within acuity) corresponding variables; correct mean square acuity:adaptation:position. balanced design setting, can formalized using Hasse diagram (Oehlert 2000).can produce interaction plot see comes : since can’t draw four dimensions, map visual acuity adaptation level panels use different colours position. figure looks different altogether paper \\(y\\) axis flipped.","code":"#> \n#> Univariate Type III Repeated-Measures ANOVA Assuming Sphericity\n#> \n#>             Sum Sq num Df Error SS den Df F value  Pr(>F)    \n#> (Intercept)   4073      1      188     11   238.6 8.4e-09 ***\n#> stimulus         2      2       43     22     0.5    0.62    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> \n#> Mauchly Tests for Sphericity\n#> \n#>          Test statistic p-value\n#> stimulus          0.678   0.143\n#> \n#> \n#> Greenhouse-Geisser and Huynh-Feldt Corrections\n#>  for Departure from Sphericity\n#> \n#>          GG eps Pr(>F[GG])\n#> stimulus  0.757       0.57\n#> \n#>          HF eps Pr(>F[HF])\n#> stimulus  0.851      0.587\nLBJ_mod <- afex::aov_ez(\n  id = \"id\",     # subject id\n  dv = \"nerror\", # response\n  between = \"acuity\",\n  within = c(\"adaptation\",\n             \"fontsize\", \n             \"position\"),\n  data = hecedsm::LBJ17_S1A)\nanova(LBJ_mod,  # model\n      correction = \"none\", # no correction for sphericity\n      es = \"pes\") \n#> Anova Table (Type 3 tests)\n#> \n#> Response: nerror\n#>                                     num Df den Df   MSE       F   pes  Pr(>F)\n#> acuity                                   1     42 1.229   30.77 0.423 1.8e-06\n#> adaptation                               1     42 0.300    7.76 0.156 0.00800\n#> acuity:adaptation                        1     42 0.300   12.73 0.233 0.00091\n#> fontsize                                 3    126 0.756 1705.68 0.976 < 2e-16\n#> acuity:fontsize                          3    126 0.756   10.04 0.193 5.6e-06\n#> position                                 4    168 0.213    9.42 0.183 6.8e-07\n#> acuity:position                          4    168 0.213    4.17 0.090 0.00303\n#> adaptation:fontsize                      3    126 0.230    3.29 0.073 0.02296\n#> acuity:adaptation:fontsize               3    126 0.230    6.98 0.142 0.00022\n#> adaptation:position                      4    168 0.138    0.60 0.014 0.66209\n#> acuity:adaptation:position               4    168 0.138    0.90 0.021 0.46406\n#> fontsize:position                       12    504 0.206    9.05 0.177 7.5e-16\n#> acuity:fontsize:position                12    504 0.206    2.70 0.060 0.00155\n#> adaptation:fontsize:position            12    504 0.134    0.51 0.012 0.90739\n#> acuity:adaptation:fontsize:position     12    504 0.134    1.18 0.027 0.29550\n#>                                        \n#> acuity                              ***\n#> adaptation                          ** \n#> acuity:adaptation                   ***\n#> fontsize                            ***\n#> acuity:fontsize                     ***\n#> position                            ***\n#> acuity:position                     ** \n#> adaptation:fontsize                 *  \n#> acuity:adaptation:fontsize          ***\n#> adaptation:position                    \n#> acuity:adaptation:position             \n#> fontsize:position                   ***\n#> acuity:fontsize:position            ** \n#> adaptation:fontsize:position           \n#> acuity:adaptation:fontsize:position    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#partial eta-square for effect sizes (es)"},{"path":"repeated-measures-and-multivariate-models.html","id":"multivariate-analysis-of-variance","chapter":"9 Repeated measures and multivariate models","heading":"9.2 Multivariate analysis of variance","text":"second paradigm modelling specify response subject fact multivariate object: can combine measurements given individual vector \\(\\boldsymbol{Y}\\). example happy fakes, tuple measurements (real, GAN1, GAN2).multivariate analysis variance model designed assuming observations follow (multivariate) normal distribution mean vector \\(\\boldsymbol{\\mu}_j\\) group \\(j\\) common covariance matrix \\(\\boldsymbol{\\Sigma}\\) comparing means groups. univariate analysis variance, multivariate normal assumption holds approximately virtue central limit theorem large samples, convergence slower larger numbers needed ensure valid.difference univariate approach now compare global mean vector \\(\\boldsymbol{\\mu}\\) comparisons. one-way analysis variance model experimental factor \\(K\\) levels balanced sample \\(n_g\\) observations per group \\(n=n_gK\\) total observations, assume group average \\(\\boldsymbol{\\mu}_k\\) \\((k=1, \\ldots, K)\\), can estimate using observations group. null hypothesis, groups mean, estimator overall mean \\(\\boldsymbol{\\mu}\\) combining \\(n\\) observations.statistic obtained decomposing total variance around global mean components due different factors leftover variability. equivalent sum square decomposition results multiple matrices, multiple ways constructing test statistics. Wilk’s \\(\\Lambda\\) popular choice. Another common choice, leads statistic giving lower power robust departure model assumptions Pillai’s trace.MANOVA model assumes covariance matrices within experimental condition. normality assumption, can use Box’s \\(M\\) statistic test hypothesis.","code":""},{"path":"repeated-measures-and-multivariate-models.html","id":"data-format","chapter":"9 Repeated measures and multivariate models","heading":"9.2.1 Data format","text":"repeated measures, sometimes convenient store measurements associated experimental condition different columns data frame spreadsheet, lines containing participants identifiers. data said wide format, since multiple measurements row. format suitable multivariate models, many statistical routines instead expect data long format, single measurement per line. Figure 9.2 illustrates difference two formats.\nFigure 9.2: Long versus wide-format data tables (illustration Garrick Aden-Buie).\nIdeally, data base long format repeated measures also include column giving order treatments assigned participants. necessary order test whether fatigue crossover effects, example plotting residuals accounting treatment subject subject, ordered time. also perform formal tests including time trends model checking whether slope significant.Overall, biggest difference within-subject designs observations correlated whereas assumed measurements independent now. needs explicitly accounted , correlation important impact testing discussed Subsection 3.4.4: failing account correlation leads \\(p\\)-values much low. see , think stupid setting duplicate every observation database: estimated marginal means , variance halved despite fact additional information. Intuitively, correlation reduces amount information provided individual: repeated measures participants, expect effective sample size anywhere total number subjects total number observations.","code":""},{"path":"repeated-measures-and-multivariate-models.html","id":"mathematical-complement","chapter":"9 Repeated measures and multivariate models","heading":"9.2.2 Mathematical complement","text":"section technical can omitted. Analogous univariate case, can decompose variance estimator terms within, total variance. Let \\(\\boldsymbol{Y}_{ik}\\) denote response vector \\(\\)th observation group \\(k\\); , can decompose variance \n\\[\\begin{align*} &\n\\underset{\\text{total variance}}{\\sum_{k=1}^K \\sum_{=1}^{n_g} (\\boldsymbol{Y}_{ik} - \\widehat{\\boldsymbol{\\mu}})(\\boldsymbol{Y}_{ik} - \\widehat{\\boldsymbol{\\mu}})^\\top} \\\\\\qquad &= \\underset{\\text{within variance}}{\\sum_{k=1}^K \\sum_{=1}^{n_g} (\\boldsymbol{Y}_{ik} - \\widehat{\\boldsymbol{\\mu}}_k)(\\boldsymbol{Y}_{ik} - \\widehat{\\boldsymbol{\\mu}}_k)^\\top} + \\underset{\\text{variance}}{\\sum_{k=1}^K n_g(\\boldsymbol{\\mu}_{k} - \\widehat{\\boldsymbol{\\mu}})(\\widehat{\\boldsymbol{\\mu}}_k - \\widehat{\\boldsymbol{\\mu}})^\\top}\n\\end{align*}\\]\ndefining covariance matrix estimators. write \\(\\widehat{\\boldsymbol{\\Sigma}}_T\\), \\(\\widehat{\\boldsymbol{\\Sigma}}_W\\), \\(\\widehat{\\boldsymbol{\\Sigma}}_B\\) respectively total, within variance estimators, can build statistic ingredients see much variability induced centering using common vector. \\(K>2\\), multiple statistics constructed, includingWilk’s \\(\\Lambda\\): \\(|\\widehat{\\boldsymbol{\\Sigma}}_W|/|\\widehat{\\boldsymbol{\\Sigma}}_W + \\widehat{\\boldsymbol{\\Sigma}}_B|\\)Roy’s maximum root: largest eigenvalue \\(\\widehat{\\boldsymbol{\\Sigma}}_W^{-1}\\widehat{\\boldsymbol{\\Sigma}}_B\\)Lawley–Hotelling trace: \\(\\mathrm{tr}(\\widehat{\\boldsymbol{\\Sigma}}_W^{-1}\\widehat{\\boldsymbol{\\Sigma}}_B)\\)Pillai’s trace: \\(\\mathrm{tr}\\left\\{\\widehat{\\boldsymbol{\\Sigma}}_B(\\widehat{\\boldsymbol{\\Sigma}}_W +\\widehat{\\boldsymbol{\\Sigma}}_B)^{-1}\\right\\}\\).four criteria lead equivalent statistics \\(p\\)-values \\(K=2\\).two-way balanced MANOVA, can perform simlar decomposition factor interaction, \n\\[\\widehat{\\boldsymbol{\\Sigma}}_T = \\widehat{\\boldsymbol{\\Sigma}}_A + \\widehat{\\boldsymbol{\\Sigma}}_B + \\widehat{\\boldsymbol{\\Sigma}}_{AB} + \\widehat{\\boldsymbol{\\Sigma}}_W.\\]Wilk’s \\(\\Lambda\\) based taking ratio determinant within-variance sum effect-variance plus within-variance, e.g., \\(|\\widehat{\\boldsymbol{\\Sigma}}_{AB} + \\widehat{\\boldsymbol{\\Sigma}}_W|\\) interaction term.","code":""},{"path":"repeated-measures-and-multivariate-models.html","id":"model-fitting","chapter":"9 Repeated measures and multivariate models","heading":"9.2.3 Model fitting","text":"can treat within-subject responses vector observations estimate model using using multivariate linear regression. Contrary univariate counterpart, model explicitly models correlation observations subject.order fit model multivariate response, first need pivot data wider format matrix rows number subjects \\(M\\) columns number response variables.data suitable format, fit multivariate model lm function using sum--zero constraints, imposed globally changing contrasts option. Syntax-wise, difference univariate case response left tilde sign (~) now matrix composed binding together vectors different responses.Example 9.3  (Happy fakes - multivariate) use data Amirabdolahian Ali-Adeeb (2021), time treating averaged repeated measures different stimulus multivariate response. first pivot data wide format, fit multivariate linear model.Since within-subject factor stimulus disappeared consider multivariate response, specify global mean vector \\(\\boldsymbol{\\mu}\\) via ~1. general, add -subject factors right-hand side equation. hypothesis equal mean translates hypothesis \\(\\boldsymbol{\\mu} = \\mu\\boldsymbol{1}_3\\), can imposed using call anova. output returns statistic \\(p\\)-values including corrections sphericity.can also use emmeans set post-hoc contrasts. Since variable, need set specs repeated measure variable appearing left hand side formula; latter labelled rep.meas default.can check output case within-subject analysis variance model fitted previously afex package.Example 9.4  (Teaching read) consider -subject repeated measure multivariate analysis variance model Baumann, Seifert-Kessell, Jones (1992). data balanced experimental condition include results three tests performed intervention: error detection task, expanded comprehension monitoring questionnaire cloze test. Note scale tests different (16, 18 56).obtain estimated covariance matrix fitted model extracting residuals \\(Y_{ik} - \\widehat{\\mu}_k\\) computing empirical covariance. results shows strong dependence tests 1 3 (correlation 0.39), much weaker dependence test2.Let us compute multivariate analysis variance modelBy default, get Pillai’s trace statistic. , clear evidence differences groups observations regardless statistic used.can compute effect size passing table, example using eta_squared(mtest) get effect size multivariate test, simple model get individual variable effect sizes.found difference, one principle investigate component response performing univariate analysis variance accounting multiple testing using, e.g., Bonferroni’s correction. fruitful avenue trying discriminate use descriptive discriminant analysis follow-, computes best fitting hyperplanes separate groups.amounts compute weights \\(\\boldsymbol{w}\\) , , computing \\(\\boldsymbol{w}^\\top\\boldsymbol{Y}\\) creating composite score adding weighted components leads maximal separation groups. Figure 9.3 shows new coordinates.\nFigure 9.3: Scatterplot observations projected onto linear discriminants post-experiment tests, group.\nLinear discriminant analysis topic ’s beyond scope course.","code":"#> Analysis of Variance Table\n#> \n#> \n#> Contrasts orthogonal to\n#> ~1\n#> \n#> Greenhouse-Geisser epsilon: 0.7565\n#> Huynh-Feldt epsilon:        0.8515\n#> \n#>             Df   F num Df den Df Pr(>F) G-G Pr H-F Pr\n#> (Intercept)  1 0.5      2     22  0.615  0.567  0.587\n#> Residuals   11\n#>  contrast         estimate    SE df t.ratio p.value\n#>  c(1, -0.5, -0.5)   -0.202 0.552 11  -0.366  0.7210\ndata(BSJ92, package = \"hecedsm\")\n# Force sum-to-zero parametrization\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n# Fit MANOVA model\nmmod <- lm(\n  cbind(posttest1, posttest2, posttest3) ~ group,\n   data = BSJ92)\n# Calculate multivariate test\nmtest <- car::Anova(mmod, test = \"Wilks\")\n# mtest\n# Get all statistics and univariate tests\nsummary(car::Anova(mmod), univariate = TRUE)\n#> \n#> Type II MANOVA Tests:\n#> \n#> Sum of squares and products for error:\n#>           posttest1 posttest2 posttest3\n#> posttest1     640.5      30.8       498\n#> posttest2      30.8     356.4      -104\n#> posttest3     498.3    -104.4      2512\n#> \n#> ------------------------------------------\n#>  \n#> Term: group \n#> \n#> Sum of squares and products for the hypothesis:\n#>           posttest1 posttest2 posttest3\n#> posttest1    108.12      6.67     190.6\n#> posttest2      6.67     95.12      56.7\n#> posttest3    190.61     56.65     357.3\n#> \n#> Multivariate Tests: group\n#>                  Df test stat approx F num Df den Df   Pr(>F)    \n#> Pillai            2     0.408     5.30      6    124 0.000068 ***\n#> Wilks             2     0.632     5.24      6    122 0.000078 ***\n#> Hotelling-Lawley  2     0.519     5.19      6    120 0.000089 ***\n#> Roy               2     0.318     6.58      3     62  0.00062 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>  Type II Sums of Squares\n#>           df posttest1 posttest2 posttest3\n#> group      2       108      95.1       357\n#> residuals 63       641     356.4      2512\n#> \n#>  F-tests\n#>       posttest1 posttest2 posttest3\n#> group      5.32      8.41      4.48\n#> \n#>  p-values\n#>       posttest1 posttest2 posttest3\n#> group 0.007     0.0006    0.015\nMASS::lda(group ~ posttest1 + posttest2 + posttest3,\n          data = BSJ92)"},{"path":"repeated-measures-and-multivariate-models.html","id":"model-assumptions-1","chapter":"9 Repeated measures and multivariate models","heading":"9.2.4 Model assumptions","text":"addition usual model assumptions (independence measurements different subjects, equal variance, additivity, etc.), MANOVA model adds two hypothesis altogether determine reliable \\(p\\)-values conclusions .first assumption multivariate normality response. central limit theorem can applied multivariate response, sample size needed overall reliably estimate correlation variance larger univariate setting. hypothesis can tested using Shapiro-Wilk normality test (null hypothesis normality) passing residuals multivariate model. test can lead rejection null hypothesis specific variables far normal, dependence structure isn’t one exhibited multivariate normal model. decent sample sizes (say \\(n=50\\) per group), assumption isn’t important others.second assumption covariance matrix individuals, regardless experimental group assignment. try checking whether covariance model group: multivariate normal assumption, leads test statistic called Box’s \\(M\\) test. Unfortunately, test quite sensitive departures multivariate normal assumption , \\(p\\)-value small, may normality heterogeneity.example, limited evidence model assumptions. course also check assumptions analysis variance model postest1, posttest2 posttest3 turn; check left exercice reader.","code":"#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  Z\n#> W = 1, p-value = 0.06#> \n#>  Box's M-test for Homogeneity of Covariance Matrices\n#> \n#> data:  cbind(posttest1, posttest2, posttest3)\n#> Chi-Sq (approx.) = 15, df = 12, p-value = 0.2"},{"path":"repeated-measures-and-multivariate-models.html","id":"power-and-effect-size","chapter":"9 Repeated measures and multivariate models","heading":"9.2.5 Power and effect size","text":"Since multivariate statistics can transformed comparison univariate \\(\\mathsf{F}\\) distribution, can estimate partial effect size . package effectsize offers measure partial \\(\\widehat{\\eta}^2\\) multivariate tests.47Power calculations beyond reach ordinary software one needs specify variance observation, correlation mean. Simulation obvious way kind design obtain answers, free G\\({}^{*}\\)Power software (Faul et al. 2007) also offers tools. See also Läuter (1978) pairwise comparisons.","code":""},{"path":"introduction-to-mixed-models.html","id":"introduction-to-mixed-models","chapter":"10 Introduction to mixed models","heading":"10 Introduction to mixed models","text":"chapter considers tools models repeated measures modern perspective, using random effects modelling. class model, called hierarchical models, multilevel models mixed models simple scenarios, give us flexibility account complex scenarios may different sources variability.example, consider large-scale replication study teaching methods. may multiple labs partaking research program unique characteristics. , can expect measurements collected within lab correlated. time, can repeated mesures participants study. One can view setup hierarchy, within-subject factor within subject within lab. settings, old-school approach analysis variance becomes difficult, impossible; doesn’t easily account heterogeneity lab sample size let us estimate variability within labs.begin journey setup repeated measures ANOVA considering one-way within-subject ANOVA model. assign participant (subject) study experimental treatments, random order. one experimental factor \\(\\) \\(n_a\\) levels, model \n\\[\\begin{align*}\\underset{\\text{response}\\vphantom{l}}{Y_{ij}} = \\underset{\\text{global mean}}{\\mu_{\\vphantom{j}}} + \\underset{\\text{mean difference}}{\\alpha_j} + \\underset{\\text{random effect subject}}{S_{\\vphantom{j}}} + \\underset{\\text{error}\\vphantom{l}}{\\varepsilon_{ij}}.\n\\end{align*}\\]\nrandom effect model, assume subject effect \\(S_i\\) random variable; take \\(S_i \\sim \\mathsf{}(0, \\sigma^2_s)\\) latter assumed independent noise \\(\\varepsilon_{ij} \\sim \\mathsf{}(0, \\sigma^2_e)\\). model parameters need estimate global mean \\(\\mu\\), mean differences \\(\\alpha_1, \\ldots, \\alpha_{n_a}\\), subject-specific variability \\(\\sigma^2_s\\) residual variability \\(\\sigma^2_e\\), sum--zero constraint \\(\\alpha_1 + \\cdots + \\alpha_{n_a}=0\\).Inclusion random effects introduces positive correlation measurements: specifically, correlation two observations subject \\(\\rho=\\sigma^2_s/(\\sigma^2_s+\\sigma^2_e)\\) zero otherwise. correlation structure termed compound symmetry, since correlation measurements, \\(\\rho\\), regardless order observations. multiple random effects, dependence structure complicated.repeated measure models, need first reduce measurements single average per within-subject factor, fit model including subject blocking factor. therefore considering subjects fixed effects including blocking factors, estimate mean effect subject: value \\(\\sigma^2_s\\) estimated mean squared error subject term, empirical estimate can negative. contrast, mixed model machinery directly estimate variance term, constrained strictly positive.","code":""},{"path":"introduction-to-mixed-models.html","id":"fixed-vs-random-effects","chapter":"10 Introduction to mixed models","heading":"10.1 Fixed vs random effects","text":"Mixed models include, definition, random fixed effects. Fixed effects model parameters corresponding overall average difference means experimental conditions. terms want perform hypothesis tests compute contrasts. far, considered models fixed effects.Random effects, hand, assumes treatments random samples population interest. gathered another sample, looking new set treatments. Random effects model variability arising sampling population focuses variance correlation parameters. Addition random effects impact population mean, induces variability correlation within subject. consensual definition, Gelman (2005) lists handful:sample exhausts population, corresponding variable fixed; sample small (.e., negligible) part population corresponding variable random [Green Tukey (1960)].Effects fixed interesting random interest underlying population (e.g., Searle, Casella McCulloch [(1992), Section 1.4])terms estimation, fixed effect terms mean parameters, random effects obtained variance correlation parameters. repeated measure approach fixed effects blocking, estimate average subject despite fact quantity interest. Estimating mean handful measurements risky business estimated effects sensitive outliers.Random effects proceed directly estimate variability arising different subjects. can still get predictions subject-specific effect, prediction shrunk toward global mean particular treatment category. gather data subjects, predictions become closer fixed effect estimates number observations per subject group increases, prediction can deviate mean estimates case measurements per subject.Oelhert:2010 identifies following step perform mixed modelIdentify sources variationIdentify whether factors crossed nestedDetermine whether factors fixed randomFigure interactions can exist whether can fitted.fit model, identifiers subjects must declared factors (categorical variables).say factors nested (\\(\\) within \\(B\\)) one can coexist within levels : implications, interaction two. -subject experiments, factors crossed, meaning can assign experimental unit subject factor combination thus interactions can occur. example, subjects nested -level factors.next step determining whether enough observations support inclusion random term. pure within-subject design, include interaction subject within-factor unless multiple replications subject, case . can therefore include subject identifier experimental factor, well interaction. Note lme4 package, random effects specified inside parenthesis.Example 10.1  (Happy fakes, remixed) consider experiment Amirabdolahian Ali-Adeeb (2021) smiling fakes emotion, time pure mixed model perspective. means can simply keep observations model accordingly.\nFigure 10.1: Jittered scatterplot individual measurements per participant stimulus type.\nFigure 10.1 shows raw measurements, including notable outliers may due data acquisition problems instrumental manipulations. Since experiment performed non-controlled setting (pandemic) different apparatus everyone acting technician, unsurprising signal--noise ratio quite small. exclude (rather arbitrarily) measurements latency minus 40.see quite bit heterogeneity participants per stimulus participant pair, albeit less interaction. estimated variance terms rather large.can also look globally statistical evidence theThe global \\(F\\) test significance stimulus based approximation; denominator degrees freedom approximate \\(F\\) statistic based Satterthwaite’s method, provides correction. evidence differences experimental conditions. rather unsurprising look raw data Figure 10.1.Example 10.2  (Verbalization memorization) consider replication study Elliott et al. (2021),\nstudied verbalization verbalization kids aged 5, 6, 7 10. replication performed 17 different school labs, adapting protocol Flavell, Beach, Chinsky (1966), overall sample 977 child partaking experiment.participant assigned three tasks: delayed recall 15 seconds wait, immediate, finally naming task (point--name). timing variable records order presented: order delayed immediate counterbalanced across individuals, naming task always occurring last. response variable number words correctly recalled five. experimenters also recorded frequency students spontaneously verbalized task (except naming task, instructed ).task order within-subject factor, whereas timing age -subject factors: particularly interested speech frequency improvement time (pairwise differences trend).fit linear mixed model random effect children id lab: since children nested lab, must specify random effects via (1 | id:lab) + (1 | lab) id unique.modify data keep 5 6 years old students, since older kids verbalized task large disbalance (14 ten years old 235, 19 269 seven years old). also exclude point--name task, since verbalization part instruction. leaves us \\(1419\\) observations can check indeed enough children condition get estimates.Given multiple students every age group, can include two-way three-way interactions \\(2^3\\) design. also include random effects student lab.focus selected part output summary() giving estimated variance terms.can interpret results follows: total variance sum id, lab residual variances components give us negligible effect lab 7 percent total variance, versus 40.5 percent children-specific variability. Since 17 labs, individual specific variability children level, random effect lab doesn’t add much correlation.type III ANOVA table shows evidence interaction task order, age verbalization (three-interaction) small difference timing verbalization. Thus, compute estimated marginal means age estimated correct number words 1.854 (95% confidence interval [1.669, 2.04]) words 5 5 years olds 2.445 (95% CI [2.234, 2.655]) words six years old. Note , despite large number children experiment, degrees freedom Kenward–Roger method much fewer, respectively 35.827 60.944 five six years old.\\(t\\)-test pairwise difference marginal effect 0.59 words standard error 0.108. Judging output, degrees freedom calculation pairwise \\(t\\)-test erroneous — seem average number entries five years old (440) six years old (506), fails account fact kid featured twice. Given large magnitude ratio, still amounts strong result provided standard error correct.can easily see limited interaction strong main effects interaction plot Figure 10.2. confidence intervals different width sample inbalance.\nFigure 10.2: Interaction plot recall task younger children.\n","code":"\nlibrary(lmerTest) \n# fit and tests for mixed models\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nmixedmod <- lmer(\n  latency ~ stimulus + \n    (1 | id) + # random effect for subject\n    (1 | id:stimulus), \n  # random effect for interaction \n  data = hecedsm::AA21 |> #remove outliers\n    dplyr::filter(latency > -40))\n# Output parameter estimates\nprint(mixedmod)\n#> Linear mixed model fit by REML ['lmerModLmerTest']\n#> Formula: latency ~ stimulus + (1 | id) + (1 | id:stimulus)\n#>    Data: dplyr::filter(hecedsm::AA21, latency > -40)\n#> REML criterion at convergence: 8008\n#> Random effects:\n#>  Groups      Name        Std.Dev.\n#>  id:stimulus (Intercept) 0.737   \n#>  id          (Intercept) 2.268   \n#>  Residual                6.223   \n#> Number of obs: 1227, groups:  id:stimulus, 36; id, 12\n#> Fixed Effects:\n#> (Intercept)    stimulus1    stimulus2  \n#>     -10.537       -0.253       -0.139#> Type III Analysis of Variance Table with Satterthwaite's method\n#>          Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n#> stimulus   65.6    32.8     2  23.3    0.85   0.44\ndata(MULTI21_D2, package = \"hecedsm\")\nMULTI21_D2_sub <- MULTI21_D2 |>\n  dplyr::filter(\n    age %in% c(\"5yo\", \"6yo\"),\n    timing != \"point-and-name\") |>\n  dplyr::mutate(\n    verbalization = factor(frequency != \"never\",\n                           labels = c(\"no\", \"yes\")),\n    age = factor(age)) # drop unused age levels\nxtabs(~ age + verbalization, data = MULTI21_D2_sub)\n#>      verbalization\n#> age    no yes\n#>   5yo 106 334\n#>   6yo  56 450\nlibrary(lmerTest)\nlibrary(emmeans)\nhmod <- lmer(\n  mcorrect ~ age*timing*verbalization + (1 | id:lab) + (1 | lab), \n  data = MULTI21_D2_sub)\n# Parameter estimates\n#summary(hmod)\n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  id:lab   (Intercept) 0.3587   0.599   \n#>  lab      (Intercept) 0.0625   0.250   \n#>  Residual             0.6823   0.826   \n#> Number of obs: 946, groups:  id:lab, 473; lab, 17\nanova(hmod, ddf = \"Kenward-Roger\")\n#> Type III Analysis of Variance Table with Kenward-Roger's method\n#>                          Sum Sq Mean Sq NumDF DenDF F value Pr(>F)    \n#> age                       20.50   20.50     1   459   30.05  7e-08 ***\n#> timing                     3.25    3.25     1   469    4.76   0.03 *  \n#> verbalization             13.61   13.61     1   459   19.94  1e-05 ***\n#> age:timing                 0.24    0.24     1   469    0.36   0.55    \n#> age:verbalization          0.08    0.08     1   462    0.12   0.72    \n#> timing:verbalization       2.64    2.64     1   469    3.88   0.05 *  \n#> age:timing:verbalization   0.27    0.27     1   469    0.40   0.53    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Check estimated marginal means for age\nemm <- emmeans(hmod, specs = \"age\")\nemm\n#>  age emmean     SE   df lower.CL upper.CL\n#>  5yo   1.85 0.0914 35.8     1.67     2.04\n#>  6yo   2.44 0.1053 60.9     2.23     2.65\n#> \n#> Results are averaged over the levels of: timing, verbalization \n#> Degrees-of-freedom method: kenward-roger \n#> Confidence level used: 0.95\n# Pairwise differences\npairdiff <- emm |> pairs()\npairdiff\n#>  contrast  estimate    SE  df t.ratio p.value\n#>  5yo - 6yo    -0.59 0.108 459  -5.480  <.0001\n#> \n#> Results are averaged over the levels of: timing, verbalization \n#> Degrees-of-freedom method: kenward-roger"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
