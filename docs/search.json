[{"path":"index.html","id":"preliminary-remarks","chapter":"Preliminary remarks","heading":"Preliminary remarks","text":"notes licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License last compiled 2021-09-05.objective course teach basic principles experimental designs statistical inference latter using R programming language. pay particular attention correct reporting interpretation results learn review critically scientific papers using experimental designs.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Learning objectives:Learning terminology associated experiments.Assessing generalizability study based consideration sample characteristics, sampling scheme population.Distinguishing observational experimental studiesDescribing words four pillars experimental designs.Understanding rationale behind requirements good experimental studies translates model assumptions.field causal inference concerned inferring effect treatment variable (sometimes called independent variable) response variable (dependent variable). general, however (Cox 1958)effects investigation tend masked fluctuations outside experimenter’s control.purpose experiments arrange data collection capable disentangling differences due treatment due (often large) intrinsic variation measurements. typically expect differences treatments (thus effect) comparatively stable relative measurement variation.","code":""},{"path":"introduction.html","id":"terminology","chapter":"1 Introduction","heading":"1.1 Terminology","text":"simplest form, experimental design comparison two treatments (experimental conditions):subjects (experimental units) different groups treatment similar characteristics treated exactly way experimentation except treatment receiving. Formally, experimental unit smallest division two units may receive different treatments.observational unit smallest level (time point, individual) measurement recorded.experimental treatments conditions (also called factor, independent variable), manipulated controlled researcher. Oftentimes, control baseline treatment relative measure improvement (e.g., placebo drugs).Additional explanatories intrinsic experimental (sub-)units termed blocking factors. Controlling allows reduce variability measurements, typically leading improved inferences.different treatments administered subjects participating study, researcher measures one outcomes (also called responses dependent variables) subject.Observed differences outcome variable experimental conditions (treatments) called treatment effect (effect size).Example 1.1  (Pedagogical experience) Suppose want study effectiveness different pedagogical approaches learning. Evidence-based pedagogical researchs point active learning leads higher retention information. corroborate research hypothesis, can design experiment different sections course assigned different teaching methods. example, student class group receives teaching assignment, experimental units sections observations units individual students.treatment teaching method (traditional teaching versus flipped classroom).Potential blocking factors experiment include strength individuals, reflects prior exposure topic, knowledge, maturity, etc. measured using preliminary exam assignment students class groups completed. Additional factors worth controlling include timing classroom (morning, afternoon, evening classes) instructors.marketing department wants know value brand determining much customers willing pay product relative cheaper generic product offered store. Economic theory suggests substitution effect: customers may prefer brand product, switch generic version price tag high. check theory, one design experiment.researcher, conduct study? Identify specific product. latter, definean adequate response variablethe experimental observational unitspotential confounding variables need accounted .","code":""},{"path":"introduction.html","id":"review-of-basic-concepts","chapter":"1 Introduction","heading":"1.2 Review of basic concepts","text":"","code":""},{"path":"introduction.html","id":"variables","chapter":"1 Introduction","heading":"1.2.1 Variables","text":"choice statistical model test depends underlying type data collected. many choices: quantitative (discrete continuous) variables numeric, qualitative (binary, nominal, ordinal) can described using adjective; prefer term categorical, evocative. choice graphical representation data contingent variable type. Specifically,variable represents characteristic population, example sex individual, price item, etc.observation set measures (variables) collected identical conditions individual given time.\nFigure 1.1: Artwork Allison Horst continuous (left) discrete variables (right).\nmodels deal -called regression models, mean quantitative variable function variables, termed explanatories. two types numerical variablesa discrete variable takes countable number values, prime examples binary variables count variables.continuous variable can take (theory) infinite possible number values, even measurements rounded measured limited precision (time, width, mass). many case, also consider discrete variables continuous take enough values (e.g., money).Categorical variables take finite values. regrouped two groups, nominal ordering levels (sex, colour, country origin) ordinal ordered (Likert scale, salary scale) ordering reflected graphs tables. bundle every categorical variable using arbitrary encoding levels: modelling, variables taking \\(K\\) possible values (levels) must transformed set \\(K-1\\) binary variables \\(T_1, \\ldots, T_K\\), corresponds logical group \\(k\\) (yes = 1, = 0), omitted level corresponding baseline \\(K-1\\) indicators zero. Failing declare categorical variables software common mistake, especially saved database using integers (1,2, \\(\\ldots\\)) rather text (Monday, Tuesday, \\(\\ldots\\)).\nFigure 1.2: Artwork Allison Horst examples categorical variables: nominal (left), ordinal (middle) binary (right).\n","code":""},{"path":"introduction.html","id":"population-sample","chapter":"1 Introduction","heading":"1.2.2 Population and samples","text":"well-designed sampling schemes results generalize beyond group observed. thus paramount importance define objective population interest want make conclusions.Generally, seek estimate characteristics population using sample (sub-group population smaller size). population interest collection individuals study targets. example, Labour Force Survey (LFS) monthly study conducted Statistics Canada, define target population “members selected household 15 years old older, whether work .” Asking every Canadian meeting definition costly process long: characteristic interest (employment) also snapshot time can vary person leaves job, enters job market become unemployed. example, collecting census impossible costly.general, therefore consider samples gather information seek obtain. purpose statistical inference draw conclusions population, using share latter accounting sources variability. pollster George Gallup made great analogy sample population:One spoonful can reflect taste whole pot, soup well-stirredA sample sub-group individuals drawn random population. won’t focus data collection, keep mind following information: sample good, must representative population study.Parcours AGIR HEC Montréal pilot project Bachelor Administration students initiated study impact flipped classroom active learning performance.think can draw conclusions efficacy teaching method comparing results students rest bachelor program? List potential issues approach addressing internal external validity, generalizability, effect lurking variables, etc.individuals selected random part sample, measurement characteristic interest also random change one sample next. larger samples typically carry information, sample size guarantee quality, following example demonstrates.Example 1.2  (Polling 1936 USA Presidential Election) Literary Digest surveyed 10 millions people mail know voting preferences 1936 USA Presidential Election. sizeable share, 2.4 millions answered, giving Alf Landon (57%) incumbent President Franklin D. Roosevelt (43%). latter nevertheless won landslide election 62% votes cast, 19% forecast error. Biased sampling differential non-response mostly responsible error: sampling frame built using ``phone number directories, drivers’ registrations, club memberships, etc.’’, skewed sample towards rich upper class white people susceptible vote GOP.contrast, Gallup correctly predicted outcome polling () 50K inhabitants. Read full story .considerations guide determining population interest study?","code":""},{"path":"introduction.html","id":"sampling","chapter":"1 Introduction","heading":"1.2.3 Sampling","text":"sampling costly, can collect limited information variable interest, drawing population sampling frame (phone books, population register, etc.) Good sampling frames can purchased sampling firms.general, randomization necessary order obtain representative sample1, one match characteristics population. Failing randomize leads introduction bias generally conclusions drawn study won’t generalizable.Even observational units selected random participate, may bias introduced due non-response. 1950s, conducting surveys relatively easier people listed telephone books; nowadays, sampling firms rely mix interactive voice response live callers, sampling frames mixing landlines, cellphones online panels together (heavy) weighting correct non-response. Sampling difficult problem engage cursorily, readers urged exercise scrutiny reading papers.Reflect choice platform used collect answers think influence composition sample returned affect non-response systematic way.examining problems related sampling, review main random sampling methods. simplest simple random sampling, whereby \\(n\\) units drawn completely random (uniformly) \\(N\\) elements sampling frame. second common scheme stratified sampling, whereby certain numbers units drawn uniformly strata, namely subgroups (e.g., gender). Finally, cluster sampling consists sampling subgroups.Example 1.3  (Illustration sampling schemes) Suppose wish look student satisfaction regarding material taught introductory statistics course offered multiple sections. population consists students enrolled course given semester list provides sampling frame. can define strata consist class group. simple random sample obtaining sampling randomly abstracting class groups, stratified sample drawing randomly number class group cluster sampling drawing students selected class groups. Cluster sampling mostly useful groups similar costs associated sampling multiple strata expensive.\nFigure 1.3: Illustration three sampling schemes nine stratum: simple random sampling (left), stratified sampling (middle) cluster sampling (right). middle, grouping corresponds stratum (e.g., age groups) whereas right contains cluster (e.g., villages)\nStratified sampling typically superior care similar proportions sampled group useful reweighting: 1.3, true proportion sampled 1/3, simple random sampling range [0.22, 0.39] among strata, compared [0.32, 0.34] stratified sample.credibility study relies large part quality data collection. customary report descriptive statistics sample description population?instances sampling, non-random avoided whenever possible. include convenience samples, consisting observational units easy access include (e.g., friends, students university, passerby street). Much like anecdotal reports, observational units need representative whole population difficult understand relate latter.recent years, proliferation studies employing data obtained web experimentation plateforms Amazon’s Mechanical Turk (MTurk), point Journal Management commissioned review (Aguinis, Villamor, Ramani 2021). samples subject self-selection bias read skepticism. reserve tools paired samples (e.g., asking people perform multiple tasks presented random order) composition population relatively unimportant. make sure sample matches target population, can use statistical tests informal comparison compare repartition individuals composition obtained census.","code":""},{"path":"introduction.html","id":"study-type","chapter":"1 Introduction","heading":"1.2.4 Study type","text":"two categories studies: observational experimental. main difference two researchers collect data observational studies intervene treatment assignment data created, whereas assignment mechanism fully determined experimenter latter case.\nexample, economist studying impact interest rates price housing can look historical records sales. Similarly, surveys studying labour market also observational: people influence type job performed employees social benefits see happened. Observational studies can lead detection association, experiment researcher controls allocation mechanism randomization can lead directly establish existence causal relationship. everything else well controlled experiment, treatment effect principle caused factor.preceding paragraph shouldn’t taken mean one get meaningful conclusions observational studies. Rather, wish highlight controlling non-random allocation potential confounding much complicated, requires practitioners make stronger (sometimes unverifiable) assumptions requires using different toolbox (including, limited differences differences, propensity score weighting, instrumental variables).\nFigure 1.4: Two two classification matrix experiments based sampling study type. Material Mine Çetinkaya-Rundel OpenIntro distributed CC -SA license.\nFigure 1.4 summarizes two preceding sections. Random allocation observational units assignment treatment leads ideal studies, may impossible due ethical considerations.","code":""},{"path":"introduction.html","id":"examples-of-experimental-designs","chapter":"1 Introduction","heading":"1.3 Examples of experimental designs","text":"One earliest example statistical experiment agricultural field trial: fact, experiments ongoing since 1841 Rothamsted Experimental Station, R. . Fisher worked 14 years developed much early theory; Yates (1964) details contribution field design experiments.Section 1.4 Berger, Maurer, Celli (2018) lists various applications experimental designs variety fields.Example 1.4  (Modern experiments /B testing) modern experiments happen online, tech companies running thousands experiments ongoing basis order discover improvement interfaces lead increased profits. Harvard Business Review article (Kohavi Thomke 2017) details small tweaks display advertisements Microsoft Bing search engine landing page lead whooping 12% increase revenues. randomized control trials, termed /B experiments, involve splitting incoming traffic separate groups; group see different views webpage differ ever slightly. experimenters compare traffic click revenues. large scale, even small effects can major financial consequences can learned despite large variability customer behaviour.","code":""},{"path":"introduction.html","id":"evidence-based-policies","chapter":"1 Introduction","heading":"1.3.1 Evidence-based policies","text":"Experimental design revolves large part understanding best allocate resources choosing effective treatment lot. multiple examples randomized control experiments used policy making. Examples includeTennessee’s Student Teacher Achievement Ratio (STAR) project (Achilles et al. 2008): study looked effect student teacher ratio conclude smaller class sizes lead better outcomes.Four-year longitudinal class-size study funded Tennessee General Assembly conducted State Department Education. 7,000 students 79 schools randomly assigned one 3 interventions: small class (13 17 students per teacher), regular class (22 25 students per teacher), regular--aide class (22 25 students full-time teacher’s aide). Classroom teachers also randomly assigned classes teach. interventions initiated students entered school kindergarten continued third grade.RAND’s Health Insurance Experiment (Brook et al. 2006): study concluded cost sharing reduced “inappropriate unnecessary” medical care (overutilization), also lead areduction “appropriate needed” medical care.HIE large-scale, randomized experiment conducted 1971 1982. study, RAND recruited 2,750 families encompassing 7,700 individuals, age 65. chosen six sites across United States provide regional urban/rural balance. Participants randomly assigned one five types health insurance plans created specifically experiment. four basic types fee--service plans: One type offered free care; three types involved varying levels cost sharing — 25 percent, 50 percent, 95 percent coinsurance (percentage medical charges consumer must pay). fifth type health insurance plan nonprofit, HMO-style group cooperative. assigned HMO received care free charge. poorer families plans involved cost sharing, amount cost sharing income-adjusted one three levels: 5, 10, 15 percent income. --pocket spending capped percentages income $1,000 annually (roughly $3,000 annually adjusted 1977 2005 levels), whichever lower.Families participated experiment 3–5 years. upper age limit adults time enrollment 61, participants become eligible Medicare experiment ended. assess participant service use, costs, quality care, RAND served families’ insurer processed claims. assess participant health, RAND administered surveys beginning end experiment also conducted comprehensive physical exams. Sixty percent participants randomly chosen receive exams beginning study, received physicals end. random use physicals beginning intended control possible health effects might stimulated physical exam alone, independent participation experiment.Oregon Health Insurance Experiment (Baicker et al. 2013): study described length Section 9.5 Telling stories data Rohan Alexander (Alexander 2022).","code":""},{"path":"introduction.html","id":"planning-experiments","chapter":"1 Introduction","heading":"1.4 Planning of experiments","text":"four pillars experimental designs;Control: experiment, allocation treatment controlled experimenter, allowing direct comparisons groups.Randomization: prevent lurking variables confounders impacting conclusions, observational units randomly allocated treatment groups estimate effect causal interpretation.Replication: multiple observations treatment allocation necessary estimate variability measurements increase precision average measurement.Blocking: technique allows experimenters control variability experimental units dividing blocks blocks similar. allows us separate variability due differences levels blocking variable overall variability, leading precision gains.Underlying design experiments use techniques eliminate much possible variability detecting cause effects least amount resources allocating wisely.","code":""},{"path":"introduction.html","id":"requirements-for-good-experiments","chapter":"1 Introduction","heading":"1.5 Requirements for good experiments","text":"Section 1.2 Cox (1958) describes various requirements necessary experiments useful. areabsence systematic errorprecisionrange validitysimplicityWe review turn.","code":""},{"path":"introduction.html","id":"absence-of-systematic-error","chapter":"1 Introduction","heading":"1.5.1 Absence of systematic error","text":"point requires careful planning listing potential confounding variables affect response.Example 1.5  Suppose wish consider differences student performance two instructors. first teaches morning classes, second teaches evening, impossible disentangle effect timing instructor performance. comparisons undertaken compelling prior evidence timing impact outcome interest.first point raised Cox thus weensure experimental units receiving one treatment differ systematic way receiving another treatment.point also motivates use double-blind procedures (experimenters participants unaware treatment allocation) use placebo control groups (avoid psychological effects, etc. associated receiving treatment lack thereof participants).Randomization core achieving goal, ensuring measurements independent one another also comes corollary.","code":""},{"path":"introduction.html","id":"variability","chapter":"1 Introduction","heading":"1.5.2 Variability","text":"second point listed Cox (1958) variability estimator. Much precision can captured signal noise ratio, effect size divided standard error. latter function \n() accuracy experimental work measurements apparatus intrinsic variability phenomenon study, (b) number experimental observational units, .e., sample size (c) choice design statistical procedures.Point () typically influenced experimenter outside choosing response variable obtain reliable measurements. Point (c) related method analysis, oftentimes standard unless robustness considerations. Point (b) core planning, notably choosing number units use allocation treatment different (sub)-units.","code":""},{"path":"introduction.html","id":"generalizability","chapter":"1 Introduction","heading":"1.5.3 Generalizability","text":"studies done objective generalizing findings beyond particular units analyzed. range validity thus crucially depends choice population sample drawn particular sampling scheme. Non-random sampling severely limits extrapolation results general settings. leads Cox advocate havingnot just empirical knowledge treatment differences , also understanding reasons differences.Even believe factor effect, may wise introduce experiment check assumption: source variability, shouldn’t impact findings time provide robustness.look continuous treatment, probably safe draw conclusions within range doses administered. Comic 1.5 absurd, makes point.\nFigure 1.5: xkcd comic 645 (Extrapolating) Randall Munroe. Alt text: third trimester, thousands babies inside .\nExample 1.6  (Generalizability) Replication studies done university often draw participants students enrolled institutions. findings thus necessarily robust extrapolated whole population characteristics strong (familiarity technology, acquaintance administrative system, political views, etc). samples often convenience samples.Example 1.7  (Spratt-Archer barley Ireland) Example 1.9 Cox (1958) mentions recollections ``Student’’ Spratt-Archer barley, new variety barley performed well experiments Irish Department Agriculture encouraged introduced elsewhere. Fuelled district skepticism new variety, Department ran experiment comparing yield Spratt-Archer barley native race. findings surprised experimenters: native barley grew quickly resistant weeds, leading higher yields. concluded initial experiments misleading Spratt-Archer barley experimented well-farmed areas.","code":""},{"path":"introduction.html","id":"simplicity","chapter":"1 Introduction","heading":"1.5.4 Simplicity","text":"fourth requirement one simplicity design, almost invariably leads simplicity statistical analysis. Randomized control-trials often viewed golden rule determining efficacy policies treatments set assumptions make pretty minimalist due randomization. researchers management necessarily comfortable advanced statistical techniques also minimizes burden. 1.6 shows hypothetical graph efficacy Moderna MRNA vaccine Covid: difference clearly visible suitable experimental setting, conclusions easily drawn.Randomization justifies use statistical tools use weak assumptions, units measurements independent one another. Drawing conclusions observational studies, contrast experimental designs requires making often unrealistic unverifiable assumptions choice techniques required handle lack randomness often beyond toolbox applied researchers.\nFigure 1.6: xkcd comic 2400 (Statistics) Randall Munroe. Alt text: reject null hypothesis based ‘hot damn, check chart’ test.\nDefine following terms word: experimental unit, factor, effect sizeWhat main benefit experimental studies observational studies?List four pillars experimental design briefly describe .","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"2 Hypothesis testing","heading":"2 Hypothesis testing","text":"applied domains, empirical evidences drive advancement field data well designed experiments contribute built science. order draw conclusions favour theory, researchers turn (often unwillingly) statistics back claims. led prevalence use null hypothesis statistical testing (NHST) framework. One important aspect reproducibility crisis misuse \\(p\\)-values journal articles: falsification null hypothesis enough provide substantive findings theory.introductory statistics course typically present hypothesis tests without giving much thoughts underlying construction principles procedures, users often reductive view statistics catalogue pre-determined procedures. make culinary analogy, users focus learning recipes rather trying understand basics cookery. chapter focuses understanding key ideas related testing.","code":""},{"path":"hypothesis-testing.html","id":"sampling-variability","chapter":"2 Hypothesis testing","heading":"2.1 Sampling variability","text":"typically interested characteristic population, oftentimes (conditional) theoretical average continuous response variable, denoted \\(\\mu\\). quantity exists, unknown us best can estimate using random samples drawn population.call numerical summaries data statistics. important distinguish procedures/formulas numerical values. estimator rule formula used calculate estimate parameter quantity interest based observed data (like recipe cake). observed data can actually compute sample mean, , estimate — actual value (cake). words,estimator procedure formula telling us transform sample data numerical summary. output random: even repeat recipe, won’t get exact output everytime.estimate numerical value obtained apply formula observed data.example, sample mean sample size \\(n\\) sum elements divided sample size, \\(\\overline{Y}=n^{-1}(Y_1 + \\cdots + Y_n)\\). inputs function \\(\\overline{Y}\\) random, estimator \\(\\overline{Y}\\) also random. illustrate point, Figure 2.1 shows five simple random samples size \\(n=10\\) drawn hypothetical population mean \\(\\mu\\) standard deviation \\(\\sigma\\), along sample mean \\(\\overline{y}\\). Thus, sampling variability implies sample means subgroups always differ even share characteristics. can view sampling variability noise: goal extract signal (typically differences means) accounting spurious results due background noise.\nFigure 2.1: Five samples size \\(n=10\\) drawn common population mean \\(\\mu\\) (horizontal line). colored segments show sample means sample.\ncan clearly see Figure 2.1 , even sample drawn population, sample mean varies one sample next result sampling variability. astute eye however notice sample means less dispersed around \\(\\mu\\) individual measurements. sample mean \\(\\overline{Y}\\) based multiple observations, information available. fundamental principle statistics: information accumulated get information, estimation becomes less noisy.Since values sample mean don’t tell whole picture, may also consider variability. sample variance \\(S_n\\) estimator standard deviation \\(\\sigma\\), \\[\\begin{align*}\nS^2_n &= \\frac{1}{n-1} \\sum_{=1}^n (X_i-\\overline{X})^2.\n\\end{align*}\\]\nsquare root variance statistic termed standard error; confused standard deviation \\(\\sigma\\) population sample observations \\(Y_1, \\ldots\\) drawn. standard deviation standard error expressed units measurements, easier interpret variance. Since standard error function sample size, however good practice report estimated standard deviation reports.\nFigure 2.2: Histograms 10 random samples size \\(n=20\\) discrete uniform distribution.\nEven drawn population, 10 samples Figure 2.2 look quite different. thing play sample variability: since \\(n=20\\) observations total, average 10% observations 10 bins, bins empty others many counts. fluctuation due randomness, chance.can thus detect whether see compatible model think generated data? key collect observations: bar height sample proportion, average 0/1 values ones indicating observation bin zero otherwise.Consider now happens increase sample size: top panel Figure 2.3 shows uniform samples increasing samples size. histogram looks like true underlying distribution (flat) sample size increases ’s nearly indistinguishable theoretical one (straight line) \\(n=10 000\\). , variability decreases tenfold every time sample size increases factor 100. bottom panel, hand, isn’t uniform distribution larger samples come closer population distribution. couldn’t spotted difference first two plots, since sampling variability important; , lack data bins attributed chance. line practical applications, limited sample size restricts capacity disentangle real differences sampling variability. must embrace uncertainty: next section, outline hypothesis testing helps us disentangle signal noise.\nFigure 2.3: Histograms data uniform distribution (top) non-uniform (bottom) increasing sample sizes 10, 100, 1000 10 000 (left right).\n","code":""},{"path":"hypothesis-testing.html","id":"tests","chapter":"2 Hypothesis testing","heading":"2.2 Hypothesis testing","text":"hypothesis test binary decision rule (yes/) used evaluate statistical evidence provided sample make decision regarding underlying population. main steps involved :define model parametersformulate alternative null hypothesischoose calculate test statisticobtain null distribution describing behaviour test statistic \\(\\mathscr{H}_0\\)calculate p-valueconclude (reject fail reject \\(\\mathscr{H}_0\\)) context problem.good analogy hypothesis tests trial murder appointed juror.judge lets choose two mutually exclusive outcome, guilty guilty, based evidence presented court.presumption innocence applies evidences judged optic: evidence remotely plausible person innocent? burden proof lies prosecution avoid much possible judicial errors. null hypothesis \\(\\mathscr{H}_0\\) guilty, whereas alternative \\(\\mathscr{H}_a\\) guilty. reasonable doubt, verdict trial guilty.test statistic (choice test) represents summary proof. overwhelming evidence, higher chance accused declared guilty. prosecutor chooses proof best outline : choice evidence (statistic) ultimately maximize evidence, parallels power test.null distribution benchmark judge evidence (jurisprudence). Given proof, odds assuming person innocent? Since possibly different every test, common report instead p-value, gives level evidence uniform scale easily interpreted.final step verdict, binary decision outcomes: guilty guilty. hypothesis test performed level \\(\\alpha\\), one reject (guilty) p-value less \\(\\alpha\\). Even declare person guilty, doesn’t mean defendant innocent vice-versa.","code":""},{"path":"hypothesis-testing.html","id":"hypothesis","chapter":"2 Hypothesis testing","heading":"2.2.1 Hypothesis","text":"statistical tests two hypotheses: null hypothesis (\\(\\mathscr{H}_0\\)) alternative hypothesis (\\(\\mathscr{H}_1\\)). Usually, null hypothesis (‘status quo’) single numerical value. alternative ’re really interested testing. 2.1, consider whether five groups mean \\(\\mathscr{H}_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_5\\) alternative least two different. two outcomes mutually exclusive cover possible cases. statistical hypothesis test allows us decide whether data provides enough evidence reject \\(\\mathscr{H}_0\\) favor \\(\\mathscr{H}_1\\), subject pre-specified risk error: know differences just due sampling variability 2.1 data fake, practice need assess evidence using numerical summary.","code":""},{"path":"hypothesis-testing.html","id":"test-statistic","chapter":"2 Hypothesis testing","heading":"2.2.2 Test statistic","text":"test statistic \\(T\\) function data takes data input outputs summary information contained sample characteristic interest, say population mean. form test statistic chosen know behaves null hypothesis true (e.g., difference overall means different groups). order assess whether numerical value \\(T\\) unusual, need know potential values taken \\(T\\) relative probability \\(\\mathscr{H}_0\\) true. allows us determine values \\(T\\) likely \\(\\mathscr{H}_0\\) true. Many statistics consider form2\n\\[\\begin{align*}\nT = \\frac{\\text{estimated effect}- \\text{postulated effect}}{\\text{estimated effect variability}} = \\frac{\\widehat{\\theta} - \\theta_0}{\\mathrm{se}(\\widehat{\\theta})}\n\\end{align*}\\]\n\\(\\widehat{\\theta}\\) estimator \\(\\theta\\), \\(\\theta_0\\) postulated value parameter \\(\\mathrm{se}(\\widehat{\\theta})\\) standard error test statistic \\(\\widehat{\\theta}\\), measure variability. quantity designed difference, \\(T\\) approximately mean zero variance one. standardization makes comparison easier.example, interested mean differences treatment group control group, denoted \\(\\mu_1\\) \\(\\mu_0\\), \\(\\theta = \\mu_0-\\mu_1\\) \\(\\mathscr{H}_0: \\mu_0 = \\mu_1\\) corresponds \\(\\mathscr{H}_0: \\theta = 0\\) difference. numerator thus consist difference sample means denominator standard error quantity, calculated using software.example, test whether mean population zero, set\n\\[\\begin{align*}\n\\mathscr{H}_0: \\mu=0, \\qquad  \\mathscr{H}_a:\\mu \\neq 0,\n\\end{align*}\\]\nusual \\(t\\)-statistic \n\\[\\begin{align}\nT &= \\frac{\\overline{X}-0}{S_n/\\sqrt{n}}\n\\tag{2.1}\n\\end{align}\\]\n\\(\\overline{X}\\) sample mean \\(X_1, \\ldots, X_n\\) denominator (2.1) standard error sample mean, \\(\\mathsf{se}(\\overline{Y}) = \\sigma/\\sqrt{n}\\). precision sample mean increases proportionally square root sample size: standard error gets halved double number observations, decreases factor 10 100 times observations. Similar calculations hold two-sample \\(t\\)-test, whereby \\(\\widehat{\\theta} = \\overline{Y}_1 - \\overline{Y}_0\\) treatment group \\(T_1\\) control \\(T_0\\). Assuming equal variance, denominator estimated using pooled variance.","code":""},{"path":"hypothesis-testing.html","id":"null-distribution-and-p-value","chapter":"2 Hypothesis testing","heading":"2.2.3 Null distribution and p-value","text":"p-value allows us decide whether observed value test statistic \\(T\\) plausible \\(\\mathscr{H}_0\\). Specifically, p-value probability test statistic equal extreme estimate computed data, assuming \\(\\mathscr{H}_0\\) true. Suppose based random sample \\(X_1, \\ldots, X_n\\) obtain statistic whose value \\(T=t\\). two-sided test \\(\\mathscr{H}_0:\\theta=\\theta_0\\) vs. \\(\\mathscr{H}_a:\\theta \\neq \\theta_0\\), p-value \\(\\mathsf{Pr}_0(|T| \\geq |t|)\\). distribution \\(T\\) symmetric around zero, p-value \n\\[\\begin{align*}\np = 2 \\times \\mathsf{Pr}_0(T \\geq |t|).\n\\end{align*}\\]determine null distribution given true data generating mechanism unknown us? simple cases, might possible enumerate possible outcomes thus quantity degree outlyingness observed statistic. general settings, can resort simulations probability theory: central limit theorem says tell us sample mean behaves like normal random variable mean \\(\\mu\\) standard deviation \\(\\sigma/\\sqrt{n}\\) \\(n\\) large enough. central limit theorem broader applications since applies average, can use derive benchmarks commonly used statistics large samples. software use approximations proxy default: normal, Student’s \\(t\\), \\(\\chi^2\\) \\(F\\) distributions reference distributions arise often.\nFigure 2.4: Density p-values null hypothesis (left) alternative signal--noise ratio 0.5 (right). probability rejection \\(\\mathscr{H}_0\\) 0.1, area curve zero \\(\\alpha=0.1\\). null, density uniform (flat rectangle height 1) values unit interval equally likely. alternative, p-values cluster towards zero probability rejecting null hypothesis increases together signal--noise (approximately 0.22 alternative).\ngenerally three ways obtaining null distributions assessing degree evidence null hypothesisexact calculationslarge sample theory (aka ‘asymptotics’ statistical lingo)simulationWhile desirable, first method applicable simple cases (counting probability getting two six throw two fair die). second method commonly used due generality ease use (particularly older times computing power scarce), fares poorly small sample sizes (‘small’ context test-dependent). last approach can used approximate null distribution many scenarios, adds layer randomness extra computations costs sometimes worth .","code":""},{"path":"hypothesis-testing.html","id":"conclusion","chapter":"2 Hypothesis testing","heading":"2.2.4 Conclusion","text":"p-value allows us make decision null hypothesis. \\(\\mathscr{H}_0\\) true, p-value follows uniform distribution, shown Figure 2.4. Thus, p-value small, means observing outcome extreme \\(T=t\\) unlikely, ’re inclined think \\(\\mathscr{H}_0\\) true. ’s always underlying risk ’re making mistake make decision. statistic, two type errors:type error: reject \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) true,type II error: fail reject \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\).two hypothesis judged equally: seek avoid error type (judicial errors, corresponding condamning innocent). prevent , fix level test, \\(\\alpha\\), captures tolerance risk commiting type error: higher level test \\(\\alpha\\), often reject null hypothesis latter true. value \\(\\alpha \\(0, 1)\\) probability rejecting \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) fact true,\n\\[\\begin{align*}\n\\alpha = \\mathsf{Pr}_0\\left(\\text{ reject } \\mathscr{H}_0\\right).\n\\end{align*}\\]\nlevel \\(\\alpha\\) fixed beforehand, typically \\(1\\)%, \\(5\\)% \\(10\\)%. Keep mind probability type error \\(\\alpha\\) null model \\(\\mathscr{H}_0\\) correct (sic) correspond data generating mechanism.focus type error best understood thinking costs moving away status quo: new website design branding costly implement, want make sure enough evidence proposal better alternative lead increased traffic revenues.make decision, compare p-value \\(P\\) level test \\(\\alpha\\):\\(P < \\alpha\\), reject \\(\\mathscr{H}_0\\);\\(P \\geq \\alpha\\), fail reject \\(\\mathscr{H}_0\\).mix level test (probability fixed beforehand researcher) p-value. test level 5%, probability type error definition \\(\\alpha\\) depend p-value. latter conditional probability observing extreme statistic given null distribution \\(\\mathscr{H}_0\\) true.Example 2.2  (Gender inequality permutation tests) consider data Rosen Jerdee (1974), look sex role stereotypes impacts promotion opportunities women candidates. experiment took place 1972 experimental units, consisted 95 male bank supervisors, submitted various memorandums asked provide ratings decisions based information provided.interested Experiment 1 related promotion employees: managers requested decide whether promote employee become branch manager based recommendations ratings potential customer employee relations. authors intervention focused description nature (complexity) manager’s job (either simple complex) sex candidate (male female): files similar otherwise.authors played two factors: nature (complexity) manager’s job (either simple complex) sex candidate (male female): files similar otherwise.consider simplicity sex factor aggregate job \\(n=93\\) replies. Table 2.1 shows counts possibility.\nTable 2.1: Promotion recommandation branch manager based sex applicant.\nnull hypothesis interest sex impact, probability promotion men women. Let \\(p_{\\text{m}}\\) \\(p_{\\text{w}}\\) denote respective probabilities; can thus write mathematically null hypothesis \\(\\mathscr{H}_0: p_{\\text{m}} = p_{\\text{w}}\\) alternative \\(\\mathscr{H}_a: p_{\\text{m}} \\neq p_{\\text{w}}\\).test statistic typically employed two two contingency tables chi-square test3, compares overall proportions promoted subgroup. sample proportion male 32/42 = ~76%, compared 19/49 ~49% female — note sample averages set promote=1 hold file=0. seems difference 16% large, spurious: standard error sample proportions roughly 3.2% male 3.4% female.discrimination based sex, expect proportion people promoted overall; 51/93 =0.55 pooled sample. simply test mean difference, rely instead chi-square test, compares expected counts (based equal promotion rates) observed counts, suitably standardized. discrepancy large expected observed, casts doubt validity null hypothesis.\nTable 2.2: Chi-square test experiment 1 Rosen Jerdee (1974)\ncounts cell large, null distribution chi-square test well approximated \\(\\chi^2\\) distribution. output test includes value statistic, degrees freedom \\(\\chi^2\\) approximation p-value, gives probability random draw \\(\\chi^2_1\\) distribution larger observed test statistic assuming null hypothesis true. p-value small, 0.001, means result quite unlikely happen chance sex-discrimination.alternative test statistics used, among odds ratio. odds event ratio number success failure: example, number promoted held files. odds promotion male 32/12, whereas female 19/30. odds ratio male versus female thus \\(\\mathsf{}=\\) (32/12) / (19/30)= 4.21. null hypothesis, \\(\\mathscr{H}_0: \\mathsf{}=\\) 1 (probability promoted) (?)Fisher’s test assumes row sum totals fixed (, number promoted/withheld files male/female fixed design stage) uses derive exact probability observing particular configuration proportion success . test statistic Fisher’s exact test, obtained running fisher.test(dat_exper1), different null distribution4. contrary, p-value close one reported \\(\\chi^2\\) test Table 2.2.\nTable 2.3: Fisher’s exact test experiment 1 Rosen Jerdee (1974)\nYet another alternative obtain benchmark assess outlyingness observed odds ratio use simulations. Consider database containing raw data 93 rows, one manager, indicator action sex hypothetical employee presented task.\nTable 2.4: First five rows database long format experiment 1 Rosen Jerdee.\nnull hypothesis, sex incidence action manager. means get idea “-” world shuffling sex labels repeatedly. Thus, obtain benchmark repeating following steps multiple times:permute labels sex,recreate contingency table aggregating counts,calculate odds ratio simulated table.\nFigure 2.5: Histogram simulated null distribution obtained using permutation test; vertical red line indicates sample odds ratio.\nReassuringly, get roughly p-value. histogram 2.5 shows distribution ofThe article concluded (light experiments)Results confirmed hypothesis male administrators tend discriminate female employees personnel decisions involving promotion, development, supervision.first experiment, managers also asked rank applications potential employee customer relations using Likert scale six items ranging (1) extremely unfavorable (6) extremely favorable. However, averages reported Table 1 along (Rosen Jerdee 1974)Mean rating male candidate 4.73 compared mean rating 4.25 female candidate (\\(F=4.76, \\text{df} = 1/80, p < .05\\)), information isn’t sufficient: don’t know test used, importantly degrees freedom (80) much compared number observations, implying non-response isn’t discussed elsewhere.Partial selective reporting statistical procedures hinders reproducibility. many improvements possible presentation, including explicitly stating test statistic employed (\\(\\chi^2\\) value seemingly doesn’t correspond chi-square test incorrectly reported), providing sample size, means variance estimates, null distribution parameters, . Without , left speculate.","code":"\n## Create a 2x2 matrix (contingency table) with the counts\ndat_exper1 <- matrix(c(32L, 12L, 19L, 30L), ncol = 2, nrow = 2, byrow = TRUE)\n# Calculate the statistic on data\nobs_stat <- chisq.test(x = dat_exper1, correct = FALSE)\n# Tidy output to get a tibble\ntest_res <- broom::tidy(obs_stat)\nlibrary(infer)\n# Calculate the odds ratio for the sample\nobs_stat <- dat_exper1_long %>%\n  specify(response = action, explanatory = sex, success = \"promote\") %>%\n  calculate(stat = \"odds ratio\", order = c(\"male\", \"female\"))\n# Approximate the null distribution using a permutation test  \nset.seed(2021) # set random seed\nnull_dist <- dat_exper1_long %>%\n    specify(response = action, explanatory = sex, success = \"promote\") %>%\n    hypothesize(null = \"independence\") %>% # sex doesn't impact decision\n    generate(reps = 9999, type = \"permute\") %>% # shuffle sex\n    calculate(stat = \"odds ratio\", order = c(\"male\", \"female\")) \n# Visualize the null distribution\nggplot(data = null_dist, # a tibble with a single variable, 'stat'\n       mapping = aes(x = stat)) + # map 'stat' to the x-axis\n  geom_bar() + # bar plot b/c data are discrete (few combinations)\n  labs(x = \"odds ratio\") + # give meaningful label\n  geom_vline(data = obs_stat, # add vertical line\n             mapping = aes(xintercept = stat), # position on x-axis of line\n             color = \"red\") # color\n# Obtain the p-value\nnull_dist %>%\n  get_p_value(obs_stat = obs_stat, direction = \"two-sided\")\n#> # A tibble: 1 × 1\n#>   p_value\n#>     <dbl>\n#> 1 0.00240"},{"path":"hypothesis-testing.html","id":"power","chapter":"2 Hypothesis testing","heading":"2.2.5 Power","text":"two sides hypothesis test: either want show unreasonable assume null hypothesis, else want show beyond reasonable doubt difference effect significative: example, one wish demonstrate new website design (alternative hypothesis) leads significant increase sales relative status quo. ability detect improvements make discoveries depends power test: larger power, greater ability reject \\(\\mathscr{H}_0\\) latter false. power summarizes level evidence various combination parameters (effect size, variability, sample size).Failing reject \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_a\\) true (guilty verdict criminal) corresponds definition type II error, probability \\(1-\\gamma\\), say. power test probability correctly rejecting \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) false, .e.,\n\\[\\begin{align*}\n\\gamma = \\mathsf{Pr}_a(\\text{reject} \\mathscr{H}_0)\n\\end{align*}\\]\nDepending alternative models, less easy detect null hypothesis false reject favor alternative.\nPower thus measure ability detect real effects.\nFigure 2.6: Comparison null distribution (full curve) specific alternative t-test (dashed line). power corresponds area curve density alternative distribution rejection area (white).\n\nFigure 2.7: Increase power due increase mean difference null alternative hypothesis. Power area rejection region (white) alternative distribution (dashed): latter shifted right relative null distribution (full line).\n\nFigure 2.8: Increase power due increase sample size decrease standard deviation population: null distribution (full line) concentrated. Power given area (white) curve alternative distribution (dashed). general, null distribution changes sample size.\nwant choose experimental design test statistic leads high power, \\(\\gamma\\) close possible one. Minimally, power test \\(\\alpha\\) reject null hypothesis \\(\\alpha\\) fraction time even \\(\\mathscr{H}_0\\) true. Power depends many criteria, notablythe effect size: bigger difference postulated value \\(\\theta_0\\) \\(\\mathscr{H}_0\\) observed behaviour, easier departures \\(\\theta_0\\).\n(Figure 2.8); ’s easier spot elephant room mouse.variability: less noisy data, easier detect differences curves (big differences easier spot, Figure 2.7 shows);sample size: observation, higher ability detect significative differences standard error decreases sample size \\(n\\) rate (typically) \\(n^{-1/2}\\). null distribution also becomes concentrated sample size increase. experimental designs, power may maximized specifying different sample size groupthe choice test statistic: example, rank-based statistics discard information observed values response, focusing instead relative ranking. resulting tests typically less powerful, robust model misspecification outliers.calculate power test, need single specific alternative hypothesis. special case, analytic derivations possible. given alternative, wesimulate repeatedly samples model hypothetical alternative worldwe compute test statistic new sampleswe transform associated p-values based postulated null hypothesis.end, calculate proportion tests lead rejection null hypothesis level \\(\\alpha\\), namely percentage p-values smaller \\(\\alpha\\).","code":""},{"path":"hypothesis-testing.html","id":"confidence-interval","chapter":"2 Hypothesis testing","heading":"2.2.6 Confidence interval","text":"confidence interval alternative way present conclusions hypothesis test performed significance level \\(\\alpha\\). often combined point estimator \\(\\hat{\\theta}\\) give indication variability estimation procedure. Wald-based \\((1-\\alpha)\\) confidence intervals parameter \\(\\theta\\) form\n\\[\\begin{align*}\n\\widehat{\\theta} \\pm \\mathfrak{q}_{\\alpha/2} \\; \\mathrm{se}(\\widehat{\\theta})\n\\end{align*}\\]\n\\(\\mathfrak{q}_{\\alpha/2}\\) \\(1-\\alpha/2\\) quantile null distribution Wald statistic\n\\[\\begin{align*}\nT =\\frac{\\widehat{\\theta}-\\theta}{\\mathrm{se}(\\widehat{\\theta})},\n\\end{align*}\\]\n\\(\\theta\\) represents postulated value fixed, unknown value parameter. bounds confidence intervals random variables, since estimators parameter standard error, \\(\\widehat{\\theta}\\) \\(\\mathrm{se}(\\widehat{\\theta})\\), random variables: values vary one sample next.example, random sample \\(X_1, \\ldots, X_n\\) normal distribution \\(\\mathsf{}(\\mu, \\sigma)\\), (\\(1-\\alpha\\)) confidence interval population mean \\(\\mu\\) \n\\[\\begin{align*}\n\\overline{X} \\pm t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}}\n\\end{align*}\\]\n\\(t_{n-1,\\alpha/2}\\) \\(1-\\alpha/2\\) quantile Student-\\(t\\) distribution \\(n-1\\) degrees freedom.interval calculated, \\(1-\\alpha\\) probability \\(\\theta\\) contained random interval \\((\\widehat{\\theta} - \\mathfrak{q}_{\\alpha/2} \\; \\mathrm{se}(\\widehat{\\theta}), \\widehat{\\theta} + \\mathfrak{q}_{\\alpha/2} \\; \\mathrm{se}(\\widehat{\\theta}))\\), \\(\\widehat{\\theta}\\) denotes estimator. obtain sample calculate confidence interval, notion probability: true value parameter \\(\\theta\\) either confidence interval . can interpret confidence interval’s follows: repeat experiment multiple times, calculate \\(1-\\alpha\\) confidence interval time, roughly \\(1-\\alpha\\) calculated confidence intervals contain true value \\(\\theta\\) repeated samples (way, flip coin, roughly 50-50 chance getting heads tails, outcome either). confidence procedure use calculate confidence intervals actual values obtain sample.\nFigure 2.9: 95% confidence intervals mean standard normal population \\(\\mathsf{}(0,1)\\), 100 random samples. average, 5% intervals fail include true mean value zero (red).\ninterested binary decision rule reject/fail reject \\(\\mathscr{H}_0\\), confidence interval equivalent p-value since leads conclusion. Whereas \\(1-\\alpha\\) confidence interval gives set values test statistic doesn’t provide enough evidence reject \\(\\mathscr{H}_0\\) level \\(\\alpha\\), p-value gives probability null obtaining result extreme postulated value precise particular value. p-value smaller \\(\\alpha\\), null value \\(\\theta\\) outside confidence interval vice-versa.","code":""},{"path":"onewayanova.html","id":"onewayanova","chapter":"3 Completely randomized designs with one factor","heading":"3 Completely randomized designs with one factor","text":"chapter describes simply experiment, corresponding comparing \\(K\\) sub-populations differ treatment received.completely randomized experiments, interested effect treatment variables. start simplicity single variable \\(K\\) different treatments levels. global hypothesis interest testing whether mean outcome \\(K\\) different treatments sub-populations equal.basic assumption designs can decompose observation obtained two components (Cox 1958)\n\\[\\begin{align*}\n\\begin{pmatrix} \\text{quantity depending } \\\\ \n\\text{particular unit} \n\\end{pmatrix} + \n\\begin{pmatrix} \\text{quantity depending} \\\\\n \\text{treatment used}\\end{pmatrix}\n\\end{align*}\\]\nadditive decomposition assumes unit unaffected (independent ) treatment units average effect treatment constant.Let \\(\\mu_1, \\ldots, \\mu_K\\) denote expectation (theoretical mean) \\(K\\) sub-populations defined different treatments. Lack difference treatments equivalent equality means, translates hypotheses\n\\[\\begin{align*}\n\\mathscr{H}_0: & \\mu_1 = \\cdots = \\mu_K \\\\\n\\mathscr{H}_a: & \\text{least two treatments different averages, }\n\\end{align*}\\]\nnull hypothesis , usual, single numerical value: imposes \\(K-1\\) restrictions (number equality signs, value global mean \\(\\mu\\) left unspecified). alternative unique, since comprises potential scenarios expectations equal.","code":""},{"path":"onewayanova.html","id":"parametrization","chapter":"3 Completely randomized designs with one factor","heading":"3.1 Parametrization","text":"one-way analysis variance consists comparing average treatment group \\(T_1, \\ldots, T_K\\). Since \\(K\\) groups, \\(K\\) averages (one per group) estimate.One slight complication arising values \\(\\mu_1, \\ldots, \\mu_K\\) unknown. (theoretical unknown) average treatment \\(T_j\\) \\(\\mu_j\\) perhaps natural parametrization, although ill-suited hypothesis testing none \\(\\mu_i\\) values known practice.equivalent formulation writes treatment group average \\(\\mu_j = \\mu + \\delta_j\\), \\(\\delta_j\\) difference treatment average \\(\\mu_j\\) global average groups. however requires imposing constraint \\(\\delta_1 + \\cdots + \\delta_K=0\\) ensure average effects equals \\(\\mu\\).common parametrization terms constrasts, namely differences reference group (say \\(T_1\\)) group interest. thus \\(\\mu_1\\) average treatment \\(T_1\\) \\(\\mu_1 + a_i\\) treatment \\(T_i\\), \\(a_i=0\\) \\(T_1\\) \\(a_i = \\mu_i -\\mu_1\\) otherwise.can still assess hypothesis comparing sample means group, noisy estimates expectation: inherent variability limit ability detect differences mean signal--noise ratio small.","code":""},{"path":"onewayanova.html","id":"f-statistic-for-anova","chapter":"3 Completely randomized designs with one factor","heading":"3.2 F-statistic for ANOVA","text":"following section tries shed light \\(F\\)-test statistic works summary evidence: isn’t straightforward way appears case. null hypothesis, groups mean \\(\\mu\\) can compute overall average \\(\\widehat{\\mu}\\) group averages \\(\\widehat{\\mu}_1, \\ldots, \\widehat{\\mu}_K\\), \\(\\widehat{\\mu}_i\\) indicates sample average \\(\\)th group.\\(F\\)-statistic heuristically\n\\[\\begin{align}\nF = \\frac{\\text{-group variability}}{\\text{within-group variability}} \n\\tag{3.1}\n\\end{align}\\]\n-group variance squared differences overall mean \\(\\widehat{\\mu}\\) group mean observation \\(\\)th group \\(\\widehat{\\mu}_i\\), suitably rescaled. mean difference, numerator estimate population variance, denominator (3.1). many observations (relatively fewer groups), ratio approximately one average.difference mean, F-statistic follows large sample F-distribution, whose shape governed two parameters named degrees freedom. first number restrictions imposed null hypothesis (\\(K-1\\), number groups minus one), second number observations minus number mean parameters estimates (\\(n-K\\), \\(n\\) overall sample size \\(K\\) number groups).5","code":""},{"path":"reproducibility-crisis.html","id":"reproducibility-crisis","chapter":"4 Reproducibility crisis","heading":"4 Reproducibility crisis","text":"Defining replicability reproducibility.Understanding scale reproducibility crisis.Recognizing common statistical fallacies.Listing strategies enhancing reproducibility.adopt terminology Claerbout Karrenbach (1992): study said reproducible external person data enough indications procedure (example, code software versions, etc.) can obtain consistent results match paper. related scientific matter replicability, process new data collected test hypothesis, potentially using different methodology. Reproducibility important enhances credibility one’s work. Extensions deal different analyses leading conclusion described Turing Way presented 4.1.\nFigure 4.1: Definition different dimensions reproducible research (Turing Way project, illustration Scriberia).\nreproducibility important? thought provoking paper, Ioannidis (2005) claimed research findings wrong. abstract paper statedThere increasing concern current published research findings false. […] framework, research finding less likely true studies conducted field smaller; effect sizes smaller; greater number lesser preselection tested relationships; greater flexibility designs, definitions, outcomes, analytical modes; greater financial interest prejudice; teams involved scientific field chase statistical significance.Since publication, collaborative efforts tried assess scale reproducibility problem reanalysing data trying replicate findings published research. example, “Reproducibility Project: Psychology” (Nosek et al. 2015)conducted replications 100 experimental correlational studies published three psychology journals using high powered designs original materials available. Replication effects half magnitude original effects, representing substantial decline. Ninety seven percent original studies significant results. Thirty six percent replications significant results; 47% original effect sizes 95% confidence interval replication effect size; 39% effects subjectively rated replicated original result; , bias original results assumed, combining original replication results left 68% significant effects. […]large share findings review replicable effects much smaller claimed, shown Figure 2 study.\nfindings show peer-review procedure foolproof: “publish--perish” mindset academia leading many researchers try achieve statistical significance costs meet 5% level criterion, whether involuntarily . problem many names: \\(p\\)-hacking, harking paraphrase story Jorge Luis Borges, garden forking paths. many degrees freedom analysis researchers refine hypothesis viewing data, conducting many unplanned comparisons reporting selected results.\nFigure 4.2: Figure 2 Nosek et al. (2015), showing scatterplot effect sizes original replication study power, rugs density plots significance 5% level.\nAnother problem selective reporting. large emphasis placed statistical significance, many studies find small effects never published, resulting gap. Figure 4.3 Zwet Cator (2021) shows \\(z\\)-scores obtained transforming confidence intervals reported Barnett Wren (2019), used data mining techniques extract confidence intervals abstracts nearly one million publication Medline published 1976 2019.\nfinding published, \\(z\\)-scores normally distributed, Figure 4.3 shows big gap bell curve approximately \\(-2\\) \\(2\\).\nFigure 4.3: Figure Zwet Cator (2021) based results Barnett Wren (2019); histogram \\(z\\)-scores one million studies Medline.\nongoing debate surrounding reproducibility crisis sparked dramatic changes academic landscape: enhance quality studies published, many journal now require authors provide code data, pre-register studies, etc. Teams lead effort (e.g., Experimental Economics Replication Project) try replicate studies, mitigated success far. inside recollection graduate student shows extent problem.course place strong emphasis identifying avoiding statistical fallacies showcasing methods enhance reproducibility. can reproducible research enhance work? one thing, workflow facilitates publication negative research, forces researchers think ahead time (receive feedback). Reproducible research data availability also leads additional citations increased credibility scientist.Among good practices arepre-registration experiments use logbook.version control systems (e.g., Git) track changes files records.archival raw data proper format accompanying documentation.Keeping logbook documenting progress helps collaborators, reviewers future-self understand decisions may seem unclear arbitrary future, even result careful thought process time made . Given pervasiveness garden forking paths, pre-registration helps prevents harking limits selective reporting unplanned tests, panacea. Critics often object pre-registration claiming binds people. misleading claim view: pre-registration doesn’t mean must stick plan exactly, merely requires explain go planned.Version control keeps records changes file can help retrieve former versions make mistakes point.\nFigure 4.4: Tweet showing widespread problems related unintentional changes raw data software.\nArchival data helps avoid unintentional irreversible manipulations original data, examples can large scale consequences illustrated Figure 4.4 (Ziemann El-Osta 2016), report flaws genetic journals due automatic conversion gene names dates Excel. problems far unique sensible data shared “” confidentiality issues, many instances data can made available licence DOI allow people reuse, cite credit work.Operating open-science environment seen opportunity make better science, offer opportunities increase impact increase publication work regardless whether results turn negative. right thing increases quality research produced, collateral benefits forces researchers validate methodology , double-check data analysis adopt good practice.Reflect workflow applied researcher designing undertaking experiments. practical aspects improve upon improve reproducibility study?","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
