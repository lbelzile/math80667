[{"path":"index.html","id":"preliminary-remarks","chapter":"Preliminary remarks","heading":"Preliminary remarks","text":"notes licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License last compiled 2021-06-04.Download install R (current version 4.1.0, nicknamed “Camp Pontanezen”) integrated development environment RStudio.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"applied domains, empirical evidences drive advancement field data well designed experiments contribute\nbuilt science. order draw conclusions favour theory, researchers turn (often unwillingly) statistics back claims. led prevalence use null hypothesis statistical testing (NHST) framework prevalence \\(p\\)-values journal articles, despite fact falsification null hypothesis enough provide substantive findings theory.introductory statistics course often present hypothesis tests without giving much thoughts underlying construction principles procedures, users often reductive view statistics catalogue pre-determined procedures. make culinary analogy, users focus learning recipes rather trying understand basics cookery.objective teach basic principles experimental designs statistical inference data obtained designs using R programming language. pay particular attention correct reporting interpretation results learn review critically scientific papers using experimental designs.","code":""},{"path":"introduction.html","id":"experimental-designs-experimental-intro","chapter":"1 Introduction","heading":"1.1 Experimental designs {experimental-intro}","text":"field causal inference concerned inferring effect treatment variable (independent variable) response variable (dependent variable). simplest form, experimental design comparison two treatments (experimental conditions).subjects (experimental units) different groups treatment similar characteristics treated exactly way experimentation except treatment receiving.experimental treatments conditions (also called factor, independent variable), manipulated controlled researcher.different treatments administered subjects participating study, researcher measures one outcomes (also called responses dependent variables) subject.Observed difference outcome variable experimental conditions (treatments) called treatment effect (effect size). everything else well controlled experiment, treatment effect must caused experimental treatments.Richard McElreath first chapter book (McElreath 2020) draws parallel statistical tests golems (.e., robots): neitherdiscern context inapropriate answers. just knows procedure […] just ’s told.responsibility therefore lies user correctly use statistical procedures aware limitations: common research questions answered simple tools. Researchers wishing perform innovative methodological research contact experts consult statisticians collect data get information best proceed mind avoid risk making misleading false claims based incorrect analysis data collection.","code":""},{"path":"introduction.html","id":"reproducibility-crisis","chapter":"1 Introduction","heading":"1.2 The reproducibility crisis","text":"Defining replicability reproducibilityUnderstanding scale reproducibility crisisRecognizing common statistical fallaciesListing strategies enhancing reproducibilityA study said reproducible external person data enough indications procedure (example, providing code indications software versions, etc.) can obtain consistent results match paper. related scientific matter replicability, process new data collected test hypothesis, potentially using different methodology.thought provoking paper, Ioannidis (2005) claimed research findings wrong. abstract paper statedThere increasing concern current published research findings false. […] framework, research finding less likely true studies conducted field smaller; effect sizes smaller; greater number lesser preselection tested relationships; greater flexibility designs, definitions, outcomes, analytical modes; greater financial interest prejudice; teams involved scientific field chase statistical significance.Since publication, collaborative efforts tried assess scale reproducibility problem reanalysing data trying replicate findings published research. example, “Reproducibility Project: Psychology” (Nosek et al. 2015)conducted replications 100 experimental correlational studies published three psychology journals using high powered designs original materials available. Replication effects half magnitude original effects, representing substantial decline. Ninety seven percent original studies significant results. Thirty six percent replications significant results; 47% original effect sizes 95% confidence interval replication effect size; 39% effects subjectively rated replicated original result; , bias original results assumed, combining original replication results left 68% significant effects. […]large share findings review replicable effects much smaller claimed, shown Figure 2 study.\nfindings show peer-review procedure foolproof: “publish--perish” mindset academia leading many researchers try achieve statistical significance costs meet statistically significant 5% level criterion, whether involuntarily . problem many names: \\(p\\)-hacking, harking paraphrase story Jorge Luis Borges, garden forking paths. many degrees freedom analysis researchers refine hypothesis viewing data, conducting many unplanned comparisons reporting selected results.\nFigure 1.1: Figure 2 Nosek et al. (2015), showing scatterplot effect sizes original replication study power, rugs density plots significance 5% level.\nAnother problem selective reporting. large emphasis placed statistical significance, many studies find small effects never published, resulting gap. Figure 1.2 Zwet Cator (2021) shows \\(z\\)-scores obtained transforming confidence intervals reported Barnett Wren (2019), used data mining techniques extract confidence intervals abstracts nearly one million publication Medline published 1976 2019.\nfinding published, \\(z\\)-scores normally distributed, Figure 1.2 shows big gap bell curve approximately \\(-2\\) \\(2\\).\nFigure 1.2: Figure Zwet Cator (2021) based results Barnett Wren (2019); histogram \\(z\\)-scores one million studies Medline.\nongoing debate surrounding reproducibility crisis sparked dramatic changes academic landscape: enhance quality studies published, many journal now require authors provide code data, pre-register studies, etc. Teams lead effort (e.g., Experimental Economics Replication Project) try replicate studies, mitigate success. inside recollection graduate student shows extent problem.course place strong emphasis identifying avoiding statistical fallacies showcasing methods enhance reproducibility. can reproducible research enhance work? one thing, workflow facilitates publication negative research, forces researchers think ahead time (receive feedback). Reproducible research data availability also leads additional citations increased credibility scientist.Among good practices arepre-registration experiments use logbook.version control systems (e.g., Git) track changes files records.archival raw data proper format accompanying documentation.Keeping logbook documenting progress helps collaborators, reviewers future-self understand decisions may seem unclear arbitrary future, even result careful thought process time made . Given pervasiveness garden forking paths, pre-registration helps prevents harking limits selective reporting unplanned tests, panacea. Critics often object pre-registration claiming binds people. misleading claim view: pre-registration doesn’t mean must stick plan exactly, merely requires explain go planned.Version control keeps records changes file can help retrieve former versions make mistakes point.\nFigure 1.3: Tweet showing widespread problems related unintentional changes raw data software.\nArchival data helps avoid unintentional irreversible manipulations original data, examples can large scale consequences illustrated Figure 1.3 (Ziemann El-Osta 2016), report flaws genetic journals due automatic conversion gene names dates. problems far unique sensible data shared “” confidentiality issues, many instances data can made available licence DOI allow people reuse, cite credit work.Operating open-science environment seen opportunity make better science, offer opportunities increase impact increase publication work regardless whether results turn negative. right thing increases quality research produced, collateral benefits forces researchers validate methodology , double-check data analysis adopt good practice.Reflect workflow applied researcher designing undertaking experiments. practical aspects improve upon improve reproducibility study?","code":""},{"path":"introduction.html","id":"planning-experiments","chapter":"1 Introduction","heading":"1.3 Planning of experiments","text":"outline various steps research must undertake experimental setting.","code":""},{"path":"introduction.html","id":"population-sample","chapter":"1 Introduction","heading":"1.4 Population and samples","text":"Generally, seek estimate characteristics population using sample (sub-group population smaller size). population interest collection individuals study targets. example, Labour Force Survey (LFS) monthly study conducted Statistics Canada, define target population “members selected household 15 years old older, whether work .” Asking every Canadian meeting definition costly process long: characteristic interest (employment) also snapshot time can vary person leaves job, enters job market become unemployed.general, therefore consider samples gather information seek obtain. purpose statistical inference draw conclusions population, using share latter accounting sources variability. pollster George Gallup made great analogy sample population:One spoonful can reflect taste whole pot, soup well-stirredA sample sub-group individuals drawn random population. won’t focus data collection, keep mind following information: sample good, must representative population study.individuals selected random part sample, measurement characteristic interest also random change one sample next. larger samples typically carry information, sample size guarantee quality, following example demonstrates.sampling costly, can collect limited information variable interest. Experimental design revolves large part understanding best allocate resources attain specified goal.","code":""},{"path":"introduction.html","id":"sources-of-variability","chapter":"1 Introduction","heading":"1.5 Sources of variability","text":"call summaries \\(T\\) data statistics, compress information contained sample summary. example, sample mean \\(\\overline{Y}=n^{-1}(Y_1 + \\cdots + Y_n)\\) function data estimator population mean \\(\\mu\\). inputs function \\(\\overline{Y}\\) random, resulting estimator also random illustrate point .Figure 1.4 shows five simple random samples size \\(n=10\\) drawn hypothetical population mean \\(\\mu\\) standard deviation \\(\\sigma\\).\nFigure 1.4: Five samples size \\(n=10\\) drawn common population mean \\(\\mu\\) (horizontal line). colored segments show sample means sample.\ncan clearly see Figure 1.4 sample mean varies one sample next result sampling variability. astute eye also notice sample means less dispersed around \\(\\mu\\) individual measurements. sample mean \\(\\overline{Y}\\) based multiple observations information accumulates.Simply looking values sample mean tell whole picture: must also consider variability. square root variance statistic termed standard error; confused standard deviation \\(\\sigma\\) population \\(Y\\) drawn. One can show standard error sample mean \\(\\mathsf{se}(\\overline{Y}) = \\sigma/\\sqrt{n}\\). standard deviation standard error expressed units measurements, easier interpret variance.next section, outline hypothesis testing helps us disentangle signal noise.","code":""},{"path":"introduction.html","id":"tests","chapter":"1 Introduction","heading":"1.6 Hypothesis testing","text":"hypothesis test binary decision rule used evaluate statistical evidence provided sample make decision regarding underlying population. main steps involved :define model parametersformulate alternative null hypothesischoose calculate test statisticobtain null distribution describing behaviour test statistic \\(\\mathscr{H}_0\\)calculate p-valueconclude (reject fail reject \\(\\mathscr{H}_0\\)) context problem.good analogy hypothesis tests trial murder appointed juror.judge lets choose two mutually exclusive outcome, guilty guilty, based evidence presented court.presumption innocence applies evidences judged optic: evidence remotely plausible person innocent? burden proof lies prosecution avoid much possible judicial errors. null hypothesis \\(\\mathscr{H}_0\\) guilty, whereas alternative \\(\\mathscr{H}_a\\) guilty. reasonable doubt, verdict trial guilty.test statistic (choice test) represents summary proof. overwhelming evidence, higher chance accused declared guilty. prosecutor chooses proof best outline : choice evidence (statistic) ultimately maximize evidence, parallels power test.null distribution benchmark judge evidence (jurisprudence). Given proof, odds assuming person innocent?final step verdict. binary decision, guilty guilty. hypothesis test performed level \\(\\alpha\\), one reject (guilty) p-value less \\(\\alpha\\). Even declare person guilty, doesn’t mean defendant innocent vice-versa.","code":""},{"path":"introduction.html","id":"hypothesis","chapter":"1 Introduction","heading":"1.6.1 Hypothesis","text":"statistical tests two hypotheses: null hypothesis (\\(\\mathscr{H}_0\\)) alternative hypothesis (\\(\\mathscr{H}_1\\)). Usually, null hypothesis single numerical value (‘status quo’) alternative ’re really interested testing. statistical hypothesis test allows us decide whether data provides enough evidence reject \\(\\mathscr{H}_0\\) favour \\(\\mathscr{H}_1\\), subject pre-specified risk error. Usually, hypothesis tests involve parameter, say \\(\\theta\\), characterizes underlying distribution population level ans whose value unknown. two-sided hypothesis test regarding parameter \\(\\theta\\) form\n\\[\\begin{align*}\n\\mathscr{H}_0: \\theta=\\theta_0 \\qquad \\text{versus} \\qquad \\mathscr{H}_a:\\theta \\neq \\theta_0.\n\\end{align*}\\]\ntesting whether \\(\\theta\\) precisely equal value \\(\\theta_0\\).completely randomized experiments single factor, testing whether mean \\(K\\) different sub-populations equal. Let \\(\\mu_1, \\ldots, \\mu_K\\) denote expectation theoretical mean \\(K\\) sub-populations. Equality means translates \n\\[\\begin{align*}\n\\mathscr{H}_0:& \\mu_1 = \\cdots = \\mu_K\n\\mathscr{H}_a:& \\text{least two means different, }\\mu_i \\neq \\mu_j (1 \\leq < j \\leq K).\n\\end{align*}\\]\nNote null hypothesis single value, whereas alternative complement, .e. potential scenarios expectations equal.One slight complication arising expectations \\(\\mu_1, \\ldots, \\mu_K\\) unknown. can assess comparing sample means group. noisy estimates expectation: inherent variability limits ability detect differences mean.","code":""},{"path":"introduction.html","id":"test-statistic","chapter":"1 Introduction","heading":"1.6.2 Test statistic","text":"test statistic \\(T\\) function data summarize information contained sample \\(\\theta\\). form test statistic chosen know underlying distribution \\(\\mathscr{H}_0\\), , potential values taken \\(T\\) relative probability \\(\\mathscr{H}_0\\) true. Indeed, \\(Y\\) random variable value change one sample next.\nallows us determine values \\(T\\) likely \\(\\mathscr{H}_0\\) true. Many statistics consider Wald statistic, form\n\\[\\begin{align*}\nT = \\frac{\\widehat{\\theta} - \\theta_0}{\\mathrm{se}(\\widehat{\\theta})}\n\\end{align*}\\]\n\\(\\widehat{\\theta}\\) estimator \\(\\theta\\), \\(\\theta_0\\) postulated value parameter \\(\\mathrm{se}(\\widehat{\\theta})\\) estimator standard deviation test statistic \\(\\widehat{\\theta}\\).example, test whether mean population zero, set\n\\[\\begin{align*}\n\\mathscr{H}_0: \\mu=0, \\qquad  \\mathscr{H}_a:\\mu \\neq 0,\n\\end{align*}\\]\nWald statistic \n\\[\\begin{align*}\nT &= \\frac{\\overline{X}-0}{S_n/\\sqrt{n}}\n\\end{align*}\\]\n\\(\\overline{X}\\) sample mean \\(X_1, \\ldots, X_n\\),\n\\[\\begin{align*}\n\\overline{X} &= \\frac{1}{n} \\sum_{=1}^n X_i = \\frac{X_1+ \\cdots + X_n}{n}\n\\end{align*}\\]\nstandard error (mean) \\(\\overline{X}\\) \\(S_n/\\sqrt{n}\\); sample variance \\(S_n\\) estimator standard deviation \\(\\sigma\\),\n\\[\\begin{align*}\nS^2_n &= \\frac{1}{n-1} \\sum_{=1}^n (X_i-\\overline{X})^2.\n\\end{align*}\\]important distinguish procedures/formulas numerical values. estimator rule formula used calculate estimate parameter quantity interest based observed data. example, sample mean \\(\\bar{X}\\) estimator population mean \\(\\mu\\). observed data can actually compute sample mean, , estimate — actual value. words,estimator procedure formula telling us use sample data compute estimate. random variable since depends sample.estimate numerical value obtained apply formula observed data","code":""},{"path":"introduction.html","id":"null-distribution-and-p-value","chapter":"1 Introduction","heading":"1.6.3 Null distribution and p-value","text":"p-value allows us decide whether observed value test statistic \\(T\\) plausible \\(\\mathscr{H}_0\\). Specifically, p-value probability test statistic equal extreme estimate computed data, assuming \\(\\mathscr{H}_0\\) true. Suppose based random sample \\(X_1, \\ldots, X_n\\) obtain statistic whose value \\(T=t\\). two-sided test \\(\\mathscr{H}_0:\\theta=\\theta_0\\) vs. \\(\\mathscr{H}_a:\\theta \\neq \\theta_0\\), p-value \\(\\mathsf{Pr}_0(|T| \\geq |t|)\\). distribution \\(T\\) symmetric around zero, p-value \n\\[\\begin{align*}\np = 2 \\times \\mathsf{Pr}_0(T \\geq |t|).\n\\end{align*}\\]Consider example two-sided test involving population mean \\(\\mathscr{H}_0:\\mu=0\\) alternative \\(\\mathscr{H}_1:\\mu \\neq 0\\). Assuming random sample comes normal (population) \\(\\mathsf{}(\\mu, \\sigma^2)\\), can shown \\(\\mathscr{H}_0\\) true (, \\(\\mu=0\\)), test statistic\n\\[\\begin{align*}\nT = \\frac{\\overline{X}}{S/\\sqrt{n}}\n\\end{align*}\\]\nfollows Student-t distribution \\(n-1\\) degrees freedom, denoted \\(\\mathsf{St}_{n-1}\\). allows us calculate p-value (either table, using statistical software). Student-t distribution symmetric zero, p-value \\(P = 2\\times\\mathsf{Pr}(T_{n-1} > |t|)\\), \\(T \\sim \\mathsf{St}_{n-1}\\).","code":""},{"path":"introduction.html","id":"conclusion","chapter":"1 Introduction","heading":"1.6.4 Conclusion","text":"p-value allows us make decision null hypothesis. \\(\\mathscr{H}_0\\) true, p-value follows uniform distribution. Thus, p-value small, means observing outcome extreme \\(T=t\\) unlikely, ’re inclined think \\(\\mathscr{H}_0\\) true. ’s always underlying risk ’re making mistake make decision. statistic, two type errors:type error: reject \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) true,type II error: fail reject \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) false.hypothesis judged equally: seek avoid error type (judicial errors, corresponding condamning innocent). prevent , fix level test, \\(\\alpha\\), captures tolerance risk commiting type error: higher level test \\(\\alpha\\), often reject null hypothesis latter true. value \\(\\alpha \\(0, 1)\\) probability rejecting \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) fact true,\n\\[\\begin{align*}\n\\alpha = \\mathsf{Pr}_0\\left(\\text{ reject } \\mathscr{H}_0\\right).\n\\end{align*}\\]\nlevel \\(\\alpha\\) fixed beforehand, typically \\(1\\)%, \\(5\\)% \\(10\\)%. Keep mind probability type error \\(\\alpha\\) null model \\(\\mathscr{H}_0\\) correct (sic) correspond data generating mechanism.focus type error best understood thinking costs moving away status quo: new website design branding costly implement, want make sure enough evidence better alternative.make decision, compare p-value \\(P\\) level test \\(\\alpha\\):\\(P < \\alpha\\), reject \\(\\mathscr{H}_0\\);\\(P \\geq \\alpha\\), fail reject \\(\\mathscr{H}_0\\).mix level test (probability fixed beforehand researcher) p-value. test level 5%, probability type error definition \\(\\alpha\\) depend p-value. latter conditional probability observing extreme likelihood given null distribution \\(\\mathscr{H}_0\\) true.","code":""},{"path":"introduction.html","id":"power","chapter":"1 Introduction","heading":"1.6.5 Power","text":"two sides hypothesis test: either want show unreasonable assume null hypothesis, else want show beyond reasonable doubt difference effect significative: example, one wish demonstrate new website design (alternative hypothesis) leads significant increase sales relative status quo. ability detect improvements make discoveries depends power test: larger power, greater ability reject \\(\\mathscr{H}_0\\) latter false.Failing reject \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_a\\) true corresponds definition type II error, probability \\(1-\\gamma\\), say. power test probability rejecting \\(\\mathscr{H}_0\\) \\(\\mathscr{H}_0\\) false, .e.,\n\\[\\begin{align*}\n\\gamma = \\mathsf{Pr}_a(\\text{reject} \\mathscr{H}_0)\n\\end{align*}\\]\nDepending alternative models, less easy detect null hypothesis false reject favor alternative.\nFigure 1.5: Comparison null distribution (full curve) specific alternative t-test (dashed line). power corresponds area curve density alternative distribution rejection area (white).\n\nFigure 1.6: Increase power due increase mean difference null alternative hypothesis. Power area rejection region (white) alternative distribution (dashed): latter shifted right relative null distribution (full line).\n\nFigure 1.7: Increase power due increase sample size decrease standard deviation population: null distribution (full line) concentrated. Power given area (white) curve alternative distribution (dashed). general, null distribution changes sample size.\nwant test high power, .e., \\(\\gamma\\) close 1 possible. Minimally, power test \\(\\alpha\\) reject null hypothesis \\(\\alpha\\) fraction time even \\(\\mathscr{H}_0\\) true. Power depends many criteria, notablythe effect size: bigger difference postulated value \\(\\theta_0\\) \\(\\mathscr{H}_0\\) observed behavior, easier detect .\n(Figure 1.7);variability: less noisy data, easier detect differences curves (big differences easier spot, Figure 1.6 shows);sample size: observation, higher ability detect significative differences standard error decreases sample size \\(n\\) rate (typically) \\(n^{-1/2}\\). null distribution also becomes concentrated sample size increase.choice test statistic: example, rank-based statistics discard information actual values care relative ranking. Resulting tests less powerful, typically robust model misspecification outliers. statistics choose standard amongst powerful: , won’t dwell factor.calculate power test, need single specific alternative hypothesis. special case, analytic derivations possible: example, one-sample t-test statistic \\(T=\\sqrt{n}(\\overline{X}_n-\\mu_0)/S_n \\sim \\mathcal{T}_{n-1}\\) normal sample follows noncentral Student-\\(t\\) distribution noncentrality parameter \\(\\Delta\\) expectation population \\(\\Delta + \\mu_0\\). general, closed-form expressions easily obtained compute instead power test Monte Carlo methods. given alternative, simulate repeatedly samples model, compute test statistic new samples associated p-values based postulated null hypothesis. can calculate proportion tests lead rejection null hypothesis level \\(\\alpha\\), namely percentage p-values smaller \\(\\alpha\\).","code":""},{"path":"introduction.html","id":"confidence-interval","chapter":"1 Introduction","heading":"1.6.6 Confidence interval","text":"confidence interval alternative way present conclusions hypothesis test performed significance level \\(\\alpha\\). often combined point estimator \\(\\hat{\\theta}\\) give indication variability estimation procedure. Wald-based \\((1-\\alpha)\\) confidence intervals parameter \\(\\theta\\) form\n\\[\\begin{align*}\n\\widehat{\\theta} \\pm \\mathfrak{q}_{\\alpha/2} \\; \\mathrm{se}(\\widehat{\\theta})\n\\end{align*}\\]\n\\(\\mathfrak{q}_{\\alpha/2}\\) \\(1-\\alpha/2\\) quantile null distribution Wald statistic\n\\[\\begin{align*}\nT =\\frac{\\widehat{\\theta}-\\theta}{\\mathrm{se}(\\widehat{\\theta})},\n\\end{align*}\\]\n\\(\\theta\\) represents postulated value fixed, unknown value parameter. bounds confidence intervals random variables, since \\(\\widehat{\\theta}\\) \\(\\mathrm{se}(\\widehat{\\theta})\\) random variables: values depend sample, vary one sample another.example, random sample \\(X_1, \\ldots, X_n\\) normal distribution \\(\\mathsf{}(\\mu, \\sigma)\\), (\\(1-\\alpha\\)) confidence interval population mean \\(\\mu\\) \n\\[\\begin{align*}\n\\overline{X} \\pm t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}}\n\\end{align*}\\]\n\\(t_{n-1,\\alpha/2}\\) \\(1-\\alpha/2\\) quantile Student-\\(t\\) distribution \\(n-1\\) degrees freedom.interval calculated, \\(1-\\alpha\\) probability \\(\\theta\\) contained random interval \\((\\widehat{\\theta} - \\mathfrak{q}_{\\alpha/2} \\; \\mathrm{se}(\\widehat{\\theta}), \\widehat{\\theta} + \\mathfrak{q}_{\\alpha/2} \\; \\mathrm{se}(\\widehat{\\theta}))\\), \\(\\widehat{\\theta}\\) denotes estimator. obtain sample calculate confidence interval, notion probability: true value parameter \\(\\theta\\) either confidence interval . can interpret confidence interval’s follows: repeat experiment multiple times, calculate \\(1-\\alpha\\) confidence interval time, roughly \\(1-\\alpha\\) calculated confidence intervals contain true value \\(\\theta\\) repeated samples (way, flip coin, roughly 50-50 chance getting heads tails, outcome either). confidence procedure use calculate confidence intervals actual values obtain sample.\nFigure 1.8: 95% confidence intervals mean standard normal population \\(\\mathsf{}(0,1)\\), 100 random samples. average, 5% intervals fail include true mean value zero (red).\ninterested binary decision rule reject/fail reject \\(\\mathscr{H}_0\\), confidence interval equivalent p-value since leads conclusion. Whereas \\(1-\\alpha\\) confidence interval gives set values test statistic doesn’t provide enough evidence reject \\(\\mathscr{H}_0\\) level \\(\\alpha\\), p-value gives probability null obtaning result extreme postulated value precise particular value. p-value smaller \\(\\alpha\\), null value \\(\\theta\\) outside confidence interval vice-versa.example, consider difference average amount spent Y members previous generations: mean difference samples -16.49 dollars thus millenials spend . However, enough conclude different significative, can say meaningful. amount spent online varies one individual next (plausibly month month), different random samples yield different mean differences.first step analysis defining parameters corresponding quantities interest formulating null alternative hypothesis function parameters. consider test difference mean two populations, say \\(\\mu_1\\) expected amount spent generation Y \\(\\mu_2\\) older generations, respective standard errors \\(\\sigma_1\\) \\(\\sigma_2\\). next write hypothesis: researcher interested whether millenials spend , alternative hypothesis, \\(\\mathscr{H}_a: \\mu_1 > \\mu_2\\). null consists values \\(\\mathscr{H}_0: \\mu_1 \\leq \\mu_2\\), \\(\\mu_1=\\mu_2\\) matters purpose testing (?)second step choice test statistic. consider Welch (1947) statistic difference mean two samples,\n\\[\\begin{align*}\nT = \\frac{\\overline{X}_1 - \\overline{X}_2}{\\left(\\frac{S_1^2}{n_1}+\\frac{S_2^2}{n_2} \\right)^{1/2}}, \\end{align*}\\]\n\\(\\overline{X}_i\\) sample mean, \\(S_i^2\\) unbiased variance estimator \\(n_i\\) sample size group \\(\\) (\\(=1, 2\\)). mean difference two samples zero, \\(\\overline{X}_1-\\overline{X}_2\\) mean zero difference variance \\(\\sigma^2_1/n_1+\\sigma^2_2/n_2\\). sample, value statistic \\(T=-2.76\\) Since value changes one sample next, need determine value compatible null hypothesis comparing null distribution \\(T\\) (\\(\\mathscr{H}_0\\) true \\(\\mu_1-\\mu_2=0\\)). perform test level \\(\\alpha=0.05\\).third step consists obtaining benchmark determine result extreme unusual. make comparisons easier, standardize statistic mean zero variance one null hypothesis \\(\\mu_1=\\mu_2\\), obtain dimensionless measure whose behaviour know large sample. (mathematical) derivation null distribution beyond scope course, given cases. Asymptotically, \\(T\\) follows standard normal distribution \\(\\mathsf{}(0, 1)\\), exists better finite-sample approximation \\(n_1\\) \\(n_2\\) small; use Satterthwaite (1946) Student-\\(t\\) distribution null distribution.remains compute p-value. null distribution well-specified \\(\\mathscr{H}_0\\) true, random variable \\(P\\) uniform \\([0, 1]\\); thus expect obtain null something larger 0.95 5% time one-sided alternative since consider \\(\\mathscr{H}_0\\) event \\(\\mathsf{Pr}(T > t)\\). \\(p\\)-value \\(1\\) , level 5%, reject null hypothesis conclude millenials spend significantly previous generation monthly online purchases, estimated average difference -16.49.","code":""},{"path":"onewayanova.html","id":"onewayanova","chapter":"2 Completely randomized designs with one factor","heading":"2 Completely randomized designs with one factor","text":"Example application + citationThinking outside box:potential confounders?reweighting using demographic informationThe null hypothesis one-way analysis variance (ANOVA) problem \\(\\mathscr{H}_0: \\mu_1 = \\mu_2 = \\mu_3\\), \\(\\mu_i\\) \\((=1, \\ldots, 3)\\) expectation group \\(\\).HomogeneityNull hypothesis comparing means multiple groups\nAssumptions\nPlot data\nData compression: need report?\nEXPERT TIPS: visualisation tools, histograms line charts ordered factors.Reminder: must handling confounding via randomization blocking (explicitly)get power ? Decomposition sum squares TOTAL = + WITHINAnova table\nhappens null\nhappens alternative: illustrations simulated dataPotential ways assessing null hypothesis\n- assumptions making process? draw power \nAlternative test statistics: Welch permutation (validity)\nPower considerations: choosing sample size\n- estimate effect size, type error, number levels (subsample size), variabilityCareful observed power (using observed effect size calculation)\nPractical significance versus statistical significanceThinking outside box: allocate sample sizes equal varianceWhat unbalanced experiments?\ncan go WRONG?\n- experiment properly conducted 2.4.6 Lawson (2014)\n- lurking variables: effective teaching methods, confounded instructor/class time/voluntary participation\n-> independence errors unit\n-> measurements normal (continuous)\n- need experimental units large enough sample sizes\n- Example variance increasing together level response","code":""},{"path":"onewayanova.html","id":"contrasts-and-pairwise-tests","chapter":"2 Completely randomized designs with one factor","heading":"2.1 Contrasts and pairwise tests","text":"Reparametrizing model\nStatistical fallacy: data dredging\nMultiple comparisons family-wise error rates\nPreplanned experiments: penalie large studies\nmuch data need reliably estimate\nMethods controlling multiplicityThinking outside box: free lunch.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
