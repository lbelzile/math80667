<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>4 Replication crisis | Experimental Design and Statistical Methods</title>
<meta name="author" content="Léo Belzile">
<meta name="description" content="In recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="4 Replication crisis | Experimental Design and Statistical Methods">
<meta property="og:type" content="book">
<meta property="og:description" content="In recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="4 Replication crisis | Experimental Design and Statistical Methods">
<meta name="twitter:description" content="In recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Experimental Design and Statistical Methods</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Experimental Design and Statistical Methods</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">2</span> Hypothesis testing</a></li>
<li><a class="" href="CRT.html"><span class="header-section-number">3</span> Completely randomized designs</a></li>
<li><a class="active" href="replication-crisis.html"><span class="header-section-number">4</span> Replication crisis</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/lbelzile/math80667a">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="replication-crisis" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Replication crisis<a class="anchor" aria-label="anchor" href="#replication-crisis"><i class="fas fa-link"></i></a>
</h1>
<p>In recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications failed to yield anything like what authors used to claim, or found much weaker findings. This chapter examines some of the causes of this lack of replicability.</p>
<div class="keyidea">
<ul>
<li>Defining replicability and reproducibility.</li>
<li>Understanding the scale of the replication crisis.</li>
<li>Recognizing common statistical fallacies.</li>
<li>Listing strategies for enhancing reproducibility.</li>
</ul>
</div>
<p>We adopt the terminology of <span class="citation">Claerbout and Karrenbach (<a href="references.html#ref-Claerbout/Karrenbach:1992" role="doc-biblioref">1992</a>)</span>: a study is said to be <strong>reproducible</strong> if an external person with the same data and enough indications about the procedure (for example, the code and software versions, etc.) can obtain consistent results that match those of a paper. A related scientific matter is <strong>replicability</strong>, which is the process by which new data are collected to test the same hypothesis, potentially using different methodology. Reproducibility is important because it enhances the credibility of one’s work. Extensions that deal with different analyses leading to the same conclusion are described in <a href="%5Bhttps://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html%5D">The Turing Way</a> and presented in <a href="replication-crisis.html#fig:turingrepdo">4.1</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:turingrepdo"></span>
<img src="figures/turing_way.jpg" alt="Definition of different dimensions of reproducible research (from The Turing Way project, illustration by Scriberia)." width="85%"><p class="caption">
Figure 4.1: Definition of different dimensions of reproducible research (from The Turing Way project, illustration by Scriberia).
</p>
</div>
<p>Why is reproducibility and replicability important? In a thought provoking paper, <span class="citation">Ioannidis (<a href="references.html#ref-Ioannidis:2005" role="doc-biblioref">2005</a>)</span> claimed that most research findings are wrong. The abstract of his paper stated</p>
<blockquote>
<p>There is increasing concern that most current published research findings are false. […] In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance.</p>
</blockquote>
<p>Since its publication, collaborative efforts have tried to assess the scale of the problem by reanalysing data and trying to replicate the findings of published research. For example, the “Reproducibility [sic] Project: Psychology” <span class="citation">(<a href="references.html#ref-Nosek:2015" role="doc-biblioref">Nosek et al. 2015</a>)</span></p>
<blockquote>
<p>conducted replications of 100 experimental and correlational studies published in three psychology journals using high powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety seven percent of original studies had significant results. Thirty six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and, if no bias in original results is assumed, combining original and replication results left 68% with significant effects. […]</p>
</blockquote>
<p>A large share of findings in the review were not replicable or the effects were much smaller than claimed, as shown by <a href="https://osf.io/447b3/">Figure 2 from the study</a>.
Such findings show that the peer-review procedure is not foolproof: the “publish-or-perish” mindset in academia is leading many researchers to try and achieve statistical significance at all costs to meet the 5% level criterion, whether involuntarily or not. This problem has many names: <span class="math inline">\(p\)</span>-hacking, harking or to paraphrase a <a href="https://en.wikipedia.org/wiki/The_Garden_of_Forking_Paths">story of Jorge Luis Borges</a>, the garden of forking paths. There are many degrees of freedom in the analysis for researchers to refine their hypothesis after viewing the data, conducting many unplanned comparisons and reporting selected results.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:repropvaluescorr"></span>
<img src="figures/RPP_psycho_repro.png" alt="Figure 2 from @Nosek:2015, showing scatterplot of effect sizes for the original and the replication study by power, with rugs and density plots by significance at the 5% level." width="85%"><p class="caption">
Figure 4.2: Figure 2 from <span class="citation">Nosek et al. (<a href="references.html#ref-Nosek:2015" role="doc-biblioref">2015</a>)</span>, showing scatterplot of effect sizes for the original and the replication study by power, with rugs and density plots by significance at the 5% level.
</p>
</div>
<p>Another problem is selective reporting. Because a large emphasis is placed on statistical significance, many studies that find small effects are never published, resulting in a gap. Figure <a href="replication-crisis.html#fig:reprozscores">4.3</a> from <span class="citation">Zwet and Cator (<a href="references.html#ref-vanZwet:2021" role="doc-biblioref">2021</a>)</span> shows <span class="math inline">\(z\)</span>-scores obtained by transforming confidence intervals reported in <span class="citation">Barnett and Wren (<a href="references.html#ref-Barnett:2019" role="doc-biblioref">2019</a>)</span>. The authors used data mining techniques to extract confidence intervals from abstracts of nearly one million publication in Medline published between 1976 and 2019. If most experiments yielded no effect and were due to natural variability, the <span class="math inline">\(z\)</span>-scores should be normally distributed, but Figure <a href="replication-crisis.html#fig:reprozscores">4.3</a> shows a big gap in the bell curve between approximately <span class="math inline">\(-2\)</span> and <span class="math inline">\(2\)</span>, indicative of selective reporting. The fact that results that do not lead to <span class="math inline">\(p &lt; 0.05\)</span> are not published is called the <strong>file-drawer</strong> problem.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:reprozscores"></span>
<img src="figures/vanZwet_Cator-zvalues.png" alt="Figure from @vanZwet:2021 based on results of @Barnett:2019; histogram of $z$-scores from one million studies from Medline." width="85%"><p class="caption">
Figure 4.3: Figure from <span class="citation">Zwet and Cator (<a href="references.html#ref-vanZwet:2021" role="doc-biblioref">2021</a>)</span> based on results of <span class="citation">Barnett and Wren (<a href="references.html#ref-Barnett:2019" role="doc-biblioref">2019</a>)</span>; histogram of <span class="math inline">\(z\)</span>-scores from one million studies from Medline.
</p>
</div>
<p>The ongoing debate surrounding the reproducibility crisis has sparked dramatic changes in the academic landscape: to enhance the quality of studies published, many journal now require authors to provide their code and data, to pre-register their studies, etc. Teams lead effort (e.g., the <a href="https://experimentaleconreplications.com/studies.html">Experimental Economics Replication Project</a>) try to replicate studies, with mitigated success so far. This <a href="https://devonprice.medium.com/questionable-research-practices-ive-taken-part-in-754b74dcaa51">inside recollection</a> by a graduate student shows the extent of the problem.</p>
<p>This course will place a strong emphasis on identifying and avoiding statistical fallacies and showcasing methods than enhance reproducibility. How can reproducible research enhance your work? For one thing, this workflow facilitates the publication of negative research, forces researchers to think ahead of time (and receive feedback). Reproducible research and data availability also leads to additional citations and increased credibility as a scientist.</p>
<p>Among good practices are</p>
<ul>
<li>pre-registration of experiments and use of a logbook.</li>
<li>clear reporting of key aspects of the experiment (choice of metric, number of items in a Likert scale, etc.)</li>
<li>version control systems (e.g., Git) that track changes to files and records.</li>
<li>archival of raw data in a proper format with accompanying documentation.</li>
</ul>
<p>Keeping a logbook and documenting your progress helps your collaborators, reviewers and your future-self understand decisions which may seem unclear and arbitrary in the future, even if they were the result of a careful thought process at the time you made them. Given the pervasiveness of the garden of forking paths, pre-registration helps you prevents harking because it limits selective reporting and unplanned tests, but it is not a panacea. Critics often object to pre-registration claiming that it binds people. This is a misleading claim in my view: pre-registration doesn’t mean that you must stick with the plan exactly, but merely requires you to explain what did not go as planned if anything.</p>
<p>Version control keeps records of changes to your file and can help you retrieve former versions if you make mistakes at some point.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:reprotweetexcelgenes"></span>
<img src="figures/reproducibility.png" alt="Tweet showing widespread problems related to unintentional changes to raw data by software." width="85%"><p class="caption">
Figure 4.4: Tweet showing widespread problems related to unintentional changes to raw data by software.
</p>
</div>
<p>Archival of data helps to avoid unintentional and irreversible manipulations of the original data, examples of which can have large scale consequences as illustrated in Figure <a href="replication-crisis.html#fig:reprotweetexcelgenes">4.4</a> <span class="citation">(<a href="references.html#ref-Ziemann:2016" role="doc-biblioref">Ziemann and El-Osta 2016</a>)</span>, who report flaws in genetic journals due to the automatic conversion of gene names to dates in Excel. These problems are <a href="https://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england">far from unique</a>. While sensitive data cannot be shared “as is” because of confidentiality issues, in many instances the data can and should be made available with a licence and a DOI to allow people to reuse it, cite and credit your work.</p>
<p>To enforce reproducibility, many journals now have policy regarding data, material and code availability. Some journals encourage such, while the trend in recent years has been to enforce. For example, Nature require the following to be reported in all published papers:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:naturereportstat"></span>
<img src="figures/Nature_reporting_statistics.png" alt="Screenshot of the Nature Reporting summary for statistics, reproduced under the CC BY 4.0 license." width="85%"><p class="caption">
Figure 4.5: Screenshot of the Nature Reporting summary for statistics, reproduced under the CC BY 4.0 license.
</p>
</div>
<p>Operating in an open-science environment should be seen as an opportunity to make better science, offer more opportunities to increase your impact and increase the likelihood that your work gets published regardless of whether the results turn out to be negative. It is the <em>right thing</em> to do and it increases the quality of research produced, with collateral benefits because it forces researchers to validate their methodology before, to double-check their data and their analysis and to adopt good practice.</p>
<div class="yourturn">
<p>Reflect on your workflow as applied researcher when designing and undertaking experiments. Which practical aspects could you improve upon to improve the reproducibility of your study?</p>
</div>

</div>
  <div class="chapter-nav">
<div class="prev"><a href="CRT.html"><span class="header-section-number">3</span> Completely randomized designs</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav"><li><a class="nav-link" href="#replication-crisis"><span class="header-section-number">4</span> Replication crisis</a></li></ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/lbelzile/math80667a/blob/master/04-reproducibility_crisis.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/lbelzile/math80667a/edit/master/04-reproducibility_crisis.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Experimental Design and Statistical Methods</strong>" was written by Léo Belzile. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
