<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>8 Replication crisis | Experimental Design and Statistical Methods</title>
<meta name="author" content="Léo Belzile">
<meta name="description" content="In recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications...">
<meta name="generator" content="bookdown 0.29 with bs4_book()">
<meta property="og:title" content="8 Replication crisis | Experimental Design and Statistical Methods">
<meta property="og:type" content="book">
<meta property="og:description" content="In recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="8 Replication crisis | Experimental Design and Statistical Methods">
<meta name="twitter:description" content="In recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Experimental Design and Statistical Methods</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Experimental Design and Statistical Methods</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">2</span> Hypothesis testing</a></li>
<li><a class="" href="CRT.html"><span class="header-section-number">3</span> Completely randomized designs</a></li>
<li><a class="" href="contrasts-multiple-testing.html"><span class="header-section-number">4</span> Contrasts and multiple testing</a></li>
<li><a class="" href="complete-factorial-designs.html"><span class="header-section-number">5</span> Complete factorial designs</a></li>
<li><a class="" href="designs-to-reduce-the-error.html"><span class="header-section-number">6</span> Designs to reduce the error</a></li>
<li><a class="" href="effect-sizes-and-power.html"><span class="header-section-number">7</span> Effect sizes and power</a></li>
<li><a class="active" href="replication-crisis.html"><span class="header-section-number">8</span> Replication crisis</a></li>
<li><a class="" href="repeated-measures-and-multivariate-models.html"><span class="header-section-number">9</span> Repeated measures and multivariate models</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/lbelzile/math80667a">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="replication-crisis" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Replication crisis<a class="anchor" aria-label="anchor" href="#replication-crisis"><i class="fas fa-link"></i></a>
</h1>
<p>In recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications failed to yield anything like what authors used to claim, or found much weaker findings. This chapter examines some of the causes of this lack of replicability.</p>
<div class="keyidea">
<ul>
<li>Defining replicability and reproducibility.</li>
<li>Understanding the scale of the replication crisis.</li>
<li>Recognizing common statistical fallacies.</li>
<li>Listing strategies for enhancing reproducibility.</li>
</ul>
</div>
<p>We adopt the terminology of <span class="citation">Claerbout and Karrenbach (<a href="references.html#ref-Claerbout/Karrenbach:1992" role="doc-biblioref">1992</a>)</span>: a study is said to be <strong>reproducible</strong> if an external person with the same data and enough indications about the procedure (for example, the code and software versions, etc.) can obtain consistent results that match those of a paper. A related scientific matter is <strong>replicability</strong>, which is the process by which new data are collected to test the same hypothesis, potentially using different methodology. Reproducibility is important because it enhances the credibility of one’s work. Extensions that deal with different analyses leading to the same conclusion are described in <a href="%5Bhttps://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html%5D">The Turing Way</a> and presented in <a href="replication-crisis.html#fig:turingrepdo">8.1</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:turingrepdo"></span>
<img src="figures/turing_way.jpg" alt="Definition of different dimensions of reproducible research (from The Turing Way project, illustration by Scriberia)." width="85%"><p class="caption">
Figure 8.1: Definition of different dimensions of reproducible research (from The Turing Way project, illustration by Scriberia).
</p>
</div>
<p>Why is reproducibility and replicability important? In a thought provoking paper, <span class="citation">Ioannidis (<a href="references.html#ref-Ioannidis:2005" role="doc-biblioref">2005</a>)</span> claimed that most research findings are wrong. The abstract of his paper stated</p>
<blockquote>
<p>There is increasing concern that most current published research findings are false. […] In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance.</p>
</blockquote>
<p>Since its publication, collaborative efforts have tried to assess the scale of the problem by reanalysing data and trying to replicate the findings of published research. For example, the “Reproducibility [sic] Project: Psychology” <span class="citation">(<a href="references.html#ref-Nosek:2015" role="doc-biblioref">Nosek et al. 2015</a>)</span></p>
<blockquote>
<p>conducted replications of 100 experimental and correlational studies published in three psychology journals using high powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety seven percent of original studies had significant results. Thirty six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and, if no bias in original results is assumed, combining original and replication results left 68% with significant effects. […]</p>
</blockquote>
<p>A large share of findings in the review were not replicable or the effects were much smaller than claimed, as shown by <a href="https://osf.io/447b3/">Figure 2 from the study</a>.
Such findings show that the peer-review procedure is not foolproof: the “publish-or-perish” mindset in academia is leading many researchers to try and achieve statistical significance at all costs to meet the 5% level criterion, whether involuntarily or not. This problem has many names: <span class="math inline">\(p\)</span>-hacking, harking or to paraphrase a <a href="https://en.wikipedia.org/wiki/The_Garden_of_Forking_Paths">story of Jorge Luis Borges</a>, the garden of forking paths. There are many degrees of freedom in the analysis for researchers to refine their hypothesis after viewing the data, conducting many unplanned comparisons and reporting selected results.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:repropvaluescorr"></span>
<img src="figures/RPP_psycho_repro.png" alt="Figure 2 from @Nosek:2015, showing scatterplot of effect sizes for the original and the replication study by power, with rugs and density plots by significance at the 5% level." width="85%"><p class="caption">
Figure 8.2: Figure 2 from <span class="citation">Nosek et al. (<a href="references.html#ref-Nosek:2015" role="doc-biblioref">2015</a>)</span>, showing scatterplot of effect sizes for the original and the replication study by power, with rugs and density plots by significance at the 5% level.
</p>
</div>
<p>Another problem is selective reporting. Because a large emphasis is placed on statistical significance, many studies that find small effects are never published, resulting in a gap. Figure <a href="replication-crisis.html#fig:reprozscores">8.3</a> from <span class="citation">Zwet and Cator (<a href="references.html#ref-vanZwet:2021" role="doc-biblioref">2021</a>)</span> shows <span class="math inline">\(z\)</span>-scores obtained by transforming confidence intervals reported in <span class="citation">Barnett and Wren (<a href="references.html#ref-Barnett:2019" role="doc-biblioref">2019</a>)</span>. The authors used data mining techniques to extract confidence intervals from abstracts of nearly one million publication in Medline published between 1976 and 2019. If most experiments yielded no effect and were due to natural variability, the <span class="math inline">\(z\)</span>-scores should be normally distributed, but Figure <a href="replication-crisis.html#fig:reprozscores">8.3</a> shows a big gap in the bell curve between approximately <span class="math inline">\(-2\)</span> and <span class="math inline">\(2\)</span>, indicative of selective reporting. The fact that results that do not lead to <span class="math inline">\(p &lt; 0.05\)</span> are not published is called the <strong>file-drawer</strong> problem.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:reprozscores"></span>
<img src="figures/vanZwet_Cator-zvalues.png" alt="Figure from @vanZwet:2021 based on results of @Barnett:2019; histogram of $z$-scores from one million studies from Medline." width="85%"><p class="caption">
Figure 8.3: Figure from <span class="citation">Zwet and Cator (<a href="references.html#ref-vanZwet:2021" role="doc-biblioref">2021</a>)</span> based on results of <span class="citation">Barnett and Wren (<a href="references.html#ref-Barnett:2019" role="doc-biblioref">2019</a>)</span>; histogram of <span class="math inline">\(z\)</span>-scores from one million studies from Medline.
</p>
</div>
<p>The ongoing debate surrounding the reproducibility crisis has sparked dramatic changes in the academic landscape: to enhance the quality of studies published, many journal now require authors to provide their code and data, to pre-register their studies, etc. Teams lead effort (e.g., the <a href="https://experimentaleconreplications.com/studies.html">Experimental Economics Replication Project</a>) try to replicate studies, with mitigated success so far. This <a href="https://devonprice.medium.com/questionable-research-practices-ive-taken-part-in-754b74dcaa51">inside recollection</a> by a graduate student shows the extent of the problem.</p>
<p>This course will place a strong emphasis on identifying and avoiding statistical fallacies and showcasing methods than enhance reproducibility. How can reproducible research enhance your work? For one thing, this workflow facilitates the publication of negative research, forces researchers to think ahead of time (and receive feedback). Reproducible research and data availability also leads to additional citations and increased credibility as a scientist.</p>
<p>Among good practices are</p>
<ul>
<li>pre-registration of experiments and use of a logbook.</li>
<li>clear reporting of key aspects of the experiment (choice of metric, number of items in a Likert scale, etc.)</li>
<li>version control systems (e.g., Git) that track changes to files and records.</li>
<li>archival of raw data in a proper format with accompanying documentation.</li>
</ul>
<p>Keeping a logbook and documenting your progress helps your collaborators, reviewers and your future-self understand decisions which may seem unclear and arbitrary in the future, even if they were the result of a careful thought process at the time you made them. Given the pervasiveness of the garden of forking paths, pre-registration helps you prevents harking because it limits selective reporting and unplanned tests, but it is not a panacea. Critics often object to pre-registration claiming that it binds people. This is a misleading claim in my view: pre-registration doesn’t mean that you must stick with the plan exactly, but merely requires you to explain what did not go as planned if anything.</p>
<p>Version control keeps records of changes to your file and can help you retrieve former versions if you make mistakes at some point.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:reprotweetexcelgenes"></span>
<img src="figures/reproducibility.png" alt="Tweet showing widespread problems related to unintentional changes to raw data by software." width="85%"><p class="caption">
Figure 8.4: Tweet showing widespread problems related to unintentional changes to raw data by software.
</p>
</div>
<p>Archival of data helps to avoid unintentional and irreversible manipulations of the original data, examples of which can have large scale consequences as illustrated in Figure <a href="replication-crisis.html#fig:reprotweetexcelgenes">8.4</a> <span class="citation">(<a href="references.html#ref-Ziemann:2016" role="doc-biblioref">Ziemann and El-Osta 2016</a>)</span>, who report flaws in genetic journals due to the automatic conversion of gene names to dates in Excel. These problems are <a href="https://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england">far from unique</a>. While sensitive data cannot be shared “as is” because of confidentiality issues, in many instances the data can and should be made available with a licence and a DOI to allow people to reuse it, cite and credit your work.</p>
<p>To enforce reproducibility, many journals now have policy regarding data, material and code availability. Some journals encourage such, while the trend in recent years has been to enforce. For example, Nature require the following to be reported in all published papers:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:naturereportstat"></span>
<img src="figures/Nature_reporting_statistics.png" alt="Screenshot of the Nature Reporting summary for statistics, reproduced under the CC BY 4.0 license." width="85%"><p class="caption">
Figure 8.5: Screenshot of the Nature Reporting summary for statistics, reproduced under the CC BY 4.0 license.
</p>
</div>
<div id="causes-of-the-replication-crisis" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Causes of the replication crisis<a class="anchor" aria-label="anchor" href="#causes-of-the-replication-crisis"><i class="fas fa-link"></i></a>
</h2>
<p>Below are multiple (non-exclusive) explanations for the lack of replication of study findings.</p>
<div id="the-garden-of-forking-paths" class="section level3" number="8.1.1">
<h3>
<span class="header-section-number">8.1.1</span> The garden of forking paths<a class="anchor" aria-label="anchor" href="#the-garden-of-forking-paths"><i class="fas fa-link"></i></a>
</h3>
<p>The garden of forking paths, named after a <a href="https://en.wikipedia.org/wiki/The_Garden_of_Forking_Paths">novel of Borges</a>, is a term coined by <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/forking.pdf">Andrew Gelman</a> to refer to researchers’ degrees of freedom. With vague hypothesis and data collection rules, it is easy for the researcher to adapt and interpret the conclusions in a way that fits his or her chosen narratives. In the words of <span class="citation">Gelman and Loken (<a href="references.html#ref-Gelman.Loken:2014" role="doc-biblioref">2014</a>)</span></p>
<blockquote>
<p>Given a particular data set, it can seem entirely appropriate to look at the data and construct reasonable rules for data exclusion, coding, and analysis that can lead to statistical significance. In such a case, researchers need to perform only one test, but that test is conditional on the data.</p>
</blockquote>
<p>This user case is not accomodated by classical testing theory. Research hypothesis are often formulated in a vague way, such that different analysis methods, tests may be compatible. <a href="https://docs.iza.org/dp15476.pdf">Abel et al. (2022) recent preprint</a> found that preregistration alone did not solve this problem, but that publication bias in randomized control trial was alleviated by publication of pre-analysis plans. This is directly related to the garden of forking path.</p>
</div>
<div id="selective-reporting" class="section level3" number="8.1.2">
<h3>
<span class="header-section-number">8.1.2</span> Selective reporting<a class="anchor" aria-label="anchor" href="#selective-reporting"><i class="fas fa-link"></i></a>
</h3>
<p>Also known as the file-drawer problem, selective reporting occurs because publication of results that fail to reach statistical significance (sic) are harder to publish. In much the same way as multiple testing, if 20 researchers perform a study but only one of them writes a paper and the result is a fluke, then this indicates. There are widespread indications publication bias, as evidence by the distribution of <span class="math inline">\(p\)</span>-values reported in papers. A <a href="https://docs.iza.org/dp15478.pdf">recent preprint of a study</a> found the prevalance to be higher in online experiments such as Amazon MTurks.</p>
<p><em>P</em>-hacking and the replication crisis has lead many leading statisticians to advocate much more stringent cutoff criterion such as <span class="math inline">\(p &lt; 0.001\)</span> instead of the usual <span class="math inline">\(p&lt;0.05\)</span> criterion as level for the test.
The level <span class="math inline">\(\alpha=5\)</span>% is essentially arbitrary and dates back to <span class="citation">Fisher (<a href="references.html#ref-Fisher:1926" role="doc-biblioref">1926</a>)</span>, who wrote</p>
<blockquote>
<p>If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred. Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fails to reach this level.</p>
</blockquote>
<div class="outsidethebox">
<p>Methods that pool together results, such as meta-analysis, are sensitive to selective reporting. Why does it matter?</p>
</div>
</div>
<div id="non-representative-samples" class="section level3" number="8.1.3">
<h3>
<span class="header-section-number">8.1.3</span> Non-representative samples<a class="anchor" aria-label="anchor" href="#non-representative-samples"><i class="fas fa-link"></i></a>
</h3>
<p>Many researchers opt for convenience samples by using online panels such as Qualtrics, Amazon MTurks, etc. The quality of those observations is at best dubious: ask yourself whether you would answer such as survey for a small amount. Manipulation checks to ensure participants are following, information is not completed by bots, a threshold for the minimal time required to complete the study, etc. are necessary (but not sufficient) conditions to ensure that the data are not rubbish.</p>
<p>A more important criticism is that the people who answer those surveys are not representative of the population as a whole: sampling bias thus plays an important role in the conclusions and, even if the summary statistics are not too different from the general population, they may exhibit different opinions, levels of skills, etc. than most.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:samplingbias"></span>
<img src="figures/samplingbias.jpg" alt="Sampling bias. Artwork by [Jonathan Hey (Sketchplanations)](https://sketchplanations.com/sampling-bias) shared under the [CC BY-NC 4.0 license](http://creativecommons.org/licenses/by-nc/4.0/)." width="85%"><p class="caption">
Figure 8.6: Sampling bias. Artwork by <a href="https://sketchplanations.com/sampling-bias">Jonathan Hey (Sketchplanations)</a> shared under the <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 license</a>.
</p>
</div>
<p>The same can be said of panels of students recruited in universities classes, who are more young, educated and perhaps may infer through backward induction the purpose of the study and answer accordingly.</p>
</div>
</div>
<div id="summary" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Summary<a class="anchor" aria-label="anchor" href="#summary"><i class="fas fa-link"></i></a>
</h2>
<p>Operating in an open-science environment should be seen as an opportunity to make better science, offer more opportunities to increase your impact and increase the likelihood that your work gets published regardless of whether the results turn out to be negative. It is the <em>right thing</em> to do and it increases the quality of research produced, with collateral benefits because it forces researchers to validate their methodology before, to double-check their data and their analysis and to adopt good practice.</p>
<p>There are many platforms for preregistering studies and sharing preanalysis plans, scripts and data, with different level of formality. One such is the <a href="https://researchbox.org/">Research Box</a>.</p>
<div class="yourturn">
<p>Reflect on your workflow as applied researcher when designing and undertaking experiments. Which practical aspects could you improve upon to improve the reproducibility of your study?</p>
</div>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="effect-sizes-and-power.html"><span class="header-section-number">7</span> Effect sizes and power</a></div>
<div class="next"><a href="repeated-measures-and-multivariate-models.html"><span class="header-section-number">9</span> Repeated measures and multivariate models</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#replication-crisis"><span class="header-section-number">8</span> Replication crisis</a></li>
<li>
<a class="nav-link" href="#causes-of-the-replication-crisis"><span class="header-section-number">8.1</span> Causes of the replication crisis</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-garden-of-forking-paths"><span class="header-section-number">8.1.1</span> The garden of forking paths</a></li>
<li><a class="nav-link" href="#selective-reporting"><span class="header-section-number">8.1.2</span> Selective reporting</a></li>
<li><a class="nav-link" href="#non-representative-samples"><span class="header-section-number">8.1.3</span> Non-representative samples</a></li>
</ul>
</li>
<li><a class="nav-link" href="#summary"><span class="header-section-number">8.2</span> Summary</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/lbelzile/math80667a/blob/master/08-reproducibility_crisis.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/lbelzile/math80667a/edit/master/08-reproducibility_crisis.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Experimental Design and Statistical Methods</strong>" was written by Léo Belzile. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
